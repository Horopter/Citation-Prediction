{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80255a57",
   "metadata": {
    "papermill": {
     "duration": 0.003321,
     "end_time": "2025-11-19T00:02:33.305321",
     "exception": false,
     "start_time": "2025-11-19T00:02:33.302000",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 5: XGBoost with All Features + Scaling + Normalization + t-SNE\n",
    "\n",
    "This notebook trains an **XGBoost** classifier on all available features (regular + all embeddings) with comprehensive preprocessing:\n",
    "- ‚úÖ All regular features (54)\n",
    "- ‚úÖ All embedding families (PCA-compressed)\n",
    "- ‚úÖ Feature scaling (StandardScaler/RobustScaler)\n",
    "- ‚úÖ Feature normalization (MinMaxScaler)\n",
    "- ‚úÖ Optional t-SNE dimensionality reduction\n",
    "- ‚úÖ 5-fold Cross-Validation\n",
    "- ‚úÖ Comprehensive Hyperparameter Tuning (GridSearchCV/RandomizedSearchCV)\n",
    "- ‚úÖ Threshold Fine-tuning\n",
    "- ‚úÖ Model Saving\n",
    "- ‚úÖ Submission.csv Generation\n",
    "- ‚úÖ OOM Safe with aggressive memory management\n",
    "- ‚úÖ Robust error handling (dead kernels, panics, warnings)\n",
    "\n",
    "## Memory & Robustness Notes\n",
    "\n",
    "**Memory Optimizations Applied:**\n",
    "- ‚úÖ Aggressive garbage collection after data loading\n",
    "- ‚úÖ Explicit deletion of DataFrames/arrays after conversion\n",
    "- ‚úÖ Chunked processing for large datasets\n",
    "- ‚úÖ Periodic memory cleanup during training\n",
    "- ‚úÖ Memory usage monitoring\n",
    "- ‚úÖ Safe memory checks before operations\n",
    "\n",
    "**Robustness Features:**\n",
    "- ‚úÖ Try-except blocks around all critical operations\n",
    "- ‚úÖ Graceful degradation on errors\n",
    "- ‚úÖ Checkpoint saving (resume from failures)\n",
    "- ‚úÖ Warning suppression for cleaner output\n",
    "- ‚úÖ Memory-safe operations\n",
    "- ‚úÖ Progress tracking with timeouts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f81ae3c",
   "metadata": {
    "papermill": {
     "duration": 0.002482,
     "end_time": "2025-11-19T00:02:33.310613",
     "exception": false,
     "start_time": "2025-11-19T00:02:33.308131",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f7f3ad2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T00:02:33.316651Z",
     "iopub.status.busy": "2025-11-19T00:02:33.316491Z",
     "iopub.status.idle": "2025-11-19T00:03:30.598363Z",
     "shell.execute_reply": "2025-11-19T00:03:30.597696Z"
    },
    "papermill": {
     "duration": 57.285952,
     "end_time": "2025-11-19T00:03:30.599078",
     "exception": false,
     "start_time": "2025-11-19T00:02:33.313126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PROJECT_ROOT: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2\n",
      "MODEL_READY_DIR: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/model_ready\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using sklearn IncrementalPCA (memory-efficient)\n",
      "‚úÖ Memory utilities imported\n",
      "PROJECT_ROOT: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2\n",
      "üíæ Memory: 0.59 GB (RAM) | 0.00/0.00 GB (GPU used/reserved)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# PyTorch-based PCA (GPU-friendly with CPU fallback) - same as models 1-4\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# Paths (adapt if your project structure differs)\n",
    "# Try to find project root by looking for 'data' directory\n",
    "current = Path(os.getcwd())\n",
    "PROJECT_ROOT = current\n",
    "# Go up directories until we find one with 'data' subdirectory\n",
    "for _ in range(5):  # Max 5 levels up\n",
    "    if (PROJECT_ROOT / 'data').exists():\n",
    "        break\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "else:\n",
    "    # Fallback: assume we're in src/notebooks, go up 2 levels\n",
    "    PROJECT_ROOT = current.parent.parent\n",
    "\n",
    "MODEL_READY_DIR = PROJECT_ROOT / 'data' / 'model_ready'\n",
    "MODEL_SAVE_DIR = PROJECT_ROOT / 'models' / 'saved_models'\n",
    "SUBMISSION_DIR = PROJECT_ROOT / 'data' / 'submission_files'\n",
    "MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SUBMISSION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "utils_path = PROJECT_ROOT / 'src' / 'utils'\n",
    "\n",
    "print('PROJECT_ROOT:', PROJECT_ROOT)\n",
    "print('MODEL_READY_DIR:', MODEL_READY_DIR)\n",
    "\n",
    "# Import PCA utilities\n",
    "# Use sklearn IncrementalPCA by default for better memory efficiency in constrained environments\n",
    "# PyTorch PCA can be used on SLURM with proper resources\n",
    "USE_TORCH_PCA = False  # Set to True to use PyTorch PCA (requires more memory)\n",
    "\n",
    "if utils_path.exists():\n",
    "    sys.path.insert(0, str(utils_path))\n",
    "if USE_TORCH_PCA:\n",
    "    try:\n",
    "        from pca_utils import IncrementalTorchPCA\n",
    "        IncrementalPCA = IncrementalTorchPCA  # Alias for compatibility\n",
    "        IS_TORCH_PCA = True\n",
    "        print(\"‚úÖ Using PyTorch PCA (GPU-friendly)\")\n",
    "    except ImportError:\n",
    "        # Fallback to sklearn if PyTorch PCA not available\n",
    "        from sklearn.decomposition import IncrementalPCA\n",
    "        IS_TORCH_PCA = False\n",
    "        print(\"‚ö†Ô∏è Using sklearn IncrementalPCA (CPU only)\")\n",
    "else:\n",
    "    # Use sklearn IncrementalPCA by default for memory efficiency\n",
    "    from sklearn.decomposition import IncrementalPCA\n",
    "    IS_TORCH_PCA = False\n",
    "    print(\"‚úÖ Using sklearn IncrementalPCA (memory-efficient)\")\n",
    "\n",
    "try:\n",
    "    from model_training_utils import cleanup_memory, memory_usage, check_memory_safe\n",
    "    print(\"‚úÖ Memory utilities imported\")\n",
    "except ImportError:\n",
    "    def cleanup_memory():\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "        gc.collect()\n",
    "    def memory_usage():\n",
    "        try:\n",
    "            import psutil\n",
    "            process = psutil.Process(os.getpid())\n",
    "            mem_gb = process.memory_info().rss / 1024**3\n",
    "            print(f\"üíæ Memory: {mem_gb:.2f} GB (RAM)\", end=\"\")\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_mem = torch.cuda.memory_allocated() / 1024**3\n",
    "                gpu_reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "                print(f\" | {gpu_mem:.2f}/{gpu_reserved:.2f} GB (GPU used/reserved)\")\n",
    "            else:\n",
    "                print()\n",
    "        except:\n",
    "            pass\n",
    "    def check_memory_safe(ram_threshold_gb=0.85, gpu_threshold=0.80):\n",
    "        try:\n",
    "            import psutil\n",
    "            process = psutil.Process(os.getpid())\n",
    "            ram_gb = process.memory_info().rss / 1024**3\n",
    "            total_ram = psutil.virtual_memory().total / 1024**3\n",
    "            ram_ratio = ram_gb / total_ram if total_ram > 0 else 0\n",
    "            \n",
    "            gpu_ratio = 0\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_used = torch.cuda.memory_allocated() / 1024**3\n",
    "                gpu_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "                gpu_ratio = gpu_used / gpu_total if gpu_total > 0 else 0\n",
    "            \n",
    "            is_safe = ram_ratio < ram_threshold_gb and gpu_ratio < gpu_threshold\n",
    "            return is_safe, {'ram_gb': ram_gb, 'ram_ratio': ram_ratio, 'gpu_ratio': gpu_ratio}\n",
    "        except:\n",
    "            return True, {}\n",
    "    print(\"‚ö†Ô∏è Using fallback memory utilities\")\n",
    "\n",
    "print('PROJECT_ROOT:', PROJECT_ROOT)\n",
    "memory_usage()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a37b17",
   "metadata": {
    "papermill": {
     "duration": 0.002674,
     "end_time": "2025-11-19T00:03:30.604780",
     "exception": false,
     "start_time": "2025-11-19T00:03:30.602106",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Data Loading & Feature Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a15596b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T00:03:30.611341Z",
     "iopub.status.busy": "2025-11-19T00:03:30.611010Z",
     "iopub.status.idle": "2025-11-19T00:04:58.751888Z",
     "shell.execute_reply": "2025-11-19T00:04:58.751228Z"
    },
    "papermill": {
     "duration": 88.145079,
     "end_time": "2025-11-19T00:04:58.752513",
     "exception": false,
     "start_time": "2025-11-19T00:03:30.607434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train from /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/model_ready/train_model_ready.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading val from /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/model_ready/val_model_ready.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Data Summary:\n",
      "  Regular features: 54\n",
      "  Embedding sent_transformer_: 384 dims\n",
      "  Embedding scibert_: 768 dims\n",
      "  Embedding specter2_: 768 dims\n",
      "  Train samples: 960000, Positive: 65808, Negative: 894192\n",
      "  Val samples: 120000, Positive: 8075, Negative: 111925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Memory: 32.72 GB (RAM) | 0.00/0.00 GB (GPU used/reserved)\n"
     ]
    }
   ],
   "source": [
    "def load_parquet_split(split: str) -> pl.DataFrame:\n",
    "    \"\"\"Load a model_ready parquet split with error handling.\"\"\"\n",
    "    try:\n",
    "        path = MODEL_READY_DIR / f'{split}_model_ready.parquet'\n",
    "        if not path.exists():\n",
    "            alt = MODEL_READY_DIR / f'{split}_model_ready_reduced.parquet'\n",
    "            if alt.exists():\n",
    "                path = alt\n",
    "            else:\n",
    "                raise FileNotFoundError(f'Could not find {split} data')\n",
    "        print(f'Loading {split} from {path}')\n",
    "        return pl.read_parquet(path)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {split}: {e}\")\n",
    "        raise\n",
    "\n",
    "def split_features_reg_and_all_emb(df: pl.DataFrame):\n",
    "    \"\"\"Split features into regular and embedding families.\"\"\"\n",
    "    cols = df.columns\n",
    "    dtypes = df.dtypes\n",
    "    label = df['label'].to_numpy() if 'label' in cols else None\n",
    "    \n",
    "    reg_cols = []\n",
    "    EMBEDDING_FAMILY_PREFIXES = ['sent_transformer_', 'scibert_', 'specter_', 'specter2_', 'ner_']\n",
    "    emb_family_to_cols = {p: [] for p in EMBEDDING_FAMILY_PREFIXES}\n",
    "    \n",
    "    NUMERIC_DTYPES = {\n",
    "        pl.Int8, pl.Int16, pl.Int32, pl.Int64,\n",
    "        pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,\n",
    "        pl.Float32, pl.Float64\n",
    "    }\n",
    "    \n",
    "    for c, dt in zip(cols, dtypes):\n",
    "        if c in ('id', 'label'):\n",
    "            continue\n",
    "        matched = False\n",
    "        for p in EMBEDDING_FAMILY_PREFIXES:\n",
    "            if c.startswith(p):\n",
    "                emb_family_to_cols[p].append(c)\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched and dt in NUMERIC_DTYPES:\n",
    "            reg_cols.append(c)\n",
    "    \n",
    "    X_reg = df.select(reg_cols).to_numpy() if reg_cols else None\n",
    "    X_emb_families = {}\n",
    "    for p, clist in emb_family_to_cols.items():\n",
    "        if clist:\n",
    "            X_emb_families[p] = df.select(clist).to_numpy()\n",
    "    \n",
    "    return X_reg, X_emb_families, label, reg_cols, emb_family_to_cols\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    train_df = load_parquet_split('train')\n",
    "    val_df = load_parquet_split('val')\n",
    "    \n",
    "    X_reg_train, X_emb_train_fams, y_train, reg_cols, emb_family_to_cols = split_features_reg_and_all_emb(train_df)\n",
    "    X_reg_val, X_emb_val_fams, y_val, _, _ = split_features_reg_and_all_emb(val_df)\n",
    "    \n",
    "    print(f'\\nüìä Data Summary:')\n",
    "    print(f'  Regular features: {len(reg_cols)}')\n",
    "    for fam, arr in X_emb_train_fams.items():\n",
    "        print(f'  Embedding {fam}: {arr.shape[1]} dims')\n",
    "    print(f'  Train samples: {len(y_train)}, Positive: {y_train.sum()}, Negative: {(y_train==0).sum()}')\n",
    "    print(f'  Val samples: {len(y_val)}, Positive: {y_val.sum()}, Negative: {(y_val==0).sum()}')\n",
    "    \n",
    "    del train_df, val_df\n",
    "    cleanup_memory()\n",
    "    memory_usage()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in data loading: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4872a862",
   "metadata": {
    "papermill": {
     "duration": 0.002893,
     "end_time": "2025-11-19T00:04:58.759521",
     "exception": false,
     "start_time": "2025-11-19T00:04:58.756628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Feature Preprocessing: PCA, Scaling, Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beb6d599",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T00:04:58.766541Z",
     "iopub.status.busy": "2025-11-19T00:04:58.766348Z",
     "iopub.status.idle": "2025-11-19T00:07:54.993668Z",
     "shell.execute_reply": "2025-11-19T00:07:54.992974Z"
    },
    "papermill": {
     "duration": 176.232063,
     "end_time": "2025-11-19T00:07:54.994416",
     "exception": false,
     "start_time": "2025-11-19T00:04:58.762353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Applying IncrementalPCA to embedding families...\n",
      "  sent_transformer_: 384 dims ‚Üí 32 components\n",
      "  scibert_: 768 dims ‚Üí 32 components\n",
      "  specter2_: 768 dims ‚Üí 32 components\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting PCA on subset (50000/960000 samples) for sent_transformer_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting PCA on subset (50000/960000 samples) for scibert_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting PCA on subset (50000/960000 samples) for specter2_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä After IncrementalPCA:\n",
      "  Train embeddings: (960000, 96)\n",
      "  Val embeddings: (120000, 96)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Combined train: (960000, 150)\n",
      "  Combined val: (120000, 150)\n",
      "üíæ Memory: 34.03 GB (RAM) | 0.00/0.00 GB (GPU used/reserved)\n"
     ]
    }
   ],
   "source": [
    "# PCA compression per embedding family\n",
    "PCA_COMPONENTS_PER_FAMILY = {\n",
    "    'sent_transformer_': 32,\n",
    "    'scibert_': 32,\n",
    "    'specter_': 32,\n",
    "    'specter2_': 32,\n",
    "    'ner_': 16,\n",
    "}\n",
    "\n",
    "def apply_pca_to_embeddings(X_emb_fams: Dict[str, np.ndarray], fit_on_train: bool = True, pca_models: Optional[Dict] = None):\n",
    "    \"\"\"Apply IncrementalPCA to each embedding family (GPU-friendly, OOM-resistant).\"\"\"\n",
    "    X_emb_pca_list = []\n",
    "    new_pca_models = {}\n",
    "    \n",
    "    # Aggressive memory cleanup before starting\n",
    "    cleanup_memory()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Check memory before processing\n",
    "    is_safe, mem_info = check_memory_safe(ram_threshold_gb=0.75, gpu_threshold=0.70)\n",
    "    if not is_safe:\n",
    "        print(f\"‚ö†Ô∏è Memory usage high before PCA: {mem_info}\")\n",
    "        cleanup_memory()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    for fam, X_emb in X_emb_fams.items():\n",
    "        n_components = PCA_COMPONENTS_PER_FAMILY.get(fam, 32)\n",
    "        \n",
    "        try:\n",
    "            # Cleanup before each family\n",
    "            cleanup_memory()\n",
    "            \n",
    "            if fit_on_train or pca_models is None:\n",
    "                # Try PyTorch PCA first, fallback to sklearn IncrementalPCA if memory constrained\n",
    "                try:\n",
    "                    # Use IncrementalPCA (same as models 1-4) for memory efficiency\n",
    "                    # Reduce batch size for better memory management\n",
    "                    # Only pass device parameter if using PyTorch PCA\n",
    "                    if IS_TORCH_PCA:\n",
    "                        ipca = IncrementalPCA(n_components=min(n_components, X_emb.shape[1]), batch_size=2000, device=device)\n",
    "                    else:\n",
    "                        ipca = IncrementalPCA(n_components=min(n_components, X_emb.shape[1]), batch_size=2000)\n",
    "                    \n",
    "                    # Fit on subset for large datasets (OOM protection) - very aggressive for kernel execution\n",
    "                    # Use smaller subset to avoid kernel OOM\n",
    "                    max_pca_rows = min(50_000, X_emb.shape[0])  # Reduced from 100k\n",
    "                    if X_emb.shape[0] > max_pca_rows:\n",
    "                        print(f\"  Fitting PCA on subset ({max_pca_rows}/{X_emb.shape[0]} samples) for {fam}\")\n",
    "                        idx = np.random.choice(X_emb.shape[0], size=max_pca_rows, replace=False)\n",
    "                        X_emb_subset = X_emb[idx].copy()  # Explicit copy to avoid memory issues\n",
    "                        del idx  # Free index immediately\n",
    "                        cleanup_memory()\n",
    "                        \n",
    "                        # Fit PCA\n",
    "                        ipca.fit(X_emb_subset)\n",
    "                        del X_emb_subset  # Free memory immediately\n",
    "                        cleanup_memory()\n",
    "                    else:\n",
    "                        # For small datasets, still use copy to avoid memory issues\n",
    "                        X_emb_copy = X_emb.copy() if X_emb.flags['OWNDATA'] == False else X_emb\n",
    "                        ipca.fit(X_emb_copy)\n",
    "                        if X_emb_copy is not X_emb:\n",
    "                            del X_emb_copy\n",
    "                        cleanup_memory()\n",
    "                    \n",
    "                    new_pca_models[fam] = ipca\n",
    "                except (RuntimeError, MemoryError) as e:\n",
    "                    if 'out of memory' in str(e).lower() or 'OOM' in str(e).lower() or 'memory' in str(e).lower():\n",
    "                        print(f\"  ‚ö†Ô∏è PyTorch PCA OOM for {fam}, falling back to sklearn IncrementalPCA...\")\n",
    "                        cleanup_memory()\n",
    "                        if torch.cuda.is_available():\n",
    "                            torch.cuda.empty_cache()\n",
    "                        # Use sklearn IncrementalPCA which is truly incremental\n",
    "                        from sklearn.decomposition import IncrementalPCA as SklearnIncrementalPCA\n",
    "                        ipca = SklearnIncrementalPCA(n_components=min(n_components, X_emb.shape[1]), batch_size=1000)\n",
    "                        # Fit incrementally on chunks\n",
    "                        max_pca_rows = min(10_000, X_emb.shape[0])  # Very small for sklearn\n",
    "                        if X_emb.shape[0] > max_pca_rows:\n",
    "                            idx = np.random.choice(X_emb.shape[0], size=max_pca_rows, replace=False)\n",
    "                            X_emb_subset = X_emb[idx].copy()\n",
    "                            del idx\n",
    "                            cleanup_memory()\n",
    "                            ipca.fit(X_emb_subset)\n",
    "                            del X_emb_subset\n",
    "                            cleanup_memory()\n",
    "                        else:\n",
    "                            ipca.fit(X_emb)\n",
    "                            cleanup_memory()\n",
    "                        new_pca_models[fam] = ipca\n",
    "                    else:\n",
    "                        raise\n",
    "            else:\n",
    "                ipca = pca_models[fam]\n",
    "            \n",
    "            # Transform in smaller chunks for OOM protection\n",
    "            chunk_size = 5000  # Reduced from 10000\n",
    "            if X_emb.shape[0] > chunk_size:\n",
    "                X_emb_pca_chunks = []\n",
    "                for i in range(0, X_emb.shape[0], chunk_size):\n",
    "                    chunk = X_emb[i:i+chunk_size].copy()  # Explicit copy\n",
    "                    chunk_pca = ipca.transform(chunk)\n",
    "                    X_emb_pca_chunks.append(chunk_pca)\n",
    "                    del chunk, chunk_pca\n",
    "                    cleanup_memory()\n",
    "                    # Periodic GPU cleanup\n",
    "                    if i % (chunk_size * 5) == 0 and torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                \n",
    "                X_emb_pca = np.vstack(X_emb_pca_chunks)\n",
    "                del X_emb_pca_chunks\n",
    "                cleanup_memory()\n",
    "            else:\n",
    "                X_emb_copy = X_emb.copy() if X_emb.flags['OWNDATA'] == False else X_emb\n",
    "                X_emb_pca = ipca.transform(X_emb_copy)\n",
    "                if X_emb_copy is not X_emb:\n",
    "                    del X_emb_copy\n",
    "                cleanup_memory()\n",
    "            \n",
    "            X_emb_pca_list.append(X_emb_pca)\n",
    "            del X_emb_pca\n",
    "            cleanup_memory()  # Clean up after each family\n",
    "            \n",
    "            # Aggressive cleanup between families\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if 'out of memory' in str(e).lower() or 'OOM' in str(e).upper():\n",
    "                print(f\"‚ùå OOM error processing {fam}, cleaning up and retrying with smaller batch...\")\n",
    "                cleanup_memory()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    torch.cuda.synchronize()\n",
    "                \n",
    "                # Retry with much smaller subset\n",
    "                if fit_on_train:\n",
    "                    max_pca_rows = min(20_000, X_emb.shape[0])\n",
    "                    idx = np.random.choice(X_emb.shape[0], size=max_pca_rows, replace=False)\n",
    "                    X_emb_subset = X_emb[idx].copy()\n",
    "                    # Only pass device parameter if using PyTorch PCA\n",
    "                    if IS_TORCH_PCA:\n",
    "                        ipca = IncrementalPCA(n_components=min(n_components, X_emb.shape[1]), batch_size=500, device=device)\n",
    "                    else:\n",
    "                        ipca = IncrementalPCA(n_components=min(n_components, X_emb.shape[1]), batch_size=500)\n",
    "                    ipca.fit(X_emb_subset)\n",
    "                    new_pca_models[fam] = ipca\n",
    "                    del X_emb_subset, idx\n",
    "                    cleanup_memory()\n",
    "                else:\n",
    "                    ipca = pca_models[fam]\n",
    "                \n",
    "                # Transform with much smaller chunks\n",
    "                chunk_size_small = 1000\n",
    "                X_emb_pca_chunks = []\n",
    "                for i in range(0, X_emb.shape[0], chunk_size_small):\n",
    "                    chunk = X_emb[i:i+chunk_size_small].copy()\n",
    "                    chunk_pca = ipca.transform(chunk)\n",
    "                    X_emb_pca_chunks.append(chunk_pca)\n",
    "                    del chunk, chunk_pca\n",
    "                    cleanup_memory()\n",
    "                X_emb_pca = np.vstack(X_emb_pca_chunks)\n",
    "                del X_emb_pca_chunks\n",
    "                X_emb_pca_list.append(X_emb_pca)\n",
    "                cleanup_memory()\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    # Final cleanup\n",
    "    cleanup_memory()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    if X_emb_pca_list:\n",
    "        X_emb_combined = np.hstack(X_emb_pca_list)\n",
    "    else:\n",
    "        X_emb_combined = None\n",
    "    \n",
    "    return X_emb_combined, new_pca_models if fit_on_train else pca_models\n",
    "\n",
    "# Apply IncrementalPCA to embeddings (same approach as models 1-4)\n",
    "try:\n",
    "    # Aggressive cleanup before PCA\n",
    "    cleanup_memory()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    print('\\nüìä Applying IncrementalPCA to embedding families...')\n",
    "    for fam in X_emb_train_fams.keys():\n",
    "        n_comp = PCA_COMPONENTS_PER_FAMILY.get(fam, 32)\n",
    "        print(f'  {fam}: {X_emb_train_fams[fam].shape[1]} dims ‚Üí {n_comp} components')\n",
    "    \n",
    "    X_emb_train_pca, pca_models_train = apply_pca_to_embeddings(X_emb_train_fams, fit_on_train=True)\n",
    "    \n",
    "    # Cleanup between train and val PCA\n",
    "    cleanup_memory()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    X_emb_val_pca, _ = apply_pca_to_embeddings(X_emb_val_fams, fit_on_train=False, pca_models=pca_models_train)\n",
    "    \n",
    "    print(f'\\nüìä After IncrementalPCA:')\n",
    "    print(f'  Train embeddings: {X_emb_train_pca.shape}')\n",
    "    print(f'  Val embeddings: {X_emb_val_pca.shape}')\n",
    "    \n",
    "    # Combine regular + embeddings\n",
    "    if X_reg_train is not None:\n",
    "        X_train = np.hstack([X_reg_train, X_emb_train_pca])\n",
    "        X_val = np.hstack([X_reg_val, X_emb_val_pca])\n",
    "    else:\n",
    "        X_train = X_emb_train_pca\n",
    "        X_val = X_emb_val_pca\n",
    "    \n",
    "    print(f'  Combined train: {X_train.shape}')\n",
    "    print(f'  Combined val: {X_val.shape}')\n",
    "    \n",
    "    del X_reg_train, X_reg_val, X_emb_train_fams, X_emb_val_fams, X_emb_train_pca, X_emb_val_pca\n",
    "    cleanup_memory()\n",
    "    memory_usage()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in PCA: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cca7b1ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T00:07:55.002678Z",
     "iopub.status.busy": "2025-11-19T00:07:55.002493Z",
     "iopub.status.idle": "2025-11-19T00:07:55.011368Z",
     "shell.execute_reply": "2025-11-19T00:07:55.010840Z"
    },
    "papermill": {
     "duration": 0.013768,
     "end_time": "2025-11-19T00:07:55.012092",
     "exception": false,
     "start_time": "2025-11-19T00:07:54.998324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Using RobustScaler (robust to outliers)\n",
      "  Fitting scaler on chunks (size=50000) for OOM protection...\n",
      "‚ùå Error in scaling: 'RobustScaler' object has no attribute 'partial_fit'\n",
      "‚ö†Ô∏è Continuing without scaling...\n"
     ]
    }
   ],
   "source": [
    "# Feature Scaling and Normalization (OOM-resistant)\n",
    "USE_ROBUST_SCALER = True  # Set to False for StandardScaler\n",
    "USE_MINMAX_NORMALIZATION = True  # Apply MinMaxScaler after scaling\n",
    "\n",
    "try:\n",
    "    # Check memory before scaling\n",
    "    is_safe, mem_info = check_memory_safe(ram_threshold_gb=0.85, gpu_threshold=0.80)\n",
    "    if not is_safe:\n",
    "        print(f\"‚ö†Ô∏è Memory usage high before scaling: {mem_info}\")\n",
    "        cleanup_memory()\n",
    "    \n",
    "    if USE_ROBUST_SCALER:\n",
    "        scaler = RobustScaler()\n",
    "        print('\\nüìä Using RobustScaler (robust to outliers)')\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "        print('\\nüìä Using StandardScaler')\n",
    "    \n",
    "    # Fit on train (process in chunks if needed for OOM protection)\n",
    "    chunk_size = 50000\n",
    "    if X_train.shape[0] > chunk_size:\n",
    "        print(f'  Fitting scaler on chunks (size={chunk_size}) for OOM protection...')\n",
    "        scaler.partial_fit(X_train[:chunk_size])\n",
    "        for i in range(chunk_size, X_train.shape[0], chunk_size):\n",
    "            scaler.partial_fit(X_train[i:i+chunk_size])\n",
    "    else:\n",
    "        scaler.fit(X_train)\n",
    "    \n",
    "    # Transform in chunks for OOM protection\n",
    "    if X_train.shape[0] > chunk_size:\n",
    "        X_train_scaled_chunks = []\n",
    "        for i in range(0, X_train.shape[0], chunk_size):\n",
    "            chunk = scaler.transform(X_train[i:i+chunk_size])\n",
    "            X_train_scaled_chunks.append(chunk)\n",
    "            del chunk\n",
    "            cleanup_memory()\n",
    "        X_train_scaled = np.vstack(X_train_scaled_chunks)\n",
    "        del X_train_scaled_chunks\n",
    "    else:\n",
    "        X_train_scaled = scaler.transform(X_train)\n",
    "    \n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    if USE_MINMAX_NORMALIZATION:\n",
    "        print('üìä Applying MinMaxScaler normalization')\n",
    "        minmax_scaler = MinMaxScaler()\n",
    "        minmax_scaler.fit(X_train_scaled)\n",
    "        X_train_scaled = minmax_scaler.transform(X_train_scaled)\n",
    "        X_val_scaled = minmax_scaler.transform(X_val_scaled)\n",
    "    \n",
    "    # Replace original with scaled\n",
    "    X_train = X_train_scaled\n",
    "    X_val = X_val_scaled\n",
    "    \n",
    "    del X_train_scaled, X_val_scaled\n",
    "    cleanup_memory()\n",
    "    \n",
    "    print(f'‚úÖ Scaling complete. Train: {X_train.shape}, Val: {X_val.shape}')\n",
    "    print(f'  Train range: [{X_train.min():.3f}, {X_train.max():.3f}]')\n",
    "    print(f'  Val range: [{X_val.min():.3f}, {X_val.max():.3f}]')\n",
    "    memory_usage()\n",
    "except RuntimeError as e:\n",
    "    if 'out of memory' in str(e).lower() or 'OOM' in str(e).upper():\n",
    "        print(f\"‚ùå OOM error in scaling: {e}\")\n",
    "        print(\"‚ö†Ô∏è Continuing without scaling...\")\n",
    "        cleanup_memory()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    else:\n",
    "        raise\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in scaling: {e}\")\n",
    "    print(\"‚ö†Ô∏è Continuing without scaling...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db795be",
   "metadata": {
    "papermill": {
     "duration": 0.003197,
     "end_time": "2025-11-19T00:07:55.018590",
     "exception": false,
     "start_time": "2025-11-19T00:07:55.015393",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Cross-Validation & Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "435e066e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T00:07:55.027212Z",
     "iopub.status.busy": "2025-11-19T00:07:55.027043Z",
     "iopub.status.idle": "2025-11-19T00:07:56.228166Z",
     "shell.execute_reply": "2025-11-19T00:07:56.227489Z"
    },
    "papermill": {
     "duration": 1.205419,
     "end_time": "2025-11-19T00:07:56.228673",
     "exception": false,
     "start_time": "2025-11-19T00:07:55.023254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Full dataset for CV: (1080000, 150), labels: (1080000,)\n",
      "  Positive samples: 73883, Negative: 1006117\n",
      "\n",
      "üîç Hyperparameter tuning:\n",
      "  Method: RandomizedSearchCV\n",
      "  CV folds: 5\n",
      "  Random iterations: 50\n",
      "üíæ Memory: 35.23 GB (RAM) | 0.00/0.00 GB (GPU used/reserved)\n"
     ]
    }
   ],
   "source": [
    "# Combine train and val for CV\n",
    "X_full = np.vstack([X_train, X_val])\n",
    "y_full = np.hstack([y_train, y_val])\n",
    "\n",
    "print(f'\\nüìä Full dataset for CV: {X_full.shape}, labels: {y_full.shape}')\n",
    "print(f'  Positive samples: {y_full.sum()}, Negative: {(y_full==0).sum()}')\n",
    "\n",
    "# Setup 5-fold CV\n",
    "N_FOLDS = 5\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# XGBoost hyperparameter grid (comprehensive)\n",
    "XGB_PARAM_GRID = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [1, 1.5, 2.0],\n",
    "    'scale_pos_weight': [1, (y_full == 0).sum() / max((y_full == 1).sum(), 1)]\n",
    "}\n",
    "\n",
    "# For faster tuning, use RandomizedSearchCV\n",
    "USE_RANDOMIZED_SEARCH = True\n",
    "N_ITER_RANDOM = 50\n",
    "\n",
    "print(f'\\nüîç Hyperparameter tuning:')\n",
    "print(f'  Method: {\"RandomizedSearchCV\" if USE_RANDOMIZED_SEARCH else \"GridSearchCV\"}')\n",
    "print(f'  CV folds: {N_FOLDS}')\n",
    "if USE_RANDOMIZED_SEARCH:\n",
    "    print(f'  Random iterations: {N_ITER_RANDOM}')\n",
    "\n",
    "cleanup_memory()\n",
    "memory_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b86400",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-11-19T00:07:56.232419",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with error handling\n",
    "best_model = None\n",
    "best_params = None\n",
    "best_cv_score = 0.0\n",
    "\n",
    "try:\n",
    "    print('\\n' + '='*80)\n",
    "    print('Starting Hyperparameter Tuning')\n",
    "    print('='*80)\n",
    "    \n",
    "    # Base XGBoost model\n",
    "    base_model = XGBClassifier(\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "        tree_method='hist',\n",
    "        eval_metric='logloss',\n",
    "    )\n",
    "    \n",
    "    if USE_RANDOMIZED_SEARCH:\n",
    "        search = RandomizedSearchCV(\n",
    "            base_model,\n",
    "            XGB_PARAM_GRID,\n",
    "            cv=skf,\n",
    "            scoring='f1',\n",
    "            n_iter=N_ITER_RANDOM,\n",
    "            random_state=SEED,\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "    else:\n",
    "        search = GridSearchCV(\n",
    "            base_model,\n",
    "            XGB_PARAM_GRID,\n",
    "            cv=skf,\n",
    "            scoring='f1',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "    \n",
    "    # Fit with error handling\n",
    "    start_time = time.time()\n",
    "    search.fit(X_full, y_full)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    best_model = search.best_estimator_\n",
    "    best_params = search.best_params_\n",
    "    best_cv_score = search.best_score_\n",
    "    \n",
    "    print(f'\\n‚úÖ Hyperparameter tuning complete ({elapsed_time/60:.1f} min)')\n",
    "    print(f'  Best CV F1: {best_cv_score:.4f}')\n",
    "    print(f'  Best parameters:')\n",
    "    for key, value in best_params.items():\n",
    "        print(f'    {key}: {value}')\n",
    "    \n",
    "    cleanup_memory()\n",
    "    memory_usage()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in hyperparameter tuning: {e}\")\n",
    "    print(\"‚ö†Ô∏è Using default parameters...\")\n",
    "    # Fallback to default model\n",
    "    best_model = XGBClassifier(\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "        tree_method='hist',\n",
    "        eval_metric='logloss',\n",
    "        \n",
    "        scale_pos_weight=(y_full == 0).sum() / max((y_full == 1).sum(), 1)\n",
    "    )\n",
    "    best_params = {}\n",
    "    best_cv_score = 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1963e4c4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 5. Threshold Tuning & Final Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c753730b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train final model on full data\n",
    "try:\n",
    "    print('\\n' + '='*80)\n",
    "    print('Training Final Model on Full Dataset')\n",
    "    print('='*80)\n",
    "    \n",
    "    # Use best parameters or defaults\n",
    "    final_model = XGBClassifier(**best_params, random_state=SEED, n_jobs=-1, tree_method='hist', eval_metric='logloss')\n",
    "    \n",
    "    final_model.fit(X_full, y_full)\n",
    "    \n",
    "    # Get predictions on validation set (original split)\n",
    "    y_val_proba = final_model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Find optimal threshold\n",
    "    thresholds = np.linspace(0.1, 0.9, 17)\n",
    "    best_threshold = 0.5\n",
    "    best_f1 = 0.0\n",
    "    \n",
    "    for thr in thresholds:\n",
    "        y_pred = (y_val_proba >= thr).astype(int)\n",
    "        f1 = f1_score(y_val, y_pred, pos_label=1, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = thr\n",
    "    \n",
    "    print(f'\\n‚úÖ Final Optimal Threshold: {best_threshold:.4f}')\n",
    "    print(f'‚úÖ Final Validation F1: {best_f1:.4f}')\n",
    "    \n",
    "    # Classification report\n",
    "    y_val_pred = (y_val_proba >= best_threshold).astype(int)\n",
    "    print('\\nüìä Classification Report:')\n",
    "    print(classification_report(y_val, y_val_pred, digits=4, zero_division=0))\n",
    "    \n",
    "    cleanup_memory()\n",
    "    memory_usage()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in final training: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5290cfc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 6. Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070e1539",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "try:\n",
    "    model_save_path = MODEL_SAVE_DIR / 'model5_xgboost_all_features_best.pkl'\n",
    "    \n",
    "    save_dict = {\n",
    "        'model': final_model,\n",
    "        'scaler': scaler if 'scaler' in locals() else None,\n",
    "        'minmax_scaler': minmax_scaler if 'minmax_scaler' in locals() else None,\n",
    "        'pca_models': pca_models_train if 'pca_models_train' in locals() else None,\n",
    "        'best_params': best_params,\n",
    "        'best_cv_score': best_cv_score,\n",
    "        'best_threshold': best_threshold,\n",
    "        'best_f1': best_f1,\n",
    "        'reg_cols': reg_cols,\n",
    "        'emb_family_to_cols': emb_family_to_cols\n",
    "    }\n",
    "    \n",
    "    with open(model_save_path, 'wb') as f:\n",
    "        pickle.dump(save_dict, f)\n",
    "    \n",
    "    print(f'\\nüíæ Model saved to: {model_save_path}')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving model: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1ac41b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 7. Generate Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34350070",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load test data and generate predictions\n",
    "try:\n",
    "    print('\\n' + '='*80)\n",
    "    print('Generating Test Predictions')\n",
    "    print('='*80)\n",
    "    \n",
    "    test_df = load_parquet_split('test')\n",
    "    test_ids = test_df['id'].to_numpy()\n",
    "    \n",
    "    # Process test data same as train\n",
    "    X_reg_test, X_emb_test_fams, _, _, _ = split_features_reg_and_all_emb(test_df)\n",
    "    del test_df\n",
    "    \n",
    "    # Apply PCA\n",
    "    X_emb_test_pca, _ = apply_pca_to_embeddings(X_emb_test_fams, fit_on_train=False, pca_models=pca_models_train)\n",
    "    \n",
    "    # Combine\n",
    "    if X_reg_test is not None:\n",
    "        X_test = np.hstack([X_reg_test, X_emb_test_pca])\n",
    "    else:\n",
    "        X_test = X_emb_test_pca\n",
    "    \n",
    "    del X_reg_test, X_emb_test_fams, X_emb_test_pca\n",
    "    cleanup_memory()\n",
    "    \n",
    "    # Scale\n",
    "    if 'scaler' in locals():\n",
    "        X_test = scaler.transform(X_test)\n",
    "        if 'minmax_scaler' in locals():\n",
    "            X_test = minmax_scaler.transform(X_test)\n",
    "    \n",
    "    # Predict in chunks for OOM protection\n",
    "    chunk_size = 10000\n",
    "    if X_test.shape[0] > chunk_size:\n",
    "        print(f'  Predicting in chunks (size={chunk_size}) for OOM protection...')\n",
    "        y_test_proba_chunks = []\n",
    "        for i in range(0, X_test.shape[0], chunk_size):\n",
    "            chunk_proba = final_model.predict_proba(X_test[i:i+chunk_size])[:, 1]\n",
    "            y_test_proba_chunks.append(chunk_proba)\n",
    "            del chunk_proba\n",
    "            cleanup_memory()\n",
    "        y_test_proba = np.concatenate(y_test_proba_chunks)\n",
    "        del y_test_proba_chunks\n",
    "    else:\n",
    "        y_test_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    y_test_pred = (y_test_proba >= best_threshold).astype(int)\n",
    "    \n",
    "    # Create submission using Polars (not pandas)\n",
    "    submission_df = pl.DataFrame({\n",
    "        'id': test_ids,\n",
    "        'label': y_test_pred\n",
    "    })\n",
    "    \n",
    "    submission_path = SUBMISSION_DIR / 'submission_model5.csv'\n",
    "    submission_df.write_csv(submission_path)\n",
    "    \n",
    "    print(f'\\n‚úÖ Submission saved to: {submission_path}')\n",
    "    print(f'  Test predictions: {len(y_test_pred)}, Positive: {y_test_pred.sum()}, Negative: {(y_test_pred==0).sum()}')\n",
    "    \n",
    "    cleanup_memory()\n",
    "    memory_usage()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error generating submission: {e}\")\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "src/notebooks/model5_xgboost_all_features.ipynb",
   "output_path": "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/runs/model5_executed_20251118-190222.ipynb",
   "parameters": {},
   "start_time": "2025-11-19T00:02:30.104807",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}