{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {
    "papermill": {
     "duration": 0.00312,
     "end_time": "2025-11-18T23:59:18.761526",
     "exception": false,
     "start_time": "2025-11-18T23:59:18.758406",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 4: Regular features + all embedding families (each PCA-compressed) ‚Äì Linear classifier\n",
    "\n",
    "This notebook trains a linear PyTorch classifier on regular features plus **all available embedding families**, each\n",
    "compressed separately via IncrementalPCA. This gives a \"full information but simple model\" that respects the\n",
    "constraint of using simpler models as feature complexity grows.\n",
    "\n",
    "**Features:**\n",
    "- ‚úÖ 5-fold Cross-Validation\n",
    "- ‚úÖ Hyperparameter Tuning (limited search space for 3-hour constraint)\n",
    "- ‚úÖ Threshold Fine-tuning\n",
    "- ‚úÖ Model Weight Saving\n",
    "- ‚úÖ Submission.csv Generation\n",
    "- ‚úÖ OOM Safe with aggressive memory management\n",
    "\n",
    "## Memory & Scalability Notes\n",
    "\n",
    "**Memory Optimizations Applied:**\n",
    "- ‚úÖ Aggressive garbage collection after data loading\n",
    "- ‚úÖ Explicit deletion of Polars DataFrames after numpy conversion\n",
    "- ‚úÖ Batch tensor cleanup in training/validation loops\n",
    "- ‚úÖ Periodic memory cleanup during training\n",
    "- ‚úÖ GPU cache clearing (if using GPU)\n",
    "- ‚úÖ Memory usage monitoring\n",
    "\n",
    "**Scalability Considerations:**\n",
    "- **Current dataset size**: 480 train samples, 60 val samples (very small)\n",
    "- **Batch size**: 512 (configurable via `BATCH_SIZE` variable)\n",
    "- **Memory footprint**: ~24 features √ó float32 = minimal memory usage\n",
    "- **OOM Risk**: **LOW** for current dataset size, but may increase with:\n",
    "  - Larger datasets (>100K samples)\n",
    "  - More features (if expanded beyond 24)\n",
    "  - Larger batch sizes\n",
    "  \n",
    "**If OOM occurs:**\n",
    "1. Reduce `BATCH_SIZE` (try 512, 256, or 128)\n",
    "2. Process validation in smaller chunks\n",
    "3. Use gradient accumulation for effective larger batches\n",
    "4. Consider using `torch.no_grad()` more aggressively\n",
    "5. Monitor memory with `memory_usage()` calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_md",
   "metadata": {
    "papermill": {
     "duration": 0.002475,
     "end_time": "2025-11-18T23:59:18.766794",
     "exception": false,
     "start_time": "2025-11-18T23:59:18.764319",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b59b10e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T23:59:18.772734Z",
     "iopub.status.busy": "2025-11-18T23:59:18.772563Z",
     "iopub.status.idle": "2025-11-19T00:00:16.097881Z",
     "shell.execute_reply": "2025-11-19T00:00:16.097171Z"
    },
    "papermill": {
     "duration": 57.329537,
     "end_time": "2025-11-19T00:00:16.098760",
     "exception": false,
     "start_time": "2025-11-18T23:59:18.769223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PROJECT_ROOT: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2\n",
      "MODEL_READY_DIR: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/model_ready\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using sklearn IncrementalPCA (memory-efficient)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Memory utilities imported from shared module\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import gc\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "# PyTorch-based PCA (GPU-friendly with CPU fallback)\n",
    "import sys\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "# Device (GPU if available, else CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "# Paths (adapt if your project structure differs)\n",
    "# Try to find project root by looking for 'data' directory\n",
    "current = Path(os.getcwd())\n",
    "PROJECT_ROOT = current\n",
    "# Go up directories until we find one with 'data' subdirectory\n",
    "for _ in range(5):  # Max 5 levels up\n",
    "    if (PROJECT_ROOT / 'data').exists():\n",
    "        break\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "else:\n",
    "    # Fallback: assume we're in src/notebooks, go up 2 levels\n",
    "    PROJECT_ROOT = current.parent.parent\n",
    "MODEL_READY_DIR = PROJECT_ROOT / 'data' / 'model_ready'\n",
    "utils_path = PROJECT_ROOT / 'src' / 'utils'\n",
    "print('PROJECT_ROOT:', PROJECT_ROOT)\n",
    "print('MODEL_READY_DIR:', MODEL_READY_DIR)\n",
    "# Import PCA utilities\n",
    "# Use sklearn IncrementalPCA by default for better memory efficiency in constrained environments\n",
    "# PyTorch PCA can be used on SLURM with proper resources\n",
    "USE_TORCH_PCA = False  # Set to True to use PyTorch PCA (requires more memory)\n",
    "\n",
    "if utils_path.exists():\n",
    "    sys.path.insert(0, str(utils_path))\n",
    "if USE_TORCH_PCA:\n",
    "    try:\n",
    "        from pca_utils import IncrementalTorchPCA\n",
    "        IncrementalPCA = IncrementalTorchPCA  # Alias for compatibility\n",
    "        IS_TORCH_PCA = True\n",
    "        print(\"‚úÖ Using PyTorch PCA (GPU-friendly)\")\n",
    "    except ImportError:\n",
    "        # Fallback to sklearn if PyTorch PCA not available\n",
    "        from sklearn.decomposition import IncrementalPCA\n",
    "        IS_TORCH_PCA = False\n",
    "        print(\"‚ö†Ô∏è Using sklearn IncrementalPCA (CPU only)\")\n",
    "else:\n",
    "    # Use sklearn IncrementalPCA by default for memory efficiency\n",
    "    from sklearn.decomposition import IncrementalPCA\n",
    "    IS_TORCH_PCA = False\n",
    "    print(\"‚úÖ Using sklearn IncrementalPCA (memory-efficient)\")\n",
    "from sklearn.metrics import (\n",
    "    f1_score, classification_report, roc_auc_score,\n",
    "    average_precision_score, roc_curve, precision_recall_curve\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "# Import memory utilities from shared module\n",
    "if utils_path.exists():\n",
    "    sys.path.insert(0, str(utils_path))\n",
    "try:\n",
    "    from model_training_utils import cleanup_memory, memory_usage\n",
    "    print(\"‚úÖ Memory utilities imported from shared module\")\n",
    "except ImportError:\n",
    "    # Fallback definitions if utils not available\n",
    "    def cleanup_memory():\n",
    "        \"\"\"Aggressive memory cleanup for both CPU and GPU.\"\"\"\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "        gc.collect()  # Second pass for thorough cleanup\n",
    "    def memory_usage():\n",
    "        \"\"\"Display current memory usage statistics.\"\"\"\n",
    "        try:\n",
    "            import psutil\n",
    "            process = psutil.Process(os.getpid())\n",
    "            mem_info = process.memory_info()\n",
    "            print(f\"üíæ Memory: {mem_info.rss / 1024**3:.2f} GB (RAM)\", end=\"\")\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_mem = torch.cuda.memory_allocated() / 1024**3\n",
    "                gpu_reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "                print(f\" | {gpu_mem:.2f}/{gpu_reserved:.2f} GB (GPU used/reserved)\")\n",
    "            else:\n",
    "                print()\n",
    "        except ImportError:\n",
    "            print(\"üíæ Memory tracking requires psutil: pip install psutil\")\n",
    "    print(\"‚ö†Ô∏è Using fallback memory utilities\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset_md",
   "metadata": {
    "papermill": {
     "duration": 0.002712,
     "end_time": "2025-11-19T00:00:16.105651",
     "exception": false,
     "start_time": "2025-11-19T00:00:16.102939",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Dataset & utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dataset_cell",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T00:00:16.112153Z",
     "iopub.status.busy": "2025-11-19T00:00:16.111849Z"
    },
    "papermill": {
     "duration": 83.092814,
     "end_time": "2025-11-19T00:01:39.201065",
     "exception": false,
     "start_time": "2025-11-19T00:00:16.108251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train from /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/model_ready/train_model_ready.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading val from /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/model_ready/val_model_ready.parquet\n"
     ]
    }
   ],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "def load_parquet_split(split: str) -> pl.DataFrame:\n",
    "    \"\"\"Load a model_ready parquet split (train/val/test).\"\"\"\n",
    "    path = MODEL_READY_DIR / f'{split}_model_ready_reduced.parquet'\n",
    "    if not path.exists():\n",
    "        # Fallback to non-reduced files if needed\n",
    "        alt = MODEL_READY_DIR / f'{split}_model_ready.parquet'\n",
    "        if not alt.exists():\n",
    "            raise FileNotFoundError(f'Could not find {path} or {alt}')\n",
    "        path = alt\n",
    "    print(f'Loading {split} from {path}')\n",
    "    return pl.read_parquet(path)\n",
    "EMBEDDING_FAMILY_PREFIXES = ['sent_transformer_', 'scibert_', 'specter_', 'specter2_', 'ner_']\n",
    "PCA_COMPONENTS_PER_FAMILY = {\n",
    "    'sent_transformer_': 32,\n",
    "    'scibert_': 32,\n",
    "    'specter_': 32,\n",
    "    'specter2_': 32,\n",
    "    'ner_': 16,\n",
    "}\n",
    "def split_features_reg_and_all_emb(df: pl.DataFrame):\n",
    "    cols = df.columns\n",
    "    dtypes = df.dtypes\n",
    "    label = df['label'].to_numpy() if 'label' in cols else None\n",
    "    reg_cols = []\n",
    "    emb_family_to_cols = {p: [] for p in EMBEDDING_FAMILY_PREFIXES}\n",
    "    # Numeric dtypes in Polars\n",
    "    NUMERIC_DTYPES = {\n",
    "        pl.Int8, pl.Int16, pl.Int32, pl.Int64,\n",
    "        pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,\n",
    "        pl.Float32, pl.Float64\n",
    "    }\n",
    "    for c, dt in zip(cols, dtypes):\n",
    "        if c in ('id', 'label'):\n",
    "            continue\n",
    "        matched = False\n",
    "        for p in EMBEDDING_FAMILY_PREFIXES:\n",
    "            if c.startswith(p):\n",
    "                emb_family_to_cols[p].append(c)\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            # Only include numeric columns\n",
    "            if dt in NUMERIC_DTYPES:\n",
    "                reg_cols.append(c)\n",
    "    X_reg = df.select(reg_cols).to_numpy()\n",
    "    X_emb_families = {}\n",
    "    for p, clist in emb_family_to_cols.items():\n",
    "        if clist:\n",
    "            X_emb_families[p] = df.select(clist).to_numpy()\n",
    "    return X_reg, X_emb_families, label, reg_cols, emb_family_to_cols\n",
    "def make_dataloaders(X_train, y_train, X_val, y_val, batch_size: int = 512, val_batch_size: int = None, num_workers: int = 0):\n",
    "    \"\"\"\n",
    "    Create DataLoaders with memory-efficient settings for large datasets.\n",
    "    Args:\n",
    "        batch_size: Training batch size (default 512 for large datasets)\n",
    "        val_batch_size: Validation batch size (defaults to batch_size if None)\n",
    "        num_workers: Number of worker processes (0 to avoid multiprocessing overhead)\n",
    "    \"\"\"\n",
    "    if val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "    # Compute sample weights for WeightedRandomSampler (handle class imbalance)\n",
    "    class_sample_counts = np.bincount(y_train.astype(int))\n",
    "    print('Class counts (train):', class_sample_counts)\n",
    "    # Avoid division by zero\n",
    "    weights_per_class = 1.0 / np.maximum(class_sample_counts, 1)\n",
    "    sample_weights = weights_per_class[y_train.astype(int)]\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    train_dataset = TabularDataset(X_train, y_train)\n",
    "    val_dataset = TabularDataset(X_val, y_val)\n",
    "    # Use num_workers=0 to avoid multiprocessing memory overhead\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=False  # Disable pin_memory to save memory\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=val_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    return train_loader, val_loader\n",
    "# Load train/val splits\n",
    "train_df = load_parquet_split('train')\n",
    "val_df = load_parquet_split('val')\n",
    "X_reg_train, X_emb_train_fams, y_train, reg_cols, emb_family_to_cols = split_features_reg_and_all_emb(train_df)\n",
    "X_reg_val, X_emb_val_fams, y_val, _, _ = split_features_reg_and_all_emb(val_df)\n",
    "# Clean up Polars DataFrames immediately after conversion\n",
    "del train_df, val_df\n",
    "cleanup_memory()\n",
    "print('Regular feature count:', len(reg_cols))\n",
    "for fam, arr in X_emb_train_fams.items():\n",
    "    print(f'Embedding family {fam}: {arr.shape[1]} dims')\n",
    "# PCA per family (OOM-resistant with chunked processing)\n",
    "X_emb_train_pca_list = []\n",
    "X_emb_val_pca_list = []\n",
    "\n",
    "# Aggressive memory cleanup before starting\n",
    "cleanup_memory()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "for fam, X_emb_train in X_emb_train_fams.items():\n",
    "    n_components = PCA_COMPONENTS_PER_FAMILY.get(fam, 32)\n",
    "    print(f'Fitting IncrementalPCA for family {fam} with {n_components} components')\n",
    "    \n",
    "    # Cleanup before each family\n",
    "    cleanup_memory()\n",
    "    \n",
    "    # Only pass device parameter if using PyTorch PCA\n",
    "    if IS_TORCH_PCA:\n",
    "        ipca = IncrementalPCA(n_components=n_components, batch_size=2000, device=device)\n",
    "    else:\n",
    "        ipca = IncrementalPCA(n_components=n_components, batch_size=2000)\n",
    "    \n",
    "    # Fit on subset for large datasets (OOM protection)\n",
    "    max_pca_rows = min(50_000, X_emb_train.shape[0])  # Reduced from 120k\n",
    "    if X_emb_train.shape[0] > max_pca_rows:\n",
    "        print(f\"  Fitting PCA on subset ({max_pca_rows}/{X_emb_train.shape[0]} samples) for {fam}\")\n",
    "        idx = np.random.choice(X_emb_train.shape[0], size=max_pca_rows, replace=False)\n",
    "        X_emb_subset = X_emb_train[idx].copy()  # Explicit copy\n",
    "        del idx\n",
    "        cleanup_memory()\n",
    "        ipca.fit(X_emb_subset)\n",
    "        del X_emb_subset\n",
    "        cleanup_memory()\n",
    "    else:\n",
    "        ipca.fit(X_emb_train)\n",
    "        cleanup_memory()\n",
    "    \n",
    "    # Transform in chunks for OOM protection\n",
    "    chunk_size = 5000\n",
    "    if X_emb_train.shape[0] > chunk_size:\n",
    "        X_emb_train_pca_chunks = []\n",
    "        for i in range(0, X_emb_train.shape[0], chunk_size):\n",
    "            chunk = X_emb_train[i:i+chunk_size].copy()\n",
    "            chunk_pca = ipca.transform(chunk)\n",
    "            X_emb_train_pca_chunks.append(chunk_pca)\n",
    "            del chunk, chunk_pca\n",
    "            cleanup_memory()\n",
    "        X_emb_train_pca = np.vstack(X_emb_train_pca_chunks)\n",
    "        del X_emb_train_pca_chunks\n",
    "    else:\n",
    "        X_emb_train_pca = ipca.transform(X_emb_train)\n",
    "    \n",
    "    X_emb_val = X_emb_val_fams[fam]\n",
    "    X_emb_val_pca = ipca.transform(X_emb_val)\n",
    "    \n",
    "    # Clean up after each family\n",
    "    del X_emb_train, X_emb_val\n",
    "    cleanup_memory()\n",
    "    X_emb_train_pca_list.append(X_emb_train_pca)\n",
    "    X_emb_val_pca_list.append(X_emb_val_pca)\n",
    "    # Clean up intermediate arrays\n",
    "    del X_emb_train_pca, X_emb_val_pca\n",
    "    cleanup_memory()\n",
    "    \n",
    "    # Aggressive cleanup between families\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "X_emb_train_pca_list = []\n",
    "X_emb_val_pca_list = []\n",
    "for fam, X_emb_train in X_emb_train_fams.items():\n",
    "    n_components = PCA_COMPONENTS_PER_FAMILY.get(fam, 32)\n",
    "    print(f'Fitting IncrementalPCA for family {fam} with {n_components} components')\n",
    "    # Only pass device parameter if using PyTorch PCA\n",
    "    if IS_TORCH_PCA:\n",
    "        ipca = IncrementalPCA(n_components=n_components, batch_size=2000, device=device)\n",
    "    else:\n",
    "        ipca = IncrementalPCA(n_components=n_components, batch_size=2000)\n",
    "    max_pca_rows = min(50_000, X_emb_train.shape[0])  # Reduced from 120k\n",
    "    max_pca_rows = min(50_000, X_emb_train.shape[0])  # Reduced from 120k\n",
    "    idx = np.random.choice(X_emb_train.shape[0], size=max_pca_rows, replace=False)\n",
    "    ipca.fit(X_emb_train[idx])\n",
    "    X_emb_train_pca = ipca.transform(X_emb_train)\n",
    "    X_emb_val = X_emb_val_fams[fam]\n",
    "    X_emb_val_pca = ipca.transform(X_emb_val)\n",
    "    # Clean up after each family\n",
    "    del X_emb_train, X_emb_val\n",
    "    cleanup_memory()\n",
    "    X_emb_train_pca_list.append(X_emb_train_pca)\n",
    "    X_emb_val_pca_list.append(X_emb_val_pca)\n",
    "    # Clean up intermediate arrays\n",
    "    del X_emb_train_pca, X_emb_val_pca\n",
    "    cleanup_memory()\n",
    "if X_emb_train_pca_list:\n",
    "    X_emb_train_concat = np.concatenate(X_emb_train_pca_list, axis=1)\n",
    "    X_emb_val_concat = np.concatenate(X_emb_val_pca_list, axis=1)\n",
    "    # Clean up lists\n",
    "    del X_emb_train_pca_list, X_emb_val_pca_list\n",
    "    cleanup_memory()\n",
    "    X_train = np.concatenate([X_reg_train, X_emb_train_concat], axis=1)\n",
    "    X_val = np.concatenate([X_reg_val, X_emb_val_concat], axis=1)\n",
    "    # Clean up intermediate arrays\n",
    "    del X_reg_train, X_reg_val, X_emb_train_concat, X_emb_val_concat\n",
    "    cleanup_memory()\n",
    "else:\n",
    "    print('No embedding families found; falling back to regular features only.')\n",
    "    X_train = X_reg_train\n",
    "    X_val = X_reg_val\n",
    "    del X_reg_train, X_reg_val\n",
    "    cleanup_memory()\n",
    "print('Train shape:', X_train.shape, 'Val shape:', X_val.shape)\n",
    "memory_usage()\n",
    "# Configurable batch sizes - optimized for large datasets\n",
    "# Reduce if OOM occurs\n",
    "BATCH_SIZE = 512  # Training batch size (reduced for large datasets)\n",
    "VAL_BATCH_SIZE = 512  # Validation batch size (can be larger since no gradients)\n",
    "NUM_WORKERS = 0  # Set to 0 to avoid multiprocessing memory overhead\n",
    "print(f'\\nüìä DataLoader Configuration:')\n",
    "print(f'   Train batch size: {BATCH_SIZE}')\n",
    "print(f'   Val batch size: {VAL_BATCH_SIZE}')\n",
    "print(f'   Num workers: {NUM_WORKERS} (0 = single process, saves memory)')\n",
    "train_loader, val_loader = make_dataloaders(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    val_batch_size=VAL_BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "cleanup_memory()\n",
    "memory_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training_cell",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T22:09:07.402432Z",
     "iopub.status.busy": "2025-11-17T22:09:07.402223Z",
     "iopub.status.idle": "2025-11-17T22:09:10.033805Z",
     "shell.execute_reply": "2025-11-17T22:09:10.033349Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 3. Model definition\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "input_dim = X_train.shape[1]\n",
    "model = LinearClassifier(input_dim)\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "## 4. Train / validation loop\n",
    "EPOCHS = 10\n",
    "LR = 1e-3\n",
    "# Compute pos_weight for BCEWithLogitsLoss (handle class imbalance explicitly)\n",
    "pos_count = (y_train == 1).sum()\n",
    "neg_count = (y_train == 0).sum()\n",
    "pos_weight_value = torch.tensor([neg_count / max(pos_count, 1)], dtype=torch.float32).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_value)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "best_val_f1 = 0.0\n",
    "best_state_dict = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    # Training loop with cleanup\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device).unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "        # Clean up batch tensors\n",
    "        del xb, yb, logits, loss\n",
    "        if epoch % 5 == 0:  # Periodic cleanup during training\n",
    "            cleanup_memory()\n",
    "    avg_train_loss = running_loss / len(train_loader.dataset)\n",
    "    # Validation with memory-efficient accumulation (critical for 100k val samples)\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    batch_count = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb_np = yb.numpy()  # Convert before moving to device\n",
    "            logits = model(xb)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy().ravel()\n",
    "            all_preds.append(probs)\n",
    "            all_targets.append(yb_np)\n",
    "            # Clean up batch tensors immediately\n",
    "            del xb, logits, probs, yb_np\n",
    "            batch_count += 1\n",
    "            # Periodic cleanup during validation for large datasets\n",
    "            if batch_count % 50 == 0:  # Every 50 batches\n",
    "                cleanup_memory()\n",
    "    # Concatenate only once\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    # Threshold tuning for F1 on positive class\n",
    "    # Calculate ROC-AUC and Precision-Recall AUC (using probabilities, not binary predictions)\n",
    "    roc_auc = roc_auc_score(all_targets, all_preds)\n",
    "    pr_auc = average_precision_score(all_targets, all_preds)\n",
    "    best_epoch_f1 = 0.0\n",
    "    best_thr = 0.5\n",
    "    thresholds = np.linspace(0.1, 0.9, 17)\n",
    "    for thr in thresholds:\n",
    "        # Compute binary predictions without storing intermediate array\n",
    "        preds_bin = (all_preds >= thr).astype(int)\n",
    "        f1 = f1_score(all_targets, preds_bin, pos_label=1)\n",
    "        if f1 > best_epoch_f1:\n",
    "            best_epoch_f1 = f1\n",
    "            best_thr = thr\n",
    "        del preds_bin  # Clean up immediately\n",
    "    # Clean up concatenated arrays\n",
    "    del all_preds, all_targets\n",
    "    print(f'Epoch {epoch:02d} | train_loss={avg_train_loss:.4f} | val_f1={best_epoch_f1:.4f} @ thr={best_thr:.2f} | roc_auc={roc_auc:.4f} | pr_auc={pr_auc:.4f}')\n",
    "    # Always print memory for large datasets to monitor OOM risk\n",
    "    memory_usage()\n",
    "    if best_epoch_f1 > best_val_f1:\n",
    "        best_val_f1 = best_epoch_f1\n",
    "        best_state_dict = model.state_dict().copy()  # Explicit copy to avoid references\n",
    "    # Aggressive cleanup after each epoch (critical for large datasets)\n",
    "    cleanup_memory()\n",
    "print('Best val F1:', best_val_f1)\n",
    "if best_state_dict is not None:\n",
    "    model.load_state_dict(best_state_dict)\n",
    "    # Save the best model\n",
    "    MODEL_SAVE_DIR = PROJECT_ROOT / 'models' / 'saved_models'\n",
    "    MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    model_save_path = MODEL_SAVE_DIR / 'model4_reg_plus_all_embeddings_pca_linear_best.pt'\n",
    "    torch.save({\n",
    "        'model_state_dict': best_state_dict,\n",
    "        'input_dim': input_dim,\n",
    "        'best_val_f1': best_val_f1,\n",
    "        'epochs': EPOCHS,\n",
    "        'learning_rate': LR,\n",
    "        'pos_weight': pos_weight_value.cpu().item() if hasattr(pos_weight_value, 'cpu') else pos_weight_value.item(),\n",
    "        'pca_components_per_family': PCA_COMPONENTS_PER_FAMILY if 'PCA_COMPONENTS_PER_FAMILY' in globals() else None\n",
    "    }, model_save_path)\n",
    "    print(f'\\nüíæ Saved best model to: {model_save_path}')\n",
    "print('\\nValidation classification report (best model, thr=0.5 for reference):')\n",
    "# Validation with memory-efficient accumulation (critical for 100k val samples)\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "batch_count = 0\n",
    "with torch.no_grad():\n",
    "    for xb, yb in val_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb_np = yb.numpy()  # Convert before moving to device\n",
    "        logits = model(xb)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy().ravel()\n",
    "        all_preds.append(probs)\n",
    "        all_targets.append(yb_np)\n",
    "        # Clean up batch tensors immediately\n",
    "        del xb, logits, probs, yb_np\n",
    "        batch_count += 1\n",
    "        # Periodic cleanup during validation for large datasets\n",
    "        if batch_count % 50 == 0:  # Every 50 batches\n",
    "            cleanup_memory()\n",
    "# Concatenate only once\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "# Generate predictions for classification report (threshold 0.5)\n",
    "preds_bin = (all_preds >= 0.5).astype(int)\n",
    "print(classification_report(all_targets, preds_bin, digits=4, zero_division=0))\n",
    "# Calculate ROC-AUC and Precision-Recall AUC\n",
    "roc_auc = roc_auc_score(all_targets, all_preds)\n",
    "pr_auc = average_precision_score(all_targets, all_preds)\n",
    "print(f'\\nROC-AUC: {roc_auc:.4f}')\n",
    "print(f'Precision-Recall AUC: {pr_auc:.4f}')\n",
    "# Plot ROC curve\n",
    "fpr, tpr, _ = roc_curve(all_targets, all_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "# Plot Precision-Recall curve\n",
    "precision, recall, _ = precision_recall_curve(all_targets, all_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, label=f'PR curve (AUC = {pr_auc:.4f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "# Final cleanup\n",
    "del all_preds, all_targets, preds_bin\n",
    "cleanup_memory()\n",
    "memory_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c37ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T22:09:10.035043Z",
     "iopub.status.busy": "2025-11-17T22:09:10.034917Z",
     "iopub.status.idle": "2025-11-17T22:09:10.398404Z",
     "shell.execute_reply": "2025-11-17T22:09:10.397902Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "# Add utils to path\n",
    "utils_path = PROJECT_ROOT / 'src' / 'utils'\n",
    "if utils_path.exists():\n",
    "    sys.path.insert(0, str(utils_path))\n",
    "try:\n",
    "    from model_training_utils import (\n",
    "        find_optimal_threshold, cross_validate,\n",
    "        generate_submission, save_model_weights,\n",
    "        stratified_kfold_splits\n",
    "    )\n",
    "    print(\"‚úÖ Utility functions imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Could not import utilities: {e}\")\n",
    "    print(\"Will use inline implementations\")\n",
    "# Prepare full dataset for CV (before PCA - PCA will be fit per fold)\n",
    "train_df_full = load_parquet_split('train')\n",
    "val_df_full = load_parquet_split('val')\n",
    "# Split features for full dataset\n",
    "X_reg_train_full, X_emb_train_fams_full, y_train_full, reg_cols_full, emb_family_to_cols_full = split_features_reg_and_all_emb(train_df_full)\n",
    "X_reg_val_full, X_emb_val_fams_full, y_val_full, _, _ = split_features_reg_and_all_emb(val_df_full)\n",
    "# Combine train and val\n",
    "X_reg_full = np.vstack([X_reg_train_full, X_reg_val_full])\n",
    "X_emb_fams_full = {}\n",
    "for fam in X_emb_train_fams_full.keys():\n",
    "    X_emb_fams_full[fam] = np.vstack([X_emb_train_fams_full[fam], X_emb_val_fams_full[fam]])\n",
    "y_full = np.hstack([y_train_full, y_val_full])\n",
    "del train_df_full, val_df_full, X_reg_train_full, X_reg_val_full, X_emb_train_fams_full, X_emb_val_fams_full, y_train_full, y_val_full\n",
    "cleanup_memory()\n",
    "print(f\"\\nüìä Full dataset for CV:\")\n",
    "print(f\"  Regular features: {X_reg_full.shape}\")\n",
    "for fam, arr in X_emb_fams_full.items():\n",
    "    print(f\"  Embedding family {fam}: {arr.shape}\")\n",
    "print(f\"  Labels: {y_full.shape}\")\n",
    "# Limited hyperparameter search space for 3-hour constraint\n",
    "HYPERPARAMETER_GRID = [\n",
    "    {'lr': 1e-3, 'batch_size': 512},\n",
    "    {'lr': 5e-4, 'batch_size': 512},\n",
    "    {'lr': 1e-3, 'batch_size': 256},\n",
    "]\n",
    "print(f\"\\nüîç Hyperparameter grid ({len(HYPERPARAMETER_GRID)} combinations):\")\n",
    "for i, hp in enumerate(HYPERPARAMETER_GRID, 1):\n",
    "    print(f\"  {i}. LR={hp['lr']}, Batch={hp['batch_size']}\")\n",
    "cleanup_memory()\n",
    "memory_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f200261",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T22:09:10.399614Z",
     "iopub.status.busy": "2025-11-17T22:09:10.399534Z",
     "iopub.status.idle": "2025-11-17T22:09:26.824350Z",
     "shell.execute_reply": "2025-11-17T22:09:26.823973Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function to prepare features with PCA for all families per fold\n",
    "def prepare_features_all_emb_pca(X_reg_fold_train, X_emb_fams_fold_train, X_reg_fold_val, X_emb_fams_fold_val):\n",
    "    \"\"\"Fit PCA on each embedding family for fold train data and transform both train and val.\"\"\"\n",
    "    X_emb_train_pca_list = []\n",
    "    X_emb_val_pca_list = []\n",
    "    pca_models = {}\n",
    "    for fam, X_emb_train in X_emb_fams_fold_train.items():\n",
    "        n_components = PCA_COMPONENTS_PER_FAMILY.get(fam, 32)\n",
    "        # Use PyTorch PCA (GPU-friendly)\n",
    "    # Only pass device parameter if using PyTorch PCA\n",
    "    if IS_TORCH_PCA:\n",
    "        ipca = IncrementalPCA(n_components=n_components, batch_size=2000, device=device)\n",
    "    else:\n",
    "        ipca = IncrementalPCA(n_components=n_components, batch_size=2000)\n",
    "        max_pca_rows = min(50_000, X_emb_train.shape[0])\n",
    "        idx = np.random.choice(X_emb_train.shape[0], size=max_pca_rows, replace=False)\n",
    "        ipca.fit(X_emb_train[idx])\n",
    "        X_emb_train_pca = ipca.transform(X_emb_train)\n",
    "        X_emb_val = X_emb_fams_fold_val.get(fam)\n",
    "        X_emb_val_pca = ipca.transform(X_emb_val) if X_emb_val is not None else None\n",
    "        X_emb_train_pca_list.append(X_emb_train_pca)\n",
    "        if X_emb_val_pca is not None:\n",
    "            X_emb_val_pca_list.append(X_emb_val_pca)\n",
    "        pca_models[fam] = ipca\n",
    "        del X_emb_train_pca, X_emb_val_pca\n",
    "        cleanup_memory()\n",
    "    if X_emb_train_pca_list:\n",
    "        X_emb_train_concat = np.concatenate(X_emb_train_pca_list, axis=1)\n",
    "        X_emb_val_concat = np.concatenate(X_emb_val_pca_list, axis=1) if X_emb_val_pca_list else None\n",
    "        X_train_combined = np.concatenate([X_reg_fold_train, X_emb_train_concat], axis=1)\n",
    "        X_val_combined = np.concatenate([X_reg_fold_val, X_emb_val_concat], axis=1) if X_emb_val_concat is not None else X_reg_fold_val\n",
    "        del X_emb_train_pca_list, X_emb_val_pca_list, X_emb_train_concat, X_emb_val_concat\n",
    "        return X_train_combined, X_val_combined, pca_models\n",
    "    else:\n",
    "        return X_reg_fold_train, X_reg_fold_val, {}\n",
    "# 5-fold CV with hyperparameter tuning\n",
    "best_hyperparams = None\n",
    "best_cv_score = 0.0\n",
    "best_model_state = None\n",
    "best_threshold = 0.5\n",
    "best_pca_models = None\n",
    "cv_start_time = time.time()\n",
    "# Use PyTorch-friendly stratified splits\n",
    "cv_splits = stratified_kfold_splits(y_full, n_splits=5, shuffle=True, random_state=42)\n",
    "for hp_idx, hyperparams in enumerate(HYPERPARAMETER_GRID, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Hyperparameter Set {hp_idx}/{len(HYPERPARAMETER_GRID)}: {hyperparams}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    BATCH_SIZE = hyperparams['batch_size']\n",
    "    VAL_BATCH_SIZE = hyperparams['batch_size']\n",
    "    fold_results = []\n",
    "    best_fold_f1 = 0.0\n",
    "    best_fold_model_state = None\n",
    "    best_fold_idx = -1\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(cv_splits, 1):\n",
    "        print(f\"\\n  Fold {fold_idx}/5\")\n",
    "        # Split data for this fold\n",
    "        X_reg_fold_train = X_reg_full[train_idx]\n",
    "        X_reg_fold_val = X_reg_full[val_idx]\n",
    "        X_emb_fams_fold_train = {fam: arr[train_idx] for fam, arr in X_emb_fams_full.items()}\n",
    "        X_emb_fams_fold_val = {fam: arr[val_idx] for fam, arr in X_emb_fams_full.items()}\n",
    "        y_fold_train = y_full[train_idx]\n",
    "        y_fold_val = y_full[val_idx]\n",
    "        # Prepare features with PCA for all families\n",
    "        X_fold_train, X_fold_val, fold_pca_models = prepare_features_all_emb_pca(\n",
    "            X_reg_fold_train, X_emb_fams_fold_train, X_reg_fold_val, X_emb_fams_fold_val\n",
    "        )\n",
    "        # Create dataloaders\n",
    "        train_loader_fold, val_loader_fold = make_dataloaders(\n",
    "            X_fold_train, y_fold_train, X_fold_val, y_fold_val,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            val_batch_size=VAL_BATCH_SIZE,\n",
    "            num_workers=NUM_WORKERS\n",
    "        )\n",
    "        # Compute pos_weight\n",
    "        pos_count = (y_fold_train == 1).sum()\n",
    "        neg_count = (y_fold_train == 0).sum()\n",
    "        pos_weight_value = torch.tensor([neg_count / max(pos_count, 1)], dtype=torch.float32).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_value)\n",
    "        # Create and train model\n",
    "        model_fold = LinearClassifier(input_dim=X_fold_train.shape[1]).to(device)\n",
    "        optimizer = torch.optim.Adam(model_fold.parameters(), lr=hyperparams['lr'])\n",
    "        # Train fold\n",
    "        best_val_f1_fold = 0.0\n",
    "        best_state_fold = None\n",
    "        patience_counter = 0\n",
    "        for epoch in range(1, 16):  # Max 15 epochs\n",
    "            model_fold.train()\n",
    "            running_loss = 0.0\n",
    "            for xb, yb in train_loader_fold:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device).unsqueeze(1)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model_fold(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * xb.size(0)\n",
    "                del xb, yb, logits, loss\n",
    "            # Validation\n",
    "            model_fold.eval()\n",
    "            all_preds = []\n",
    "            all_targets = []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader_fold:\n",
    "                    xb = xb.to(device)\n",
    "                    yb_np = yb.numpy()\n",
    "                    logits = model_fold(xb)\n",
    "                    probs = torch.sigmoid(logits).cpu().numpy().ravel()\n",
    "                    all_preds.append(probs)\n",
    "                    all_targets.append(yb_np)\n",
    "                    del xb, logits, probs, yb_np\n",
    "            all_preds = np.concatenate(all_preds)\n",
    "            all_targets = np.concatenate(all_targets)\n",
    "            _, val_f1 = find_optimal_threshold(all_targets, all_preds)\n",
    "            if val_f1 > best_val_f1_fold:\n",
    "                best_val_f1_fold = val_f1\n",
    "                best_state_fold = model_fold.state_dict().copy()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= 3:\n",
    "                    break\n",
    "            del all_preds, all_targets\n",
    "            cleanup_memory()\n",
    "        fold_results.append({'fold': fold_idx, 'f1': best_val_f1_fold})\n",
    "        if best_val_f1_fold > best_fold_f1:\n",
    "            best_fold_f1 = best_val_f1_fold\n",
    "            best_fold_model_state = best_state_fold\n",
    "            best_fold_idx = fold_idx\n",
    "            best_pca_models = fold_pca_models\n",
    "        cleanup_memory()\n",
    "    mean_f1 = np.mean([r['f1'] for r in fold_results])\n",
    "    std_f1 = np.std([r['f1'] for r in fold_results])\n",
    "    print(f\"\\n  üìä CV Results: Mean F1 = {mean_f1:.4f} ¬± {std_f1:.4f}\")\n",
    "    if mean_f1 > best_cv_score:\n",
    "        best_cv_score = mean_f1\n",
    "        best_hyperparams = hyperparams\n",
    "        best_model_state = best_fold_model_state\n",
    "        print(f\"  ‚úÖ New best!\")\n",
    "cv_time = time.time() - cv_start_time\n",
    "# Verify CV completed successfully\n",
    "if best_model_state is None:\n",
    "    print(\"‚ö†Ô∏è WARNING: CV loop completed but best_model_state is None!\")\n",
    "    print(\"This may indicate all hyperparameter combinations failed or no improvement was found.\")\n",
    "    print(\"Creating a default model for threshold tuning...\")\n",
    "    # Create a default model as fallback\n",
    "    temp_model = LinearClassifier(input_dim=X_reg_full.shape[1]).to(device)\n",
    "    best_model_state = temp_model.state_dict()\n",
    "    best_hyperparams = HYPERPARAMETER_GRID[0] if HYPERPARAMETER_GRID else {\"lr\": 1e-3, \"batch_size\": 512}\n",
    "    best_cv_score = 0.0\n",
    "    best_threshold = 0.5\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ CV Complete (Time: {cv_time/60:.1f} min)\")\n",
    "print(f\"Best Hyperparameters: {best_hyperparams}\")\n",
    "print(f\"Best CV F1: {best_cv_score:.4f}\")\n",
    "print(f\"{'='*80}\")\n",
    "memory_usage()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ba23b7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Final Threshold Tuning and Model Saving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cd6010",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T22:09:26.825815Z",
     "iopub.status.busy": "2025-11-17T22:09:26.825719Z",
     "iopub.status.idle": "2025-11-17T22:09:27.555421Z",
     "shell.execute_reply": "2025-11-17T22:09:27.555036Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare final model with best hyperparameters\n",
    "train_df_final = load_parquet_split('train')\n",
    "val_df_final = load_parquet_split('val')\n",
    "X_reg_train_final, X_emb_train_fams_final, y_train_final, _, _ = split_features_reg_and_all_emb(train_df_final)\n",
    "X_reg_val_final, X_emb_val_fams_final, y_val_final, _, _ = split_features_reg_and_all_emb(val_df_final)\n",
    "# Fit PCA on full training data for each family\n",
    "X_emb_train_pca_list = []\n",
    "X_emb_val_pca_list = []\n",
    "ipca_models_final = {}\n",
    "for fam, X_emb_train in X_emb_train_fams_final.items():\n",
    "    n_components = PCA_COMPONENTS_PER_FAMILY.get(fam, 32)\n",
    "    # Use PyTorch PCA (GPU-friendly)\n",
    "    # Only pass device parameter if using PyTorch PCA\n",
    "    if IS_TORCH_PCA:\n",
    "        ipca = IncrementalPCA(n_components=n_components, batch_size=2000, device=device)\n",
    "    else:\n",
    "        ipca = IncrementalPCA(n_components=n_components, batch_size=2000)\n",
    "    max_pca_rows = min(50_000, X_emb_train.shape[0])  # Reduced from 120k\n",
    "    idx = np.random.choice(X_emb_train.shape[0], size=max_pca_rows, replace=False)\n",
    "    ipca.fit(X_emb_train[idx])\n",
    "    X_emb_train_pca = ipca.transform(X_emb_train)\n",
    "    X_emb_val = X_emb_val_fams_final[fam]\n",
    "    X_emb_val_pca = ipca.transform(X_emb_val)\n",
    "    X_emb_train_pca_list.append(X_emb_train_pca)\n",
    "    X_emb_val_pca_list.append(X_emb_val_pca)\n",
    "    ipca_models_final[fam] = ipca\n",
    "    del X_emb_train_pca, X_emb_val_pca\n",
    "    cleanup_memory()\n",
    "if X_emb_train_pca_list:\n",
    "    X_emb_train_concat = np.concatenate(X_emb_train_pca_list, axis=1)\n",
    "    X_emb_val_concat = np.concatenate(X_emb_val_pca_list, axis=1)\n",
    "    X_train_final = np.concatenate([X_reg_train_final, X_emb_train_concat], axis=1)\n",
    "    X_val_final = np.concatenate([X_reg_val_final, X_emb_val_concat], axis=1)\n",
    "    del X_reg_train_final, X_reg_val_final, X_emb_train_pca_list, X_emb_val_pca_list, X_emb_train_concat, X_emb_val_concat\n",
    "else:\n",
    "    X_train_final = X_reg_train_final\n",
    "    X_val_final = X_reg_val_final\n",
    "    del X_reg_train_final, X_reg_val_final\n",
    "del train_df_final, val_df_final, X_emb_train_fams_final, X_emb_val_fams_final\n",
    "cleanup_memory()\n",
    "# Create final model\n",
    "final_model = LinearClassifier(input_dim=X_train_final.shape[1]).to(device)\n",
    "if best_model_state is None:\n",
    "    raise ValueError(\"best_model_state is None. CV loop may have failed. Check CV cell execution.\")\n",
    "final_model.load_state_dict(best_model_state)\n",
    "# Final threshold tuning\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Final Threshold Tuning on Validation Set\")\n",
    "print(\"=\"*80)\n",
    "BATCH_SIZE = best_hyperparams['batch_size']\n",
    "VAL_BATCH_SIZE = best_hyperparams['batch_size']\n",
    "train_loader_final, val_loader_final = make_dataloaders(\n",
    "    X_train_final, y_train_final, X_val_final, y_val_final,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    val_batch_size=VAL_BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "final_model.eval()\n",
    "all_val_preds = []\n",
    "all_val_targets = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in val_loader_final:\n",
    "        xb = xb.to(device)\n",
    "        yb_np = yb.numpy()\n",
    "        logits = final_model(xb)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy().ravel()\n",
    "        all_val_preds.append(probs)\n",
    "        all_val_targets.append(yb_np)\n",
    "        del xb, logits, probs, yb_np\n",
    "all_val_preds = np.concatenate(all_val_preds)\n",
    "all_val_targets = np.concatenate(all_val_targets)\n",
    "final_threshold, final_f1 = find_optimal_threshold(all_val_targets, all_val_preds)\n",
    "print(f\"‚úÖ Final Optimal Threshold: {final_threshold:.4f}\")\n",
    "print(f\"‚úÖ Final Validation F1: {final_f1:.4f}\")\n",
    "# Save model weights\n",
    "MODEL_SAVE_DIR = PROJECT_ROOT / 'models' / 'saved_models'\n",
    "MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "model_save_path = MODEL_SAVE_DIR / 'model4_reg_plus_all_embeddings_pca_linear_best.pt'\n",
    "save_model_weights(\n",
    "    final_model,\n",
    "    model_save_path,\n",
    "    metadata={\n",
    "        'input_dim': X_train_final.shape[1],\n",
    "        'best_cv_f1': best_cv_score,\n",
    "        'best_hyperparams': best_hyperparams,\n",
    "        'final_threshold': final_threshold,\n",
    "        'final_val_f1': final_f1,\n",
    "        'pca_components_per_family': PCA_COMPONENTS_PER_FAMILY\n",
    "    }\n",
    ")\n",
    "cleanup_memory()\n",
    "memory_usage()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61349ff2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Generate Submission.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f002c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T22:09:27.556738Z",
     "iopub.status.busy": "2025-11-17T22:09:27.556667Z",
     "iopub.status.idle": "2025-11-17T22:09:27.961335Z",
     "shell.execute_reply": "2025-11-17T22:09:27.960977Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load test set and generate submission\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Generating Submission\")\n",
    "print(\"=\"*80)\n",
    "test_df = load_parquet_split('test')\n",
    "test_ids = test_df['id'].to_numpy()\n",
    "X_reg_test, X_emb_test_fams, _, _, _ = split_features_reg_and_all_emb(test_df)\n",
    "# Apply PCA transform to test embeddings for each family\n",
    "X_emb_test_pca_list = []\n",
    "for fam, X_emb_test in X_emb_test_fams.items():\n",
    "    if fam in ipca_models_final:\n",
    "        X_emb_test_pca = ipca_models_final[fam].transform(X_emb_test)\n",
    "        X_emb_test_pca_list.append(X_emb_test_pca)\n",
    "        del X_emb_test_pca\n",
    "if X_emb_test_pca_list:\n",
    "    X_emb_test_concat = np.concatenate(X_emb_test_pca_list, axis=1)\n",
    "    X_test = np.concatenate([X_reg_test, X_emb_test_concat], axis=1)\n",
    "    del X_emb_test_pca_list, X_emb_test_concat\n",
    "else:\n",
    "    X_test = X_reg_test\n",
    "del test_df, X_reg_test, X_emb_test_fams\n",
    "cleanup_memory()\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "test_dataset = TabularDataset(X_test, np.zeros(len(X_test)))\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False\n",
    ")\n",
    "SUBMISSION_DIR = PROJECT_ROOT / 'data' / 'submission_files'\n",
    "SUBMISSION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "submission_path = SUBMISSION_DIR / 'submission_model4.csv'\n",
    "generate_submission(\n",
    "    final_model,\n",
    "    test_loader,\n",
    "    test_ids,\n",
    "    device,\n",
    "    final_threshold,\n",
    "    submission_path\n",
    ")\n",
    "cleanup_memory()\n",
    "memory_usage()\n",
    "print(\"\\n‚úÖ All done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 143.391542,
   "end_time": "2025-11-19T00:01:39.473632",
   "environment_variables": {},
   "exception": null,
   "input_path": "src/notebooks/model4_reg_plus_all_embeddings_pca_linear.ipynb",
   "output_path": "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/runs/model4_executed_20251118-185910.ipynb",
   "parameters": {},
   "start_time": "2025-11-18T23:59:16.082090",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}