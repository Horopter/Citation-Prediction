{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {
    "id": "header",
    "papermill": {
     "duration": 0.00479,
     "end_time": "2025-11-19T23:48:04.467563",
     "exception": false,
     "start_time": "2025-11-19T23:48:04.462773",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model: PyTorch MLP with All Features (No PCA)\n",
    "\n",
    "This notebook trains a **PyTorch-based Multi-Layer Perceptron (MLP)** classifier on all available features (regular + all embeddings) with comprehensive preprocessing:\n",
    "- âœ… All regular features\n",
    "- âœ… All embedding families (no PCA compression)\n",
    "- âœ… Feature scaling (StandardScaler)\n",
    "- âœ… Fixed Hyperparameters with CV Validation\n",
    "- âœ… Threshold Fine-tuning\n",
    "- âœ… Model Saving\n",
    "- âœ… Submission.csv Generation\n",
    "- âœ… OOM Safe with aggressive memory management\n",
    "- âœ… SMOTETomek for class imbalance\n",
    "- âœ… GPU acceleration with CPU fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "navigation",
   "metadata": {
    "id": "navigation",
    "papermill": {
     "duration": 0.003852,
     "end_time": "2025-11-19T23:48:04.475569",
     "exception": false,
     "start_time": "2025-11-19T23:48:04.471717",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ðŸ“‘ PyTorch MLP - Code Navigation Index\n",
    "\n",
    "## Quick Navigation\n",
    "- **[Setup](#1-setup)** - Imports, paths, device configuration, robustness utilities\n",
    "- **[Data Loading](#2-data-loading--feature-extraction)** - Load and split features (NO PCA)\n",
    "- **[SMOTETomek](#3-class-imbalance-handling-smotetomek)** - Class imbalance resampling\n",
    "- **[Feature Scaling](#4-feature-scaling)** - StandardScaler normalization\n",
    "- **[Hyperparameter Selection](#5-hyperparameter-selection)** - Fixed hyperparameters with CV validation\n",
    "- **[Threshold Tuning](#6-threshold-tuning--final-evaluation)** - Optimal threshold finding\n",
    "- **[Model Saving](#7-save-model)** - Save model weights and metadata\n",
    "- **[Submission](#8-generate-submission)** - Generate test predictions\n",
    "\n",
    "## Model Type: PyTorch MLP (all features, no PCA)\n",
    "\n",
    "## Key Features\n",
    "âœ… GPU-friendly with CPU fallback  \n",
    "âœ… Aggressive garbage collection  \n",
    "âœ… OOM resistant with chunked processing  \n",
    "âœ… Kernel panic resistant (signal handlers, checkpoints)  \n",
    "âœ… Polars-only (no pandas)  \n",
    "âœ… Fixed hyperparameters with CV validation  \n",
    "âœ… SMOTETomek for class imbalance  \n",
    "âœ… Feature scaling & normalization  \n",
    "âœ… Fine-grained threshold optimization  \n",
    "âœ… Model weights saved  \n",
    "âœ… Chunked/batched data processing  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_header",
   "metadata": {
    "id": "setup_header",
    "papermill": {
     "duration": 0.003994,
     "end_time": "2025-11-19T23:48:04.483667",
     "exception": false,
     "start_time": "2025-11-19T23:48:04.479673",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T23:48:04.492561Z",
     "iopub.status.busy": "2025-11-19T23:48:04.492391Z",
     "iopub.status.idle": "2025-11-19T23:48:24.243884Z",
     "shell.execute_reply": "2025-11-19T23:48:24.243132Z"
    },
    "id": "imports",
    "papermill": {
     "duration": 19.756833,
     "end_time": "2025-11-19T23:48:24.244585",
     "exception": false,
     "start_time": "2025-11-19T23:48:04.487752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import gc\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from typing import Dict, Optional, Tuple\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import signal\n",
    "import atexit\n",
    "from functools import wraps\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "startup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T23:48:24.255024Z",
     "iopub.status.busy": "2025-11-19T23:48:24.254766Z",
     "iopub.status.idle": "2025-11-19T23:48:24.742962Z",
     "shell.execute_reply": "2025-11-19T23:48:24.742310Z"
    },
    "id": "startup",
    "papermill": {
     "duration": 0.493695,
     "end_time": "2025-11-19T23:48:24.743479",
     "exception": false,
     "start_time": "2025-11-19T23:48:24.249784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL_PYTORCH_MLP EXECUTION STARTED\n",
      "Start Time: 2025-11-19 18:48:24\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# STARTUP & REPRODUCIBILITY\n",
    "# =========================\n",
    "\n",
    "TOTAL_START_TIME = time.time()\n",
    "START_TIME_STR = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"MODEL_PYTORCH_MLP EXECUTION STARTED\")\n",
    "print(f\"Start Time: {START_TIME_STR}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "paths",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T23:48:24.752779Z",
     "iopub.status.busy": "2025-11-19T23:48:24.752527Z",
     "iopub.status.idle": "2025-11-19T23:48:24.758351Z",
     "shell.execute_reply": "2025-11-19T23:48:24.757794Z"
    },
    "id": "paths",
    "papermill": {
     "duration": 0.011184,
     "end_time": "2025-11-19T23:48:24.758962",
     "exception": false,
     "start_time": "2025-11-19T23:48:24.747778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2\n",
      "MODEL_READY_DIR: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/model_ready\n"
     ]
    }
   ],
   "source": [
    "# ==============\n",
    "# PATH MANAGEMENT\n",
    "# ==============\n",
    "\n",
    "current = Path(os.getcwd())\n",
    "PROJECT_ROOT = current\n",
    "for _ in range(5):\n",
    "    if (PROJECT_ROOT / \"data\").exists():\n",
    "        break\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "else:\n",
    "    PROJECT_ROOT = current.parent.parent\n",
    "\n",
    "MODEL_READY_DIR = PROJECT_ROOT / \"data\" / \"model_ready\"\n",
    "MODEL_SAVE_DIR = PROJECT_ROOT / \"models\" / \"saved_models\"\n",
    "SUBMISSION_DIR = PROJECT_ROOT / \"data\" / \"submission_files\"\n",
    "MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SUBMISSION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "utils_path = PROJECT_ROOT / \"src\" / \"utils\"\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"MODEL_READY_DIR:\", MODEL_READY_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ml_libs",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T23:48:24.768156Z",
     "iopub.status.busy": "2025-11-19T23:48:24.767981Z",
     "iopub.status.idle": "2025-11-19T23:48:45.800127Z",
     "shell.execute_reply": "2025-11-19T23:48:45.799463Z"
    },
    "id": "ml_libs",
    "papermill": {
     "duration": 21.037685,
     "end_time": "2025-11-19T23:48:45.800833",
     "exception": false,
     "start_time": "2025-11-19T23:48:24.763148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==========\n",
    "# ML LIBRARIES\n",
    "# ==========\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report, precision_recall_curve, roc_curve, confusion_matrix\n",
    "from imblearn.combine import SMOTETomek\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ==========\n",
    "# VISUALIZATION LIBRARIES\n",
    "# ==========\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Image\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "memory_utils",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T23:48:45.811667Z",
     "iopub.status.busy": "2025-11-19T23:48:45.811372Z",
     "iopub.status.idle": "2025-11-19T23:48:45.819378Z",
     "shell.execute_reply": "2025-11-19T23:48:45.818784Z"
    },
    "id": "memory_utils",
    "papermill": {
     "duration": 0.013501,
     "end_time": "2025-11-19T23:48:45.819860",
     "exception": false,
     "start_time": "2025-11-19T23:48:45.806359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Using fallback memory utilities\n",
      "ðŸ’¾ Memory: 0.62 GB (RAM) | 0.00/0.00 GB (GPU used/reserved)\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# MEMORY UTILITIES (FALLBACK DEFS)\n",
    "# ===============================\n",
    "try:\n",
    "    from model_training_utils import cleanup_memory, memory_usage, check_memory_safe\n",
    "    print(\"âœ… Memory utilities imported from shared module\")\n",
    "except ImportError:\n",
    "    def cleanup_memory():\n",
    "        \"\"\"Aggressive memory cleanup for both CPU and GPU.\"\"\"\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "            torch.cuda.ipc_collect()\n",
    "        gc.collect()\n",
    "    \n",
    "    def memory_usage():\n",
    "        \"\"\"Display current memory usage statistics.\"\"\"\n",
    "        try:\n",
    "            import psutil\n",
    "            process = psutil.Process(os.getpid())\n",
    "            mem_gb = process.memory_info().rss / 1024**3\n",
    "            print(f\"ðŸ’¾ Memory: {mem_gb:.2f} GB (RAM)\", end=\"\")\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_mem = torch.cuda.memory_allocated() / 1024**3\n",
    "                gpu_reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "                print(f\" | {gpu_mem:.2f}/{gpu_reserved:.2f} GB (GPU used/reserved)\")\n",
    "            else:\n",
    "                print()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def check_memory_safe(ram_threshold_gb=0.85, gpu_threshold=0.80):\n",
    "        \"\"\"Check if memory usage is safe for operations.\"\"\"\n",
    "        try:\n",
    "            import psutil\n",
    "            process = psutil.Process(os.getpid())\n",
    "            ram_gb = process.memory_info().rss / 1024**3\n",
    "            total_ram = psutil.virtual_memory().total / 1024**3\n",
    "            ram_ratio = ram_gb / total_ram if total_ram > 0 else 0\n",
    "            gpu_ratio = 0\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_used = torch.cuda.memory_allocated() / 1024**3\n",
    "                gpu_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "                gpu_ratio = gpu_used / gpu_total if gpu_total > 0 else 0\n",
    "            is_safe = ram_ratio < ram_threshold_gb and gpu_ratio < gpu_threshold\n",
    "            return is_safe, {\"ram_gb\": ram_gb, \"ram_ratio\": ram_ratio, \"gpu_ratio\": gpu_ratio}\n",
    "        except:\n",
    "            return True, {}\n",
    "    \n",
    "    print(\"âš ï¸ Using fallback memory utilities\")\n",
    "\n",
    "memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "robustness_utils",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T23:48:45.829444Z",
     "iopub.status.busy": "2025-11-19T23:48:45.829271Z",
     "iopub.status.idle": "2025-11-19T23:48:45.848002Z",
     "shell.execute_reply": "2025-11-19T23:48:45.847448Z"
    },
    "id": "robustness_utils",
    "papermill": {
     "duration": 0.024221,
     "end_time": "2025-11-19T23:48:45.848519",
     "exception": false,
     "start_time": "2025-11-19T23:48:45.824298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Enhanced robustness utilities loaded\n",
      "âœ… Training robustness wrappers loaded\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# ROBUSTNESS/CHECKPOINT UTILITIES\n",
    "# ===============================\n",
    "\n",
    "_checkpoint_state = {\n",
    "    \"pca_complete\": False,\n",
    "    \"scaling_complete\": False,\n",
    "    \"cv_complete\": False,\n",
    "    \"final_model_trained\": False,\n",
    "    \"last_saved_checkpoint\": None,\n",
    "}\n",
    "\n",
    "def save_checkpoint(state_name: str, data: dict, checkpoint_dir: Path = None):\n",
    "    \"\"\"Save checkpoint to resume from failures.\"\"\"\n",
    "    if checkpoint_dir is None:\n",
    "        checkpoint_dir = PROJECT_ROOT / \"data\" / \"checkpoints\"\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    checkpoint_path = checkpoint_dir / f\"model_pytorch_mlp_checkpoint_{state_name}.pkl\"\n",
    "    try:\n",
    "        with open(checkpoint_path, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        _checkpoint_state[\"last_saved_checkpoint\"] = checkpoint_path\n",
    "        print(f\"âœ… Checkpoint saved: {checkpoint_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Failed to save checkpoint: {e}\")\n",
    "\n",
    "def load_checkpoint(state_name: str, checkpoint_dir: Path = None):\n",
    "    \"\"\"Load checkpoint to resume from failures.\"\"\"\n",
    "    if checkpoint_dir is None:\n",
    "        checkpoint_dir = PROJECT_ROOT / \"data\" / \"checkpoints\"\n",
    "    checkpoint_path = checkpoint_dir / f\"model_pytorch_mlp_checkpoint_{state_name}.pkl\"\n",
    "    if checkpoint_path.exists():\n",
    "        try:\n",
    "            with open(checkpoint_path, \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "            print(f\"âœ… Checkpoint loaded: {checkpoint_path}\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Failed to load checkpoint: {e}\")\n",
    "    return None\n",
    "\n",
    "def safe_operation(operation_name: str, max_retries: int = 3, checkpoint_on_success: bool = False):\n",
    "    \"\"\"Decorator for safe operations with retry and checkpoint support.\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    is_safe, mem_info = check_memory_safe(ram_threshold_gb=0.80, gpu_threshold=0.75)\n",
    "                    if not is_safe:\n",
    "                        cleanup_memory()\n",
    "                        if torch.cuda.is_available():\n",
    "                            torch.cuda.empty_cache()\n",
    "                        time.sleep(1)\n",
    "                    result = func(*args, **kwargs)\n",
    "                    cleanup_memory()\n",
    "                    if checkpoint_on_success:\n",
    "                        save_checkpoint(operation_name, {\"status\": \"complete\", \"result\": result})\n",
    "                    return result\n",
    "                except (MemoryError, RuntimeError) as e:\n",
    "                    error_msg = str(e).lower()\n",
    "                    if \"out of memory\" in error_msg or \"oom\" in error_msg:\n",
    "                        if attempt < max_retries - 1:\n",
    "                            cleanup_memory()\n",
    "                            if torch.cuda.is_available():\n",
    "                                torch.cuda.empty_cache()\n",
    "                            time.sleep(2)\n",
    "                            continue\n",
    "                        else:\n",
    "                            raise\n",
    "                    else:\n",
    "                        raise\n",
    "                except Exception as e:\n",
    "                    if attempt < max_retries - 1:\n",
    "                        cleanup_memory()\n",
    "                        time.sleep(1)\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise\n",
    "            return None\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "def chunked_operation(\n",
    "    data,\n",
    "    operation_func,\n",
    "    chunk_size: int = 10000,\n",
    "    progress_every: int = 10,\n",
    "    operation_name: str = \"operation\",\n",
    "):\n",
    "    \"\"\"Execute operation on data in chunks with progress tracking.\"\"\"\n",
    "    total_chunks = (len(data) + chunk_size - 1) // chunk_size\n",
    "    results = []\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        chunk_num = i // chunk_size + 1\n",
    "        chunk = data[i : i + chunk_size]\n",
    "        try:\n",
    "            is_safe, mem_info = check_memory_safe(ram_threshold_gb=0.85, gpu_threshold=0.80)\n",
    "            if not is_safe:\n",
    "                cleanup_memory()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                time.sleep(0.5)\n",
    "            chunk_result = operation_func(chunk)\n",
    "            results.append(chunk_result)\n",
    "            if chunk_num % progress_every == 0 or chunk_num == total_chunks:\n",
    "                print(f\"  Progress: {chunk_num}/{total_chunks} chunks ({chunk_num*100//total_chunks}%)\")\n",
    "            del chunk\n",
    "            if chunk_num % 5 == 0:\n",
    "                cleanup_memory()\n",
    "        except (MemoryError, RuntimeError) as e:\n",
    "            error_msg = str(e).lower()\n",
    "            if \"out of memory\" in error_msg or \"oom\" in error_msg:\n",
    "                cleanup_memory()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                smaller_chunk_size = max(1000, chunk_size // 2)\n",
    "                if smaller_chunk_size < chunk_size:\n",
    "                    return chunked_operation(\n",
    "                        data[i:],\n",
    "                        operation_func,\n",
    "                        chunk_size=smaller_chunk_size,\n",
    "                        progress_every=progress_every,\n",
    "                        operation_name=operation_name,\n",
    "                    )\n",
    "                else:\n",
    "                    raise\n",
    "            else:\n",
    "                raise\n",
    "    return results\n",
    "\n",
    "def emergency_cleanup():\n",
    "    \"\"\"Emergency cleanup on exit.\"\"\"\n",
    "    try:\n",
    "        cleanup_memory()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        print(\"âœ… Emergency cleanup completed\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "atexit.register(emergency_cleanup)\n",
    "\n",
    "def signal_handler(signum, frame):\n",
    "    \"\"\"Handle signals for graceful shutdown.\"\"\"\n",
    "    print(f\"âš ï¸ Received signal {signum}, saving checkpoint...\")\n",
    "    save_checkpoint(\"emergency\", {\"status\": \"signal_received\", \"signal\": signum})\n",
    "    emergency_cleanup()\n",
    "    raise KeyboardInterrupt\n",
    "\n",
    "try:\n",
    "    signal.signal(signal.SIGINT, signal_handler)\n",
    "    signal.signal(signal.SIGTERM, signal_handler)\n",
    "except:\n",
    "    pass\n",
    "print(\"âœ… Enhanced robustness utilities loaded\")\n",
    "\n",
    "def safe_prediction(predict_func, *args, **kwargs):\n",
    "    \"\"\"Execute prediction with chunked processing.\"\"\"\n",
    "    try:\n",
    "        is_safe, mem_info = check_memory_safe(ram_threshold_gb=0.85, gpu_threshold=0.80)\n",
    "        if not is_safe:\n",
    "            cleanup_memory()\n",
    "        if \"X\" in kwargs and len(kwargs[\"X\"]) > 50000:\n",
    "            X = kwargs[\"X\"]\n",
    "            chunk_size = 10000\n",
    "            predictions = []\n",
    "            for i in range(0, len(X), chunk_size):\n",
    "                chunk = X[i : i + chunk_size]\n",
    "                kwargs[\"X\"] = chunk\n",
    "                chunk_preds = predict_func(*args, **kwargs)\n",
    "                predictions.append(chunk_preds)\n",
    "                del chunk, chunk_preds\n",
    "                if i % (chunk_size * 5) == 0:\n",
    "                    cleanup_memory()\n",
    "            return np.concatenate(predictions)\n",
    "        else:\n",
    "            return predict_func(*args, **kwargs)\n",
    "    except (MemoryError, RuntimeError) as e:\n",
    "        error_msg = str(e).lower()\n",
    "        if \"out of memory\" in error_msg or \"oom\" in error_msg:\n",
    "            cleanup_memory()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            if \"X\" in kwargs:\n",
    "                X = kwargs[\"X\"]\n",
    "                chunk_size = 5000\n",
    "                predictions = []\n",
    "                for i in range(0, len(X), chunk_size):\n",
    "                    chunk = X[i : i + chunk_size]\n",
    "                    kwargs[\"X\"] = chunk\n",
    "                    chunk_preds = predict_func(*args, **kwargs)\n",
    "                    predictions.append(chunk_preds)\n",
    "                    del chunk, chunk_preds\n",
    "                    cleanup_memory()\n",
    "                return np.concatenate(predictions)\n",
    "            else:\n",
    "                raise\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "print(\"âœ… Training robustness wrappers loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_loading_header",
   "metadata": {
    "id": "data_loading_header",
    "papermill": {
     "duration": 0.004106,
     "end_time": "2025-11-19T23:48:45.856895",
     "exception": false,
     "start_time": "2025-11-19T23:48:45.852789",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Data Loading & Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "data_loading",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T23:48:45.866091Z",
     "iopub.status.busy": "2025-11-19T23:48:45.865925Z",
     "iopub.status.idle": "2025-11-19T23:50:21.059042Z",
     "shell.execute_reply": "2025-11-19T23:50:21.058403Z"
    },
    "id": "data_loading",
    "papermill": {
     "duration": 95.198664,
     "end_time": "2025-11-19T23:50:21.059730",
     "exception": false,
     "start_time": "2025-11-19T23:48:45.861066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 1: Data Loading\n",
      "================================================================================\n",
      "Loading train from /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/model_ready/train_model_ready.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading val from /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/model_ready/val_model_ready.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Data Summary:\n",
      "  Regular features: 54\n",
      "  Total features: 1974\n",
      "  Embedding sent_transformer_: 384 dims (NO PCA)\n",
      "  Embedding sent_transformer_: 384 dims\n",
      "  Embedding scibert_: 768 dims (NO PCA)\n",
      "  Embedding scibert_: 768 dims\n",
      "  Embedding specter2_: 768 dims (NO PCA)\n",
      "  Embedding specter2_: 768 dims\n",
      "  Train samples: 960000, Positive: 65808, Negative: 894192\n",
      "  Val samples: 120000, Positive: 8075, Negative: 111925\n",
      "\n",
      "â±ï¸  Data Loading Time: 92.85 seconds (1.55 minutes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Memory: 48.62 GB (RAM) | 0.00/0.00 GB (GPU used/reserved)\n"
     ]
    }
   ],
   "source": [
    "def load_parquet_split(split: str) -> pl.DataFrame:\n",
    "    \"\"\"Load a model_ready parquet split with error handling.\"\"\"\n",
    "    try:\n",
    "        path = MODEL_READY_DIR / f\"{split}_model_ready.parquet\"\n",
    "        if not path.exists():\n",
    "            alt = MODEL_READY_DIR / f\"{split}_model_ready_reduced.parquet\"\n",
    "            if alt.exists():\n",
    "                path = alt\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Could not find {split} data\")\n",
    "        print(f\"Loading {split} from {path}\")\n",
    "        return pl.read_parquet(path)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading {split}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def split_features_reg_and_all_emb(df: pl.DataFrame):\n",
    "    \"\"\"Split features into regular and embedding families.\"\"\"\n",
    "    cols = df.columns\n",
    "    dtypes = df.dtypes\n",
    "    label = df[\"label\"].to_numpy() if \"label\" in cols else None\n",
    "\n",
    "    reg_cols = []\n",
    "    EMBEDDING_FAMILY_PREFIXES = [\"sent_transformer_\", \"scibert_\", \"specter_\", \"specter2_\", \"ner_\"]\n",
    "    emb_family_to_cols = {p: [] for p in EMBEDDING_FAMILY_PREFIXES}\n",
    "\n",
    "    NUMERIC_DTYPES = {\n",
    "        pl.Int8,\n",
    "        pl.Int16,\n",
    "        pl.Int32,\n",
    "        pl.Int64,\n",
    "        pl.UInt8,\n",
    "        pl.UInt16,\n",
    "        pl.UInt32,\n",
    "        pl.UInt64,\n",
    "        pl.Float32,\n",
    "        pl.Float64,\n",
    "    }\n",
    "\n",
    "    for c, dt in zip(cols, dtypes):\n",
    "        if c in (\"id\", \"label\"):\n",
    "            continue\n",
    "        matched = False\n",
    "        for p in EMBEDDING_FAMILY_PREFIXES:\n",
    "            if c.startswith(p):\n",
    "                emb_family_to_cols[p].append(c)\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched and dt in NUMERIC_DTYPES:\n",
    "            reg_cols.append(c)\n",
    "\n",
    "    X_reg = df.select(reg_cols).to_numpy() if reg_cols else None\n",
    "    X_emb_families = {}\n",
    "    for p, clist in emb_family_to_cols.items():\n",
    "        if clist:\n",
    "            X_emb_families[p] = df.select(clist).to_numpy()\n",
    "\n",
    "    return X_reg, X_emb_families, label, reg_cols, emb_family_to_cols\n",
    "\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PHASE 1: Data Loading\")\n",
    "    print(\"=\" * 80)\n",
    "    phase_start = time.time()\n",
    "    \n",
    "    train_df = load_parquet_split(\"train\")\n",
    "    val_df = load_parquet_split(\"val\")\n",
    "\n",
    "    X_reg_train, X_emb_train_fams, y_train, reg_cols, emb_family_to_cols = (\n",
    "        split_features_reg_and_all_emb(train_df)\n",
    "    )\n",
    "    X_reg_val, X_emb_val_fams, y_val, _, _ = split_features_reg_and_all_emb(val_df)\n",
    "\n",
    "    # Combine regular + ALL embeddings (NO PCA)\n",
    "    X_emb_train_list = []\n",
    "    X_emb_val_list = []\n",
    "    for fam in X_emb_train_fams.keys():\n",
    "        X_emb_train_list.append(X_emb_train_fams[fam])\n",
    "        X_emb_val_list.append(X_emb_val_fams[fam])\n",
    "    \n",
    "    X_emb_train = np.hstack(X_emb_train_list) if X_emb_train_list else None\n",
    "    X_emb_val = np.hstack(X_emb_val_list) if X_emb_val_list else None\n",
    "\n",
    "    if X_reg_train is not None:\n",
    "        X_train = np.hstack([X_reg_train, X_emb_train]) if X_emb_train is not None else X_reg_train\n",
    "        X_val = np.hstack([X_reg_val, X_emb_val]) if X_emb_val is not None else X_reg_val\n",
    "    else:\n",
    "        X_train = X_emb_train\n",
    "        X_val = X_emb_val\n",
    "\n",
    "    phase_time = time.time() - phase_start\n",
    "    print(f\"\\nðŸ“Š Data Summary:\")\n",
    "    print(f\"  Regular features: {len(reg_cols)}\")\n",
    "    print(f\"  Total features: {X_train.shape[1]}\")\n",
    "    for fam, arr in X_emb_train_fams.items():\n",
    "        print(f\"  Embedding {fam}: {arr.shape[1]} dims (NO PCA)\")\n",
    "        print(f\"  Embedding {fam}: {arr.shape[1]} dims\")\n",
    "    print(\n",
    "        f\"  Train samples: {len(y_train)}, Positive: {y_train.sum()}, Negative: {(y_train==0).sum()}\"\n",
    "    )\n",
    "    print(f\"  Val samples: {len(y_val)}, Positive: {y_val.sum()}, Negative: {(y_val==0).sum()}\")\n",
    "    print(f\"\\nâ±ï¸  Data Loading Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\")\n",
    "\n",
    "    del train_df, val_df, X_reg_train, X_reg_val, X_emb_train_fams, X_emb_val_fams\n",
    "    del X_emb_train_list, X_emb_val_list, X_emb_train, X_emb_val\n",
    "    cleanup_memory()\n",
    "    memory_usage()\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smote_header",
   "metadata": {
    "id": "smote_header",
    "papermill": {
     "duration": 0.006602,
     "end_time": "2025-11-19T23:50:21.073851",
     "exception": false,
     "start_time": "2025-11-19T23:50:21.067249",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Class Imbalance Handling: SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smote",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:53:01.442578Z",
     "iopub.status.busy": "2025-11-19T09:53:01.442146Z",
     "iopub.status.idle": "2025-11-19T09:53:02.420974Z",
     "shell.execute_reply": "2025-11-19T09:53:02.420619Z"
    },
    "id": "smote",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-11-19T23:50:21.078570",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Skip SMOTETomek for very large datasets (>100k samples) - use class_weight instead\n",
    "USE_CLASS_WEIGHT = False  # Will be set based on dataset size\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 2: SMOTETomek Resampling\")\n",
    "print(\"=\" * 80)\n",
    "phase_start = time.time()\n",
    "\n",
    "print(\"\\nðŸ“Š Checking class imbalance and applying SMOTETomek resampling...\")\n",
    "print(f\"  Before: {len(X_train)} samples, Positive: {y_train.sum()}, Negative: {(y_train == 0).sum()}\")\n",
    "print(f\"  Imbalance ratio: {(y_train == 0).sum() / max(y_train.sum(), 1):.2f}:1\")\n",
    "\n",
    "try:\n",
    "    # SMOTETomek is REQUIRED - use adaptive strategy for large datasets\n",
    "    if len(X_train) > 500_000:\n",
    "        print(f\"  âš ï¸ Large dataset detected ({len(X_train):,} samples), using sampling_strategy=0.2 for memory efficiency\")\n",
    "        smt = SMOTETomek(random_state=42, sampling_strategy=0.2, n_jobs=-1)\n",
    "    else:\n",
    "        smt = SMOTETomek(random_state=42, sampling_strategy=0.4, n_jobs=-1)\n",
    "    \n",
    "    # Fit and resample with memory cleanup\n",
    "    print(\"  Fitting SMOTETomek...\")\n",
    "    cleanup_memory()\n",
    "    X_train_resampled, y_train_resampled = smt.fit_resample(X_train, y_train)\n",
    "    cleanup_memory()\n",
    "\n",
    "    phase_time = time.time() - phase_start\n",
    "    print(f\"  After: {len(X_train_resampled)} samples, Positive: {y_train_resampled.sum()}, Negative: {(y_train_resampled == 0).sum()}\")\n",
    "    print(f\"  Balance ratio: {(y_train_resampled == 0).sum() / max(y_train_resampled.sum(), 1):.2f}:1\")\n",
    "    print(f\"\\nâ±ï¸  SMOTETomek Resampling Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\")\n",
    "\n",
    "    X_train = X_train_resampled\n",
    "    y_train = y_train_resampled\n",
    "\n",
    "    del X_train_resampled, y_train_resampled\n",
    "    cleanup_memory()\n",
    "except Exception as e:\n",
    "    phase_time = time.time() - phase_start\n",
    "    print(f\"  âš ï¸ SMOTETomek failed: {e}\")\n",
    "    print(\"  Continuing with original training data...\")\n",
    "    print(f\"\\nâ±ï¸  SMOTETomek Time (failed): {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\")\n",
    "    cleanup_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scaling_header",
   "metadata": {
    "id": "scaling_header",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 4. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scaling",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:53:02.430182Z",
     "iopub.status.busy": "2025-11-19T09:53:02.430071Z",
     "iopub.status.idle": "2025-11-19T09:53:02.577129Z",
     "shell.execute_reply": "2025-11-19T09:53:02.576780Z"
    },
    "id": "scaling",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 3: Feature Scaling\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "phase_start = time.time()\n",
    "print(\"\\nðŸ“Š Applying Feature Scaling to combined features...\")\n",
    "\n",
    "# Store raw (unscaled) data for CV Pipeline (scaler will be fit per fold)\n",
    "X_train_raw = X_train.copy()\n",
    "X_val_raw = X_val.copy()\n",
    "y_train_raw = y_train.copy()\n",
    "y_val_raw = y_val.copy()\n",
    "\n",
    "# Use StandardScaler (RobustScaler doesn't support partial_fit)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# For large datasets, fit on sample then transform in chunks\n",
    "CHUNK_SIZE = 50000\n",
    "\n",
    "if X_train.shape[0] > CHUNK_SIZE:\n",
    "    print(f\"  Fitting scaler on sample ({min(CHUNK_SIZE, X_train.shape[0])} samples) for OOM protection...\")\n",
    "    sample_indices = np.random.choice(X_train.shape[0], size=min(CHUNK_SIZE, X_train.shape[0]), replace=False)\n",
    "    scaler.fit(X_train[sample_indices])\n",
    "    del sample_indices\n",
    "    cleanup_memory()\n",
    "\n",
    "    # Transform train in chunks\n",
    "    print(f\"  Transforming train data in chunks (size={CHUNK_SIZE})...\")\n",
    "    X_train_chunks = []\n",
    "    for i in range(0, X_train.shape[0], CHUNK_SIZE):\n",
    "        chunk = scaler.transform(X_train[i:i + CHUNK_SIZE])\n",
    "        X_train_chunks.append(chunk)\n",
    "        del chunk\n",
    "        if i % (CHUNK_SIZE * 5) == 0:\n",
    "            cleanup_memory()\n",
    "    X_train = np.vstack(X_train_chunks)\n",
    "    del X_train_chunks\n",
    "    cleanup_memory()\n",
    "\n",
    "    # Transform val in chunks\n",
    "    if X_val.shape[0] > CHUNK_SIZE:\n",
    "        print(f\"  Transforming val data in chunks (size={CHUNK_SIZE})...\")\n",
    "        X_val_chunks = []\n",
    "        for i in range(0, X_val.shape[0], CHUNK_SIZE):\n",
    "            chunk = scaler.transform(X_val[i:i + CHUNK_SIZE])\n",
    "            X_val_chunks.append(chunk)\n",
    "            del chunk\n",
    "        X_val = np.vstack(X_val_chunks)\n",
    "        del X_val_chunks\n",
    "    else:\n",
    "        X_val = scaler.transform(X_val)\n",
    "else:\n",
    "    # Small dataset - fit and transform normally\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "cleanup_memory()\n",
    "phase_time = time.time() - phase_start\n",
    "print(\"  âœ… Scaling complete!\")\n",
    "print(f\"\\nâ±ï¸  Feature Scaling Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\")\n",
    "memory_usage()\n",
    "\n",
    "# Store raw (unscaled) data again for safety (if further processing needed)\n",
    "X_train_raw = X_train.copy()\n",
    "X_val_raw = X_val.copy()\n",
    "y_train_raw = y_train.copy()\n",
    "y_val_raw = y_val.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pytorch_model_header",
   "metadata": {
    "id": "pytorch_model_header",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 5. PyTorch MLP Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pytorch_model",
   "metadata": {
    "id": "pytorch_model",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Enhanced Multi-Layer Perceptron with BatchNorm and Residual Connections.\"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dims: Tuple[int, ...], dropout_rate: float = 0.0, \n",
    "                 activation: str = 'relu', use_batch_norm: bool = True, use_residual: bool = False):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.use_residual = use_residual\n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            # Linear layer\n",
    "            self.layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            \n",
    "            # Batch normalization\n",
    "            if use_batch_norm:\n",
    "                self.layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            \n",
    "            # Activation\n",
    "            if activation == 'relu':\n",
    "                self.layers.append(nn.ReLU())\n",
    "            elif activation == 'tanh':\n",
    "                self.layers.append(nn.Tanh())\n",
    "            elif activation == 'gelu':\n",
    "                self.layers.append(nn.GELU())\n",
    "            elif activation == 'swish':\n",
    "                self.layers.append(nn.SiLU())  # Swish/SiLU\n",
    "            else:\n",
    "                self.layers.append(nn.ReLU())\n",
    "            \n",
    "            # Dropout\n",
    "            if dropout_rate > 0:\n",
    "                self.layers.append(nn.Dropout(dropout_rate))\n",
    "            \n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer (with sigmoid)\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(prev_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Xavier/Glorot initialization for better convergence.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.BatchNorm1d):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Store input for residual connection\n",
    "        if self.use_residual and len(self.layers) > 0:\n",
    "            residual = x\n",
    "        \n",
    "        # Forward through hidden layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Residual connection (if dimensions match)\n",
    "        if self.use_residual and x.shape == residual.shape:\n",
    "            x = x + residual\n",
    "        \n",
    "        # Output layer\n",
    "        return self.output(x).squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optuna_header",
   "metadata": {
    "id": "optuna_header",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 6. Hyperparameter Selection (Fixed Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optuna_setup",
   "metadata": {
    "id": "optuna_setup",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 4: Hyperparameter Selection (Fixed Parameters)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use fixed hyperparameters (no Optuna) - IMPROVED FOR BETTER PERFORMANCE\n",
    "print(\"\\nðŸ“Š Using improved fixed hyperparameters:\")\n",
    "best_params = {\n",
    "    'n_layers': 4,  # Deeper network\n",
    "    'hidden_dim_base': 512,  # Wider network\n",
    "    'dim_strategy': 'decreasing',  # Start wide, get narrower\n",
    "    'dropout_rate': 0.3,  # More regularization\n",
    "    'activation': 'swish',  # Swish/SiLU activation (better than ReLU)\n",
    "    'learning_rate': 0.0005,  # Lower learning rate for stability\n",
    "    'batch_size': 256,  # Larger batch size\n",
    "    'weight_decay': 1e-3,  # More weight decay\n",
    "    'use_batch_norm': True,  # Batch normalization\n",
    "    'use_residual': False,  # Can enable if needed\n",
    "    'label_smoothing': 0.05,  # Label smoothing for better generalization\n",
    "}\n",
    "\n",
    "print(\"  Hyperparameters:\")\n",
    "for key, value in best_params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Optional: Quick CV validation with fixed params\n",
    "print(\"\\nðŸ” Running quick CV validation with fixed parameters...\")\n",
    "MAX_SAMPLES_FOR_CV = 50000\n",
    "X_full = np.vstack([X_train_raw, X_val_raw])\n",
    "y_full = np.hstack([y_train_raw, y_val_raw])\n",
    "\n",
    "if len(X_full) > MAX_SAMPLES_FOR_CV:\n",
    "    print(f\"âš ï¸ Dataset too large ({len(X_full)} samples), using subset ({MAX_SAMPLES_FOR_CV} samples) for CV\")\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_full, _, y_full, _ = train_test_split(\n",
    "        X_full, y_full,\n",
    "        train_size=MAX_SAMPLES_FOR_CV,\n",
    "        stratify=y_full,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    print(f\"  Using {len(X_full)} samples for CV validation\")\n",
    "    cleanup_memory()\n",
    "\n",
    "# Scale the CV data\n",
    "scaler_cv = StandardScaler()\n",
    "X_full_scaled = scaler_cv.fit_transform(X_full)\n",
    "\n",
    "print(f\"\\nðŸ“Š CV dataset: {X_full_scaled.shape}, labels: {y_full.shape}\")\n",
    "print(f\"  Positive samples: {y_full.sum()}, Negative: {(y_full == 0).sum()}\")\n",
    "\n",
    "# Setup Stratified K-Fold\n",
    "N_FOLDS = 5\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Build hidden_dims\n",
    "n_layers = best_params['n_layers']\n",
    "hidden_dim_base = best_params['hidden_dim_base']\n",
    "dim_strategy = best_params['dim_strategy']\n",
    "\n",
    "hidden_dims = []\n",
    "for i in range(n_layers):\n",
    "    if dim_strategy == 'decreasing':\n",
    "        dim = hidden_dim_base // (2 ** i)\n",
    "    else:\n",
    "        dim = hidden_dim_base\n",
    "    hidden_dims.append(max(32, dim))\n",
    "\n",
    "print(f\"\\n  Network architecture: {X_full_scaled.shape[1]} -> {hidden_dims} -> 1\")\n",
    "\n",
    "# Quick CV validation\n",
    "cv_scores = []\n",
    "print(f\"\\n  Running {N_FOLDS}-fold CV...\")\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_full_scaled, y_full)):\n",
    "    X_fold_train = X_full_scaled[train_idx]\n",
    "    y_fold_train = y_full[train_idx]\n",
    "    X_fold_val = X_full_scaled[val_idx]\n",
    "    y_fold_val = y_full[val_idx]\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_fold_train).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_fold_train).to(device)\n",
    "    X_val_tensor = torch.FloatTensor(X_fold_val).to(device)\n",
    "    y_val_tensor = torch.FloatTensor(y_fold_val).to(device)\n",
    "    \n",
    "    # Create model with improved architecture\n",
    "    model = MLP(\n",
    "        input_dim=X_full_scaled.shape[1],\n",
    "        hidden_dims=tuple(hidden_dims),\n",
    "        dropout_rate=best_params['dropout_rate'],\n",
    "        activation=best_params['activation'],\n",
    "        use_batch_norm=best_params.get('use_batch_norm', True),\n",
    "        use_residual=best_params.get('use_residual', False)\n",
    "    ).to(device)\n",
    "    \n",
    "    # Use AdamW optimizer (better weight decay)\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=best_params['learning_rate'], \n",
    "        weight_decay=best_params['weight_decay'],\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler (cosine annealing with warm restarts)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    # Calculate class weights for imbalanced data\n",
    "    pos_weight = (y_fold_train == 0).sum() / max((y_fold_train == 1).sum(), 1)\n",
    "    pos_weight_tensor = torch.tensor(pos_weight, device=device)\n",
    "    \n",
    "    # Training with improved techniques\n",
    "    model.train()\n",
    "    n_epochs = 50  # More epochs for better convergence\n",
    "    best_val_f1 = 0.0\n",
    "    patience = 10  # More patience\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Label smoothing\n",
    "    label_smoothing = best_params.get('label_smoothing', 0.0)\n",
    "    \n",
    "    # Loss function with class weights\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "    \n",
    "    # Progress bar for epochs\n",
    "    epoch_pbar = tqdm(range(n_epochs), desc=f\"Fold {fold_idx+1}/{N_FOLDS}\", leave=False)\n",
    "    for epoch in epoch_pbar:\n",
    "        # Mini-batch training\n",
    "        indices = torch.randperm(len(X_train_tensor), device=device)\n",
    "        batch_losses = []\n",
    "        for i in range(0, len(X_train_tensor), best_params['batch_size']):\n",
    "            batch_indices = indices[i:i + best_params['batch_size']]\n",
    "            X_batch = X_train_tensor[batch_indices]\n",
    "            y_batch = y_train_tensor[batch_indices]\n",
    "            \n",
    "            # Label smoothing\n",
    "            if label_smoothing > 0:\n",
    "                y_batch_smooth = y_batch * (1 - label_smoothing) + (1 - y_batch) * label_smoothing\n",
    "            else:\n",
    "                y_batch_smooth = y_batch\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            # Use logits for BCEWithLogitsLoss (remove sigmoid from forward if using this)\n",
    "            # For now, use regular BCE with smoothed labels\n",
    "            loss = nn.BCELoss()(outputs, y_batch_smooth)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            batch_losses.append(loss.item())\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Validation\n",
    "        if (epoch + 1) % 3 == 0:  # Validate more frequently\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(X_val_tensor)\n",
    "                val_preds = (val_outputs.cpu().numpy() >= 0.5).astype(int)\n",
    "                val_f1 = f1_score(y_fold_val, val_preds)\n",
    "                \n",
    "                if val_f1 > best_val_f1:\n",
    "                    best_val_f1 = val_f1\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        epoch_pbar.close()\n",
    "                        break\n",
    "            model.train()\n",
    "        \n",
    "        # Update progress bar\n",
    "        avg_loss = np.mean(batch_losses) if batch_losses else 0.0\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        epoch_pbar.set_postfix({\n",
    "            'loss': f'{avg_loss:.4f}',\n",
    "            'val_f1': f'{best_val_f1:.4f}',\n",
    "            'lr': f'{current_lr:.2e}',\n",
    "            'patience': patience_counter\n",
    "        })\n",
    "    \n",
    "    epoch_pbar.close()\n",
    "    \n",
    "    cv_scores.append(best_val_f1)\n",
    "    print(f\"    Fold {fold_idx + 1}/{N_FOLDS}: F1 = {best_val_f1:.4f}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, optimizer, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor\n",
    "    cleanup_memory()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "best_cv_score = np.mean(cv_scores)\n",
    "print(f\"\\nâœ… CV validation complete\")\n",
    "print(f\"  Mean CV F1: {best_cv_score:.4f}\")\n",
    "print(f\"  Std CV F1: {np.std(cv_scores):.4f}\")\n",
    "\n",
    "cleanup_memory()\n",
    "memory_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optuna_study",
   "metadata": {
    "id": "optuna_study",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run Optuna study (if enabled) or use default hyperparameters\n",
    "USE_OPTUNA = False\n",
    "if USE_OPTUNA:\n",
    "    try:\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=TPESampler(seed=SEED)\n",
    "        )\n",
    "\n",
    "        print(\"\\nðŸš€ Starting Optuna optimization...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        study.optimize(\n",
    "            objective,\n",
    "            n_trials=N_TRIALS,\n",
    "            timeout=TIMEOUT_SECONDS,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        best_params = study.best_params\n",
    "        best_cv_score = study.best_value\n",
    "\n",
    "        print(f\"\\nâœ… Optuna optimization complete ({elapsed_time/60:.1f} min)\")\n",
    "        print(f\"  Best CV F1: {best_cv_score:.4f}\")\n",
    "        print(f\"  Best parameters:\")\n",
    "        for key, value in best_params.items():\n",
    "            print(f\"    {key}: {value}\")\n",
    "\n",
    "        cleanup_memory()\n",
    "        memory_usage()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error in Optuna optimization: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        best_params = {}\n",
    "        best_cv_score = 0.0\n",
    "cleanup_memory()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_training_header",
   "metadata": {
    "id": "final_training_header",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 7. Final Model Training & Threshold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_training",
   "metadata": {
    "id": "final_training",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train final model on full data\n",
    "try:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PHASE 5: Final Model Training & Threshold Tuning\")\n",
    "    print(\"=\" * 80)\n",
    "    phase_start = time.time()\n",
    "    print(\"Training Final Model on Full Dataset...\")\n",
    "\n",
    "    # Use best parameters from Optuna or improved defaults\n",
    "    if 'best_params' not in locals() or not best_params:\n",
    "        print(\"  âš ï¸ best_params not found, using improved defaults\")\n",
    "        best_params = {\n",
    "            'n_layers': 4,\n",
    "            'hidden_dim_base': 512,\n",
    "            'dim_strategy': 'decreasing',\n",
    "            'dropout_rate': 0.3,\n",
    "            'activation': 'swish',\n",
    "            'learning_rate': 0.0005,\n",
    "            'batch_size': 256,\n",
    "            'weight_decay': 1e-3,\n",
    "            'use_batch_norm': True,\n",
    "            'use_residual': False,\n",
    "            'label_smoothing': 0.05\n",
    "        }\n",
    "        best_cv_score = 0.0\n",
    "\n",
    "    # Build hidden_dims from best_params\n",
    "    n_layers = best_params.get('n_layers', 3)\n",
    "    hidden_dim_base = best_params.get('hidden_dim_base', 256)\n",
    "    dim_strategy = best_params.get('dim_strategy', 'decreasing')\n",
    "    \n",
    "    hidden_dims = []\n",
    "    for i in range(n_layers):\n",
    "        if dim_strategy == 'decreasing':\n",
    "            dim = hidden_dim_base // (2 ** i)\n",
    "        else:\n",
    "            dim = hidden_dim_base\n",
    "        hidden_dims.append(max(32, dim))\n",
    "\n",
    "    # Create final model with improved architecture\n",
    "    final_model = MLP(\n",
    "        input_dim=X_train.shape[1],\n",
    "        hidden_dims=tuple(hidden_dims),\n",
    "        dropout_rate=best_params.get('dropout_rate', 0.3),\n",
    "        activation=best_params.get('activation', 'swish'),\n",
    "        use_batch_norm=best_params.get('use_batch_norm', True),\n",
    "        use_residual=best_params.get('use_residual', False)\n",
    "    ).to(device)\n",
    "\n",
    "    # Use AdamW optimizer with improved settings\n",
    "    optimizer = optim.AdamW(\n",
    "        final_model.parameters(), \n",
    "        lr=best_params.get('learning_rate', 0.0005), \n",
    "        weight_decay=best_params.get('weight_decay', 1e-3),\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=15, T_mult=2, eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    # Calculate class weights\n",
    "    pos_weight = (y_train == 0).sum() / max((y_train == 1).sum(), 1)\n",
    "    pos_weight_tensor = torch.tensor(pos_weight, device=device)\n",
    "    \n",
    "    batch_size = best_params.get('batch_size', 256)\n",
    "    label_smoothing = best_params.get('label_smoothing', 0.05)\n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "    X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "    y_val_tensor = torch.FloatTensor(y_val).to(device)\n",
    "\n",
    "    final_model.train()\n",
    "    n_epochs = 150  # More epochs\n",
    "    best_val_f1 = 0.0\n",
    "    patience = 20  # More patience\n",
    "    patience_counter = 0\n",
    "\n",
    "    print(f\"  Training for up to {n_epochs} epochs with early stopping (patience={patience})...\")\n",
    "    print(f\"  Using class weight: {pos_weight:.2f}, label smoothing: {label_smoothing}\")\n",
    "\n",
    "    # Progress bar for epochs\n",
    "    epoch_pbar = tqdm(range(n_epochs), desc=\"Training\", unit=\"epoch\")\n",
    "    for epoch in epoch_pbar:\n",
    "        # Mini-batch training\n",
    "        indices = torch.randperm(len(X_train_tensor), device=device)\n",
    "        batch_losses = []\n",
    "        for i in range(0, len(X_train_tensor), batch_size):\n",
    "            batch_indices = indices[i:i + batch_size]\n",
    "            X_batch = X_train_tensor[batch_indices]\n",
    "            y_batch = y_train_tensor[batch_indices]\n",
    "            \n",
    "            # Label smoothing\n",
    "            if label_smoothing > 0:\n",
    "                y_batch_smooth = y_batch * (1 - label_smoothing) + (1 - y_batch) * label_smoothing\n",
    "            else:\n",
    "                y_batch_smooth = y_batch\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = final_model(X_batch)\n",
    "            loss = nn.BCELoss()(outputs, y_batch_smooth)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(final_model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            batch_losses.append(loss.item())\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Validation every epoch\n",
    "        final_model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = final_model(X_val_tensor)\n",
    "            val_proba = val_outputs.cpu().numpy()\n",
    "            val_preds = (val_proba >= 0.5).astype(int)\n",
    "            val_f1 = f1_score(y_val, val_preds)\n",
    "            \n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                patience_counter = 0\n",
    "                # Save best model state\n",
    "                best_model_state = final_model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    epoch_pbar.set_description(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                    epoch_pbar.close()\n",
    "                    print(f\"\\n  Early stopping at epoch {epoch + 1}\")\n",
    "                    break\n",
    "        \n",
    "        final_model.train()\n",
    "        \n",
    "        # Update progress bar\n",
    "        avg_loss = np.mean(batch_losses) if batch_losses else 0.0\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        epoch_pbar.set_postfix({\n",
    "            'loss': f'{avg_loss:.4f}',\n",
    "            'val_f1': f'{val_f1:.4f}',\n",
    "            'best_f1': f'{best_val_f1:.4f}',\n",
    "            'lr': f'{current_lr:.2e}',\n",
    "            'patience': f'{patience_counter}/{patience}'\n",
    "        })\n",
    "    \n",
    "    epoch_pbar.close()\n",
    "\n",
    "    # Load best model state\n",
    "    if 'best_model_state' in locals():\n",
    "        final_model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Get predictions on validation set\n",
    "    final_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_val_proba = final_model(X_val_tensor).cpu().numpy()\n",
    "\n",
    "    # Find optimal threshold\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(y_val, y_val_proba)\n",
    "    f1_scores_pr = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    best_pr_idx = np.argmax(f1_scores_pr)\n",
    "    best_pr_threshold = pr_thresholds[best_pr_idx] if best_pr_idx < len(pr_thresholds) else 0.5\n",
    "    best_pr_f1 = f1_scores_pr[best_pr_idx]\n",
    "\n",
    "    # Manual fine-grained search\n",
    "    thresholds = np.concatenate([\n",
    "        np.linspace(0.01, 0.05, 20),\n",
    "        np.linspace(0.05, 0.15, 50),\n",
    "        np.linspace(0.15, 0.3, 30),\n",
    "        np.linspace(0.3, 0.9, 20)\n",
    "    ])\n",
    "\n",
    "    best_threshold = best_pr_threshold\n",
    "    best_f1 = best_pr_f1\n",
    "    for thr in thresholds:\n",
    "        y_pred = (y_val_proba >= thr).astype(int)\n",
    "        f1 = f1_score(y_val, y_pred, pos_label=1, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = thr\n",
    "\n",
    "    phase_time = time.time() - phase_start\n",
    "    print(f\"\\nâœ… Final Optimal Threshold: {best_threshold:.4f}\")\n",
    "    print(f\"âœ… Final Validation F1: {best_f1:.4f}\")\n",
    "    print(f\"\\nâ±ï¸  Final Model Training & Threshold Tuning Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\")\n",
    "\n",
    "    # Classification report\n",
    "    y_val_pred = (y_val_proba >= best_threshold).astype(int)\n",
    "    print(\"\\nðŸ“Š Classification Report:\")\n",
    "    print(classification_report(y_val, y_val_pred, digits=4, zero_division=0))\n",
    "\n",
    "    cleanup_memory()\n",
    "    memory_usage()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error in final training: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_model_header",
   "metadata": {
    "id": "save_model_header",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_model",
   "metadata": {
    "id": "save_model",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "try:\n",
    "    model_save_path = MODEL_SAVE_DIR / \"model_pytorch_mlp_all_features_best.pkl\"\n",
    "\n",
    "    save_dict = {\n",
    "        \"model_state_dict\": final_model.state_dict(),\n",
    "        \"model_config\": {\n",
    "            \"input_dim\": X_train.shape[1],\n",
    "            \"hidden_dims\": hidden_dims,\n",
    "            \"dropout_rate\": best_params.get('dropout_rate', 0.2),\n",
    "            \"activation\": best_params.get('activation', 'relu')\n",
    "        },\n",
    "        \"scaler\": scaler,\n",
    "        \"best_params\": best_params,\n",
    "        \"best_cv_score\": best_cv_score,\n",
    "        \"best_threshold\": best_threshold,\n",
    "        \"best_f1\": best_f1,\n",
    "        \"reg_cols\": reg_cols,\n",
    "        \"emb_family_to_cols\": emb_family_to_cols,\n",
    "    }\n",
    "\n",
    "    with open(model_save_path, \"wb\") as f:\n",
    "        pickle.dump(save_dict, f)\n",
    "\n",
    "    print(f\"\\nðŸ’¾ Model saved to: {model_save_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error saving model: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "submission_header",
   "metadata": {
    "id": "submission_header",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 9. Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "submission",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:53:33.017628Z",
     "iopub.status.busy": "2025-11-19T09:53:33.017517Z",
     "iopub.status.idle": "2025-11-19T09:53:36.481762Z",
     "shell.execute_reply": "2025-11-19T09:53:36.480968Z"
    },
    "id": "submission",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_work_id(id_value: str) -> str:\n",
    "    \"\"\"Extract work_id from URL or return as is if already just ID.\"\"\"\n",
    "    id_str = str(id_value)\n",
    "    # If it already looks like a work ID, just return it\n",
    "    if id_str.startswith('W') and len(id_str) > 1 and '/' not in id_str:\n",
    "        return id_str\n",
    "    # Otherwise, extract from URL or string\n",
    "    match = re.search(r'W\\d+', id_str)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return id_str\n",
    "\n",
    "# Load test data and generate predictions\n",
    "try:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PHASE 6: Test Predictions\")\n",
    "    print(\"=\" * 80)\n",
    "    phase_start = time.time()\n",
    "    print(\"Generating Test Predictions...\")\n",
    "\n",
    "    test_df = load_parquet_split(\"test\")\n",
    "    test_ids = test_df[\"id\"].to_numpy()\n",
    "\n",
    "    # Process test data same as train\n",
    "    X_reg_test, X_emb_test_fams, _, _, _ = split_features_reg_and_all_emb(test_df)\n",
    "    del test_df\n",
    "\n",
    "    # Combine embeddings (NO PCA)\n",
    "    X_emb_test_list = []\n",
    "    for fam in X_emb_test_fams.keys():\n",
    "        X_emb_test_list.append(X_emb_test_fams[fam])\n",
    "    X_emb_test = np.hstack(X_emb_test_list) if X_emb_test_list else None\n",
    "\n",
    "    if X_reg_test is not None:\n",
    "        X_test = np.hstack([X_reg_test, X_emb_test]) if X_emb_test is not None else X_reg_test\n",
    "    else:\n",
    "        X_test = X_emb_test\n",
    "\n",
    "    del X_reg_test, X_emb_test_fams, X_emb_test_list, X_emb_test\n",
    "    cleanup_memory()\n",
    "\n",
    "    # Scale\n",
    "    if \"scaler\" in locals():\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Predict in chunks\n",
    "    chunk_size = 10000\n",
    "    final_model.eval()\n",
    "    y_test_proba_chunks = []\n",
    "    \n",
    "    for i in range(0, X_test.shape[0], chunk_size):\n",
    "        X_test_chunk = torch.FloatTensor(X_test[i:i + chunk_size]).to(device)\n",
    "        with torch.no_grad():\n",
    "            chunk_proba = final_model(X_test_chunk).cpu().numpy()\n",
    "        y_test_proba_chunks.append(chunk_proba)\n",
    "        del X_test_chunk\n",
    "        cleanup_memory()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    y_test_proba = np.concatenate(y_test_proba_chunks)\n",
    "    del y_test_proba_chunks\n",
    "\n",
    "    y_test_pred = (y_test_proba >= best_threshold).astype(int)\n",
    "\n",
    "    # Create submission using Polars\n",
    "    work_ids = np.array([extract_work_id(str(id_val)) for id_val in test_ids])\n",
    "    submission_df = pl.DataFrame({\"work_id\": work_ids, \"label\": y_test_pred})\n",
    "\n",
    "    submission_path = SUBMISSION_DIR / \"submission_model_pytorch_mlp.csv\"\n",
    "    submission_df.write_csv(submission_path)\n",
    "\n",
    "    phase_time = time.time() - phase_start\n",
    "    print(f\"\\nâœ… Submission saved to: {submission_path}\")\n",
    "    print(f\"  Test predictions: {len(y_test_pred)}, Positive: {y_test_pred.sum()}, Negative: {(y_test_pred==0).sum()}\")\n",
    "    print(f\"\\nâ±ï¸  Test Predictions Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\")\n",
    "\n",
    "    cleanup_memory()\n",
    "    memory_usage()\n",
    "    \n",
    "    # Print total execution time summary\n",
    "    total_time = time.time() - TOTAL_START_TIME\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    # Print total execution time summary\n",
    "    total_time = time.time() - TOTAL_START_TIME\n",
    "    END_TIME_STR = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL_PYTORCH_MLP EXECUTION COMPLETED\")\n",
    "    print(f\"Start Time: {START_TIME_STR}\")\n",
    "    print(f\"End Time: {END_TIME_STR}\")\n",
    "    print(f\"Total Execution Time: {total_time:.2f} seconds ({total_time/60:.2f} minutes / {total_time/3600:.2f} hours)\")\n",
    "    print(f\"Final Validation F1 Score: {best_f1:.4f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(f\"End Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Final Validation F1 Score: {best_f1:.4f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    total_time = time.time() - TOTAL_START_TIME\n",
    "    print(f\"\\nâŒ Error generating submission: {e}\")\n",
    "    print(f\"Execution failed after {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "src/notebooks/model_pytorch_mlp_all_features.ipynb",
   "output_path": "/scratch/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/runs/model_pytorch_mlp_executed_20251119-184753.ipynb",
   "parameters": {},
   "start_time": "2025-11-19T23:48:01.731634",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}