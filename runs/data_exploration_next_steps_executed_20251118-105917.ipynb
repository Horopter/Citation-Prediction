{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c84e1595",
   "metadata": {
    "papermill": {
     "duration": 0.003549,
     "end_time": "2025-11-18T15:59:27.031644",
     "exception": false,
     "start_time": "2025-11-18T15:59:27.028095",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Exploration: Next Steps (Polars-only, Scalable)\n",
    "\n",
    "This notebook assumes that you have already run **`data_exploration_organized.ipynb`**.\n",
    "\n",
    "From that notebook, the following outputs are expected in `data/results`:\n",
    "\n",
    "- Base tabular features: `X_train.parquet`, `X_val.parquet`, `X_test.parquet`\n",
    "- Labels (if supervised): `y_train.npy`, `y_val.npy`\n",
    "- NLP embedding features, per model and split, for example:\n",
    "  - `sent_transformer_X_train.parquet`, `sent_transformer_X_val.parquet`, `sent_transformer_X_test.parquet`\n",
    "  - `scibert_X_train.parquet`, `scibert_X_val.parquet`, `scibert_X_test.parquet`\n",
    "  - (optionally) `specter2_X_train.parquet`, `specter2_X_val.parquet`, `specter2_X_test.parquet`\n",
    "\n",
    "Each embedding parquet contains an `id` column plus one column per embedding dimension\n",
    "(e.g. `sent_transformer_dim_0`, `sent_transformer_dim_1`, ...).\n",
    "\n",
    "This notebook will:\n",
    "\n",
    "1. Load the **base feature matrices** and all available **embedding parquets**\n",
    "2. Merge embeddings into the base feature matrices by `id`\n",
    "3. Inspect and impute missing values in a train-centric way\n",
    "4. Replace `is_oa` with derived feature `is_not_oa` (1 - is_oa) for better signal\n",
    "5. Run a set of statistical analyses (Pearson, Spearman, Chi-square, Cram√©r's V, ANOVA, Tukey's HSD)\n",
    "6. Keep **all non-embedding features** (no feature reduction) to preserve signals that may be sparse in sample but informative in full dataset\n",
    "7. Save model-ready train / validation / test parquet files for downstream ML\n",
    "\n",
    "Implementation is **Polars-only** for dataframes (no pandas). NumPy, SciPy and statsmodels are used only for\n",
    "vectorised numerical routines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c919c96c",
   "metadata": {
    "papermill": {
     "duration": 0.002895,
     "end_time": "2025-11-18T15:59:27.037839",
     "exception": false,
     "start_time": "2025-11-18T15:59:27.034944",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Imports & directory configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb87d44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T15:59:27.044535Z",
     "iopub.status.busy": "2025-11-18T15:59:27.044373Z",
     "iopub.status.idle": "2025-11-18T16:01:27.815413Z",
     "shell.execute_reply": "2025-11-18T16:01:27.814736Z"
    },
    "papermill": {
     "duration": 120.778878,
     "end_time": "2025-11-18T16:01:27.819656",
     "exception": false,
     "start_time": "2025-11-18T15:59:27.040778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2\n",
      "DATA_DIR:     /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/processed\n",
      "RESULTS_DIR:  /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/results\n",
      "MODEL_DIR:    /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/model_ready\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Optional: statsmodels for Tukey's HSD\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "    HAS_STATSMODELS = True\n",
    "except ImportError:\n",
    "    HAS_STATSMODELS = False\n",
    "    print(\"‚ö†Ô∏è statsmodels not found. Tukey's HSD will be skipped unless you install statsmodels.\")\n",
    "\n",
    "_current_dir = Path(os.getcwd()).parent.parent\n",
    "if (_current_dir / \"data\").exists():\n",
    "    PROJECT_ROOT = _current_dir\n",
    "else:\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "DATA_DIR    = PROJECT_ROOT / \"data/processed\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"data/results\"\n",
    "MODEL_DIR   = PROJECT_ROOT / \"data/model_ready\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")\n",
    "print(f\"DATA_DIR:     {DATA_DIR}\")\n",
    "print(f\"RESULTS_DIR:  {RESULTS_DIR}\")\n",
    "print(f\"MODEL_DIR:    {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cc70a5",
   "metadata": {
    "papermill": {
     "duration": 0.002976,
     "end_time": "2025-11-18T16:01:27.825911",
     "exception": false,
     "start_time": "2025-11-18T16:01:27.822935",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Load base feature matrices & labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd3d4c1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T16:01:27.833061Z",
     "iopub.status.busy": "2025-11-18T16:01:27.832779Z",
     "iopub.status.idle": "2025-11-18T16:01:52.023296Z",
     "shell.execute_reply": "2025-11-18T16:01:52.022675Z"
    },
    "papermill": {
     "duration": 24.195107,
     "end_time": "2025-11-18T16:01:52.024052",
     "exception": false,
     "start_time": "2025-11-18T16:01:27.828945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base feature file locations:\n",
      "   /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/results/X_train.parquet\n",
      "   /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/results/X_val.parquet\n",
      "   /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/results/X_test.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base shapes (before merging embeddings):\n",
      "  X_train_base: (960000, 57)\n",
      "  X_val_base:   (120000, 57)\n",
      "  X_test_base:  (120000, 57)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded labels:\n",
      "  y_train: (960000,)\n",
      "  y_val:   (120000,)\n"
     ]
    }
   ],
   "source": [
    "X_train_path = RESULTS_DIR / \"X_train.parquet\"\n",
    "X_val_path   = RESULTS_DIR / \"X_val.parquet\"\n",
    "X_test_path  = RESULTS_DIR / \"X_test.parquet\"\n",
    "\n",
    "print(\"Base feature file locations:\")\n",
    "print(\"  \", X_train_path)\n",
    "print(\"  \", X_val_path)\n",
    "print(\"  \", X_test_path)\n",
    "\n",
    "if not X_train_path.exists() or not X_val_path.exists() or not X_test_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"One or more X_*.parquet files are missing in data/results. \"\n",
    "        \"Please run data_exploration_organized.ipynb first.\"\n",
    "    )\n",
    "\n",
    "X_train_base = pl.read_parquet(X_train_path)\n",
    "X_val_base   = pl.read_parquet(X_val_path)\n",
    "X_test_base  = pl.read_parquet(X_test_path)\n",
    "\n",
    "print(\"\\nBase shapes (before merging embeddings):\")\n",
    "print(\"  X_train_base:\", X_train_base.shape)\n",
    "print(\"  X_val_base:  \", X_val_base.shape)\n",
    "print(\"  X_test_base: \", X_test_base.shape)\n",
    "\n",
    "y_train_path = RESULTS_DIR / \"y_train.npy\"\n",
    "y_val_path   = RESULTS_DIR / \"y_val.npy\"\n",
    "\n",
    "y_train = np.load(y_train_path) if y_train_path.exists() else None\n",
    "y_val   = np.load(y_val_path)   if y_val_path.exists()   else None\n",
    "\n",
    "if y_train is not None:\n",
    "    print(\"\\nLoaded labels:\")\n",
    "    print(\"  y_train:\", y_train.shape)\n",
    "    if y_val is not None:\n",
    "        print(\"  y_val:  \", y_val.shape)\n",
    "else:\n",
    "    print(\"\\nNo y_train.npy found. Target-based analyses will be limited.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3553d1",
   "metadata": {
    "papermill": {
     "duration": 0.003204,
     "end_time": "2025-11-18T16:01:52.031653",
     "exception": false,
     "start_time": "2025-11-18T16:01:52.028449",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Discover and merge embedding parquets by `id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aa10d23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T16:01:52.039051Z",
     "iopub.status.busy": "2025-11-18T16:01:52.038855Z",
     "iopub.status.idle": "2025-11-18T16:06:25.694161Z",
     "shell.execute_reply": "2025-11-18T16:06:25.693237Z"
    },
    "papermill": {
     "duration": 273.662318,
     "end_time": "2025-11-18T16:06:25.697114",
     "exception": false,
     "start_time": "2025-11-18T16:01:52.034796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered embedding models (based on *_X_train.parquet):\n",
      "['scibert', 'sent_transformer', 'specter2']\n",
      "\n",
      "üîó Merging embeddings for model: scibert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train: 57 -> 825 columns after merging scibert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val:   57 -> 825 columns after merging scibert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  test:  57 -> 825 columns after merging scibert\n",
      "\n",
      "üîó Merging embeddings for model: sent_transformer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train: 825 -> 1209 columns after merging sent_transformer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val:   825 -> 1209 columns after merging sent_transformer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  test:  825 -> 1209 columns after merging sent_transformer\n",
      "\n",
      "üîó Merging embeddings for model: specter2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train: 1209 -> 1977 columns after merging specter2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val:   1209 -> 1977 columns after merging specter2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  test:  1209 -> 1977 columns after merging specter2\n",
      "\n",
      "Shapes AFTER embedding merge:\n",
      "  X_train: (960000, 1977)\n",
      "  X_val:   (120000, 1977)\n",
      "  X_test:  (120000, 1977)\n"
     ]
    }
   ],
   "source": [
    "def discover_embedding_models(results_dir: Path) -> List[str]:\n",
    "    models: List[str] = []\n",
    "    for p in results_dir.glob(\"*_X_train.parquet\"):\n",
    "        name = p.name.replace(\"_X_train.parquet\", \"\")\n",
    "        if name in {\"X\", \"base\", \"features\"}:\n",
    "            continue\n",
    "        models.append(name)\n",
    "    return sorted(set(models))\n",
    "\n",
    "embedding_models = discover_embedding_models(RESULTS_DIR)\n",
    "print(\"Discovered embedding models (based on *_X_train.parquet):\")\n",
    "print(embedding_models if embedding_models else \"(none)\")\n",
    "\n",
    "def load_embedding_split(model_name: str, split: str) -> pl.DataFrame:\n",
    "    path = RESULTS_DIR / f\"{model_name}_X_{split}.parquet\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Expected embedding file not found: {path}\")\n",
    "    df = pl.read_parquet(path)\n",
    "    if \"id\" not in df.columns:\n",
    "        raise ValueError(f\"Embedding file {path} does not contain an 'id' column.\")\n",
    "    cols = [\"id\"] + [c for c in df.columns if c != \"id\"]\n",
    "    return df.select(cols)\n",
    "\n",
    "def merge_embeddings(\n",
    "    X_train_base: pl.DataFrame,\n",
    "    X_val_base: pl.DataFrame,\n",
    "    X_test_base: pl.DataFrame,\n",
    "    models: List[str],\n",
    ") -> tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:\n",
    "    X_train = X_train_base\n",
    "    X_val   = X_val_base\n",
    "    X_test  = X_test_base\n",
    "\n",
    "    if \"id\" not in X_train.columns:\n",
    "        raise ValueError(\"Base feature matrices must contain an 'id' column for merging embeddings.\")\n",
    "\n",
    "    for model_name in models:\n",
    "        # Check if all required files exist for this model\n",
    "        required_files = [\n",
    "            RESULTS_DIR / f\"{model_name}_X_train.parquet\",\n",
    "            RESULTS_DIR / f\"{model_name}_X_val.parquet\",\n",
    "            RESULTS_DIR / f\"{model_name}_X_test.parquet\"\n",
    "        ]\n",
    "        missing_files = [f for f in required_files if not f.exists()]\n",
    "        \n",
    "        if missing_files:\n",
    "            print(f\"\\n‚ö†Ô∏è  Skipping {model_name} embeddings - missing files:\")\n",
    "            for f in missing_files:\n",
    "                print(f\"     {f.name}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nüîó Merging embeddings for model: {model_name}\")\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            try:\n",
    "                emb_df = load_embedding_split(model_name, split)\n",
    "                if split == \"train\":\n",
    "                    before_cols = len(X_train.columns)\n",
    "                    X_train = X_train.join(emb_df, on=\"id\", how=\"left\")\n",
    "                    after_cols = len(X_train.columns)\n",
    "                    print(f\"  train: {before_cols} -> {after_cols} columns after merging {model_name}\")\n",
    "                elif split == \"val\":\n",
    "                    before_cols = len(X_val.columns)\n",
    "                    X_val = X_val.join(emb_df, on=\"id\", how=\"left\")\n",
    "                    after_cols = len(X_val.columns)\n",
    "                    print(f\"  val:   {before_cols} -> {after_cols} columns after merging {model_name}\")\n",
    "                else:\n",
    "                    before_cols = len(X_test.columns)\n",
    "                    X_test = X_test.join(emb_df, on=\"id\", how=\"left\")\n",
    "                    after_cols = len(X_test.columns)\n",
    "                    print(f\"  test:  {before_cols} -> {after_cols} columns after merging {model_name}\")\n",
    "            except (FileNotFoundError, ValueError) as e:\n",
    "                print(f\"  ‚ö†Ô∏è  Skipping {split} split for {model_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "if embedding_models:\n",
    "    X_train, X_val, X_test = merge_embeddings(X_train_base, X_val_base, X_test_base, embedding_models)\n",
    "else:\n",
    "    print(\"\\nNo embedding parquets discovered; using base features only.\")\n",
    "    X_train, X_val, X_test = X_train_base, X_val_base, X_test_base\n",
    "\n",
    "print(\"\\nShapes AFTER embedding merge:\")\n",
    "print(\"  X_train:\", X_train.shape)\n",
    "print(\"  X_val:  \", X_val.shape)\n",
    "print(\"  X_test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9303b9",
   "metadata": {
    "papermill": {
     "duration": 0.00508,
     "end_time": "2025-11-18T16:06:25.708128",
     "exception": false,
     "start_time": "2025-11-18T16:06:25.703048",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Dataset overview & missingness diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b62b2b57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T16:06:25.716774Z",
     "iopub.status.busy": "2025-11-18T16:06:25.716539Z",
     "iopub.status.idle": "2025-11-18T16:06:39.016431Z",
     "shell.execute_reply": "2025-11-18T16:06:39.015771Z"
    },
    "papermill": {
     "duration": 13.305267,
     "end_time": "2025-11-18T16:06:39.017003",
     "exception": false,
     "start_time": "2025-11-18T16:06:25.711736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Overview: X_train (with embeddings)\n",
      "================================================================================\n",
      "Shape: (960000, 1977)\n",
      "\n",
      "Dtypes:\n",
      "[Int64, Int64, Float64, Float64, Float64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, String, Int64, Int64, Int64, String, Int64, Float64, Int64, Float64, Float64, Float64, Int64, Float64, Float64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, String, Int64, Int64, Int64, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32]\n",
      "\n",
      "Head:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 1_977)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ abstract_ ‚îÜ abstract_ ‚îÜ avg_autho ‚îÜ avg_autho ‚îÜ ‚Ä¶ ‚îÜ specter2_ ‚îÜ specter2_ ‚îÜ specter2_ ‚îÜ specter2 ‚îÇ\n",
      "‚îÇ length    ‚îÜ word_coun ‚îÜ r_citatio ‚îÜ r_h_index ‚îÜ   ‚îÜ dim_96    ‚îÜ dim_97    ‚îÜ dim_98    ‚îÜ _dim_99  ‚îÇ\n",
      "‚îÇ ---       ‚îÜ t         ‚îÜ ns        ‚îÜ ---       ‚îÜ   ‚îÜ ---       ‚îÜ ---       ‚îÜ ---       ‚îÜ ---      ‚îÇ\n",
      "‚îÇ i64       ‚îÜ ---       ‚îÜ ---       ‚îÜ f64       ‚îÜ   ‚îÜ f32       ‚îÜ f32       ‚îÜ f32       ‚îÜ f32      ‚îÇ\n",
      "‚îÇ           ‚îÜ i64       ‚îÜ f64       ‚îÜ           ‚îÜ   ‚îÜ           ‚îÜ           ‚îÜ           ‚îÜ          ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ 992       ‚îÜ 138       ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -0.900445 ‚îÜ 0.003919  ‚îÜ 0.136321  ‚îÜ 0.725182 ‚îÇ\n",
      "‚îÇ 0         ‚îÜ 0         ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -0.463011 ‚îÜ -0.412438 ‚îÜ -0.185211 ‚îÜ 0.617742 ‚îÇ\n",
      "‚îÇ 3101      ‚îÜ 455       ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -0.517267 ‚îÜ -0.262704 ‚îÜ 0.400528  ‚îÜ 0.385779 ‚îÇ\n",
      "‚îÇ 0         ‚îÜ 0         ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -0.562287 ‚îÜ 0.268097  ‚îÜ 0.31879   ‚îÜ 0.323413 ‚îÇ\n",
      "‚îÇ 1033      ‚îÜ 163       ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -0.790856 ‚îÜ -0.280721 ‚îÜ 0.670876  ‚îÜ 0.414337 ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "================================================================================\n",
      "Overview: X_val (with embeddings)\n",
      "================================================================================\n",
      "Shape: (120000, 1977)\n",
      "\n",
      "Dtypes:\n",
      "[Int64, Int64, Float64, Float64, Float64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, String, Int64, Int64, Int64, String, Int64, Float64, Int64, Float64, Float64, Float64, Int64, Float64, Float64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, String, Int64, Int64, Int64, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32]\n",
      "\n",
      "Head:\n",
      "shape: (5, 1_977)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ abstract_ ‚îÜ abstract_ ‚îÜ avg_autho ‚îÜ avg_autho ‚îÜ ‚Ä¶ ‚îÜ specter2_ ‚îÜ specter2_ ‚îÜ specter2_ ‚îÜ specter2 ‚îÇ\n",
      "‚îÇ length    ‚îÜ word_coun ‚îÜ r_citatio ‚îÜ r_h_index ‚îÜ   ‚îÜ dim_96    ‚îÜ dim_97    ‚îÜ dim_98    ‚îÜ _dim_99  ‚îÇ\n",
      "‚îÇ ---       ‚îÜ t         ‚îÜ ns        ‚îÜ ---       ‚îÜ   ‚îÜ ---       ‚îÜ ---       ‚îÜ ---       ‚îÜ ---      ‚îÇ\n",
      "‚îÇ i64       ‚îÜ ---       ‚îÜ ---       ‚îÜ f64       ‚îÜ   ‚îÜ f32       ‚îÜ f32       ‚îÜ f32       ‚îÜ f32      ‚îÇ\n",
      "‚îÇ           ‚îÜ i64       ‚îÜ f64       ‚îÜ           ‚îÜ   ‚îÜ           ‚îÜ           ‚îÜ           ‚îÜ          ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ 1184      ‚îÜ 187       ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -0.412907 ‚îÜ 0.27174   ‚îÜ -0.014599 ‚îÜ 0.856591 ‚îÇ\n",
      "‚îÇ 534       ‚îÜ 76        ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -1.065146 ‚îÜ -0.152267 ‚îÜ 0.442473  ‚îÜ 0.595237 ‚îÇ\n",
      "‚îÇ 1305      ‚îÜ 149       ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -0.242316 ‚îÜ 0.472974  ‚îÜ -0.267344 ‚îÜ 0.659313 ‚îÇ\n",
      "‚îÇ 0         ‚îÜ 0         ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -0.624002 ‚îÜ 0.151777  ‚îÜ 0.366445  ‚îÜ 0.229336 ‚îÇ\n",
      "‚îÇ 2058      ‚îÜ 278       ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -0.90492  ‚îÜ -0.486897 ‚îÜ -0.154748 ‚îÜ 0.5964   ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "================================================================================\n",
      "Overview: X_test (with embeddings)\n",
      "================================================================================\n",
      "Shape: (120000, 1977)\n",
      "\n",
      "Dtypes:\n",
      "[Int64, Int64, Float64, Float64, Float64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, String, Int64, Int64, Int64, String, Int64, Float64, Int64, Float64, Float64, Float64, Int64, Float64, Float64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, String, Int64, Int64, Int64, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32]\n",
      "\n",
      "Head:\n",
      "shape: (5, 1_977)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ abstract_ ‚îÜ abstract_ ‚îÜ avg_autho ‚îÜ avg_autho ‚îÜ ‚Ä¶ ‚îÜ specter2_ ‚îÜ specter2_ ‚îÜ specter2_ ‚îÜ specter2 ‚îÇ\n",
      "‚îÇ length    ‚îÜ word_coun ‚îÜ r_citatio ‚îÜ r_h_index ‚îÜ   ‚îÜ dim_96    ‚îÜ dim_97    ‚îÜ dim_98    ‚îÜ _dim_99  ‚îÇ\n",
      "‚îÇ ---       ‚îÜ t         ‚îÜ ns        ‚îÜ ---       ‚îÜ   ‚îÜ ---       ‚îÜ ---       ‚îÜ ---       ‚îÜ ---      ‚îÇ\n",
      "‚îÇ i64       ‚îÜ ---       ‚îÜ ---       ‚îÜ f64       ‚îÜ   ‚îÜ f32       ‚îÜ f32       ‚îÜ f32       ‚îÜ f32      ‚îÇ\n",
      "‚îÇ           ‚îÜ i64       ‚îÜ f64       ‚îÜ           ‚îÜ   ‚îÜ           ‚îÜ           ‚îÜ           ‚îÜ          ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ 502       ‚îÜ 78        ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -1.072871 ‚îÜ -0.183582 ‚îÜ 0.240901  ‚îÜ -0.07718 ‚îÇ\n",
      "‚îÇ           ‚îÜ           ‚îÜ           ‚îÜ           ‚îÜ   ‚îÜ           ‚îÜ           ‚îÜ           ‚îÜ 6        ‚îÇ\n",
      "‚îÇ 584       ‚îÜ 83        ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -0.654841 ‚îÜ 0.270539  ‚îÜ 0.130878  ‚îÜ 0.488515 ‚îÇ\n",
      "‚îÇ 803       ‚îÜ 117       ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -0.980761 ‚îÜ 0.350272  ‚îÜ 0.478677  ‚îÜ 0.365676 ‚îÇ\n",
      "‚îÇ 0         ‚îÜ 0         ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -0.249309 ‚îÜ -0.040984 ‚îÜ -0.003698 ‚îÜ 0.509199 ‚îÇ\n",
      "‚îÇ 0         ‚îÜ 0         ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -0.216537 ‚îÜ -0.247736 ‚îÜ 0.009335  ‚îÜ 0.661677 ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns with most missing values (train perspective):\n",
      "shape: (30, 7)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ column            ‚îÜ train_missing ‚îÜ val_missing ‚îÜ test_missing ‚îÜ train_pct ‚îÜ val_pct  ‚îÜ test_pct ‚îÇ\n",
      "‚îÇ ---               ‚îÜ ---           ‚îÜ ---         ‚îÜ ---          ‚îÜ ---       ‚îÜ ---      ‚îÜ ---      ‚îÇ\n",
      "‚îÇ str               ‚îÜ i64           ‚îÜ i64         ‚îÜ i64          ‚îÜ f64       ‚îÜ f64      ‚îÜ f64      ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ oa_status_bronze  ‚îÜ 896574        ‚îÜ 112319      ‚îÜ 112234       ‚îÜ 0.933931  ‚îÜ 0.935992 ‚îÜ 0.935283 ‚îÇ\n",
      "‚îÇ oa_status_hybrid  ‚îÜ 894459        ‚îÜ 112332      ‚îÜ 112282       ‚îÜ 0.931728  ‚îÜ 0.9361   ‚îÜ 0.935683 ‚îÇ\n",
      "‚îÇ oa_status_green   ‚îÜ 884087        ‚îÜ 110609      ‚îÜ 110525       ‚îÜ 0.920924  ‚îÜ 0.921742 ‚îÜ 0.921042 ‚îÇ\n",
      "‚îÇ oa_status_diamond ‚îÜ 855879        ‚îÜ 107382      ‚îÜ 99511        ‚îÜ 0.891541  ‚îÜ 0.89485  ‚îÜ 0.829258 ‚îÇ\n",
      "‚îÇ oa_status_gold    ‚îÜ 794909        ‚îÜ 99271       ‚îÜ 99147        ‚îÜ 0.82803   ‚îÜ 0.827258 ‚îÜ 0.826225 ‚îÇ\n",
      "‚îÇ ‚Ä¶                 ‚îÜ ‚Ä¶             ‚îÜ ‚Ä¶           ‚îÜ ‚Ä¶            ‚îÜ ‚Ä¶         ‚îÜ ‚Ä¶        ‚îÜ ‚Ä¶        ‚îÇ\n",
      "‚îÇ language          ‚îÜ 0             ‚îÜ 0           ‚îÜ 0            ‚îÜ 0.0       ‚îÜ 0.0      ‚îÜ 0.0      ‚îÇ\n",
      "‚îÇ max_author_h_inde ‚îÜ 0             ‚îÜ 0           ‚îÜ 0            ‚îÜ 0.0       ‚îÜ 0.0      ‚îÜ 0.0      ‚îÇ\n",
      "‚îÇ x                 ‚îÜ               ‚îÜ             ‚îÜ              ‚îÜ           ‚îÜ          ‚îÜ          ‚îÇ\n",
      "‚îÇ max_concept_score ‚îÜ 0             ‚îÜ 0           ‚îÜ 0            ‚îÜ 0.0       ‚îÜ 0.0      ‚îÜ 0.0      ‚îÇ\n",
      "‚îÇ max_topic_score   ‚îÜ 0             ‚îÜ 0           ‚îÜ 0            ‚îÜ 0.0       ‚îÜ 0.0      ‚îÜ 0.0      ‚îÇ\n",
      "‚îÇ nlp_avg_sentence_ ‚îÜ 0             ‚îÜ 0           ‚îÜ 0            ‚îÜ 0.0       ‚îÜ 0.0      ‚îÜ 0.0      ‚îÇ\n",
      "‚îÇ length            ‚îÜ               ‚îÜ             ‚îÜ              ‚îÜ           ‚îÜ          ‚îÜ          ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Missingness report saved to: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/model_ready/missingness_report.csv\n"
     ]
    }
   ],
   "source": [
    "def describe_dataframe(df: pl.DataFrame, name: str) -> None:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Overview: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"Shape:\", df.shape)\n",
    "    print(\"\\nDtypes:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"\\nHead:\")\n",
    "    print(df.head())\n",
    "\n",
    "describe_dataframe(X_train, \"X_train (with embeddings)\")\n",
    "describe_dataframe(X_val,   \"X_val (with embeddings)\")\n",
    "describe_dataframe(X_test,  \"X_test (with embeddings)\")\n",
    "\n",
    "train_nulls = X_train.select(pl.all().null_count()).to_dicts()[0]\n",
    "val_nulls   = X_val.select(pl.all().null_count()).to_dicts()[0]\n",
    "test_nulls  = X_test.select(pl.all().null_count()).to_dicts()[0]\n",
    "\n",
    "rows_train = X_train.height\n",
    "rows_val   = X_val.height\n",
    "rows_test  = X_test.height\n",
    "\n",
    "records = []\n",
    "for col in X_train.columns:\n",
    "    records.append({\n",
    "        \"column\": col,\n",
    "        \"train_missing\": train_nulls.get(col, 0),\n",
    "        \"val_missing\":   val_nulls.get(col, 0),\n",
    "        \"test_missing\":  test_nulls.get(col, 0),\n",
    "        \"train_pct\": train_nulls.get(col, 0) / rows_train if rows_train > 0 else 0.0,\n",
    "        \"val_pct\":   val_nulls.get(col, 0) / rows_val   if rows_val > 0 else 0.0,\n",
    "        \"test_pct\":  test_nulls.get(col, 0) / rows_test  if rows_test > 0 else 0.0,\n",
    "    })\n",
    "\n",
    "missing_report = pl.DataFrame(records).sort(\"train_missing\", descending=True)\n",
    "print(\"\\nColumns with most missing values (train perspective):\")\n",
    "print(missing_report.head(30))\n",
    "\n",
    "missing_report_path = MODEL_DIR / \"missingness_report.csv\"\n",
    "missing_report.write_csv(missing_report_path)\n",
    "print(f\"\\nüíæ Missingness report saved to: {missing_report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1914f67b",
   "metadata": {
    "papermill": {
     "duration": 0.004515,
     "end_time": "2025-11-18T16:06:39.028706",
     "exception": false,
     "start_time": "2025-11-18T16:06:39.024191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Train-centric missing value imputation (including embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e94021e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T16:06:39.038691Z",
     "iopub.status.busy": "2025-11-18T16:06:39.038493Z",
     "iopub.status.idle": "2025-11-18T16:06:57.034562Z",
     "shell.execute_reply": "2025-11-18T16:06:57.033881Z"
    },
    "papermill": {
     "duration": 18.002084,
     "end_time": "2025-11-18T16:06:57.035263",
     "exception": false,
     "start_time": "2025-11-18T16:06:39.033179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Features by type (after dropping all-missing columns)\n",
      "Numeric features    : 1974\n",
      "Categorical features: 2\n",
      "Embedding features  : 1920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample numeric impute values:\n",
      "  abstract_length: 219.0\n",
      "  abstract_word_count: 31.0\n",
      "  avg_author_citations: 0.0\n",
      "  avg_author_h_index: 0.0\n",
      "  avg_concept_score: 0.34736020060606065\n",
      "  avg_topic_score: 0.0\n",
      "  first_author_citations: 0.0\n",
      "  first_author_h_index: 0.0\n",
      "  first_author_papers: 0.0\n",
      "  has_abstract: 1.0\n",
      "\n",
      "Sample categorical impute values:\n",
      "  language: en\n",
      "  type: article\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post-imputation missingness (train):\n",
      "shape: (1_977, 1)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ column_0 ‚îÇ\n",
      "‚îÇ ---      ‚îÇ\n",
      "‚îÇ u32      ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ 0        ‚îÇ\n",
      "‚îÇ 0        ‚îÇ\n",
      "‚îÇ 0        ‚îÇ\n",
      "‚îÇ 0        ‚îÇ\n",
      "‚îÇ 0        ‚îÇ\n",
      "‚îÇ ‚Ä¶        ‚îÇ\n",
      "‚îÇ 0        ‚îÇ\n",
      "‚îÇ 0        ‚îÇ\n",
      "‚îÇ 0        ‚îÇ\n",
      "‚îÇ 0        ‚îÇ\n",
      "‚îÇ 0        ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "üíæ Imputation spec saved to: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/model_ready/imputation_spec.json\n",
      "\n",
      "üîÑ Replacing is_oa with is_not_oa (derived feature)...\n",
      "‚úÖ Replaced is_oa with is_not_oa in all datasets\n"
     ]
    }
   ],
   "source": [
    "id_cols = [c for c in X_train.columns if c.lower() == \"id\"]\n",
    "feature_cols = [c for c in X_train.columns if c not in id_cols]\n",
    "\n",
    "train_nulls = X_train.select(pl.all().null_count()).to_dicts()[0]\n",
    "all_missing_cols = [c for c in feature_cols if train_nulls.get(c, 0) == X_train.height]\n",
    "\n",
    "if all_missing_cols:\n",
    "    print(\"Dropping columns that are entirely missing in train:\")\n",
    "    print(all_missing_cols)\n",
    "    X_train = X_train.drop(all_missing_cols)\n",
    "    X_val   = X_val.drop([c for c in all_missing_cols if c in X_val.columns])\n",
    "    X_test  = X_test.drop([c for c in all_missing_cols if c in X_test.columns])\n",
    "    feature_cols = [c for c in feature_cols if c not in all_missing_cols]\n",
    "\n",
    "embedding_cols: List[str] = []\n",
    "for m in embedding_models:\n",
    "    prefix = f\"{m}_\"\n",
    "    embedding_cols.extend([c for c in feature_cols if c.startswith(prefix)])\n",
    "embedding_cols = sorted(set(embedding_cols))\n",
    "\n",
    "numeric_cols: List[str] = []\n",
    "categorical_cols: List[str] = []\n",
    "\n",
    "NUMERIC_DTYPES = {\n",
    "    pl.Int8, pl.Int16, pl.Int32, pl.Int64,\n",
    "    pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,\n",
    "    pl.Float32, pl.Float64\n",
    "}\n",
    "\n",
    "for c, dt in zip(X_train.columns, X_train.dtypes):\n",
    "    if c in feature_cols:\n",
    "        if dt in NUMERIC_DTYPES:\n",
    "            numeric_cols.append(c)\n",
    "        else:\n",
    "            categorical_cols.append(c)\n",
    "\n",
    "print(\"\\n# Features by type (after dropping all-missing columns)\")\n",
    "print(\"Numeric features    :\", len(numeric_cols))\n",
    "print(\"Categorical features:\", len(categorical_cols))\n",
    "print(\"Embedding features  :\", len(embedding_cols))\n",
    "\n",
    "numeric_exprs = [pl.col(c).median().alias(c) for c in numeric_cols]\n",
    "numeric_medians_row = X_train.select(numeric_exprs).to_dicts()[0] if numeric_exprs else {}\n",
    "numeric_impute_values: Dict[str, float] = {c: float(v) for c, v in numeric_medians_row.items()}\n",
    "\n",
    "categorical_impute_values: Dict[str, Any] = {}\n",
    "for col in categorical_cols:\n",
    "    mode_df = X_train.select(pl.col(col).mode())\n",
    "    if mode_df.height > 0:\n",
    "        mode_val = mode_df.select(pl.col(col))[0, 0]\n",
    "        categorical_impute_values[col] = mode_val\n",
    "    else:\n",
    "        categorical_impute_values[col] = \"__missing__\"\n",
    "\n",
    "print(\"\\nSample numeric impute values:\")\n",
    "for k in list(numeric_impute_values.keys())[:10]:\n",
    "    print(f\"  {k}: {numeric_impute_values[k]}\")\n",
    "\n",
    "print(\"\\nSample categorical impute values:\")\n",
    "for k in list(categorical_impute_values.keys())[:10]:\n",
    "    print(f\"  {k}: {categorical_impute_values[k]}\")\n",
    "\n",
    "def apply_imputation(\n",
    "    df: pl.DataFrame,\n",
    "    numeric_values: Dict[str, float],\n",
    "    categorical_values: Dict[str, Any],\n",
    "    id_cols: List[str],\n",
    ") -> pl.DataFrame:\n",
    "    out = df\n",
    "\n",
    "    for col, val in numeric_values.items():\n",
    "        if col in out.columns:\n",
    "            out = out.with_columns(pl.col(col).cast(pl.Float64).fill_null(val))\n",
    "\n",
    "    for col, val in categorical_values.items():\n",
    "        if col in out.columns:\n",
    "            out = out.with_columns(pl.col(col).cast(pl.Utf8).fill_null(str(val)))\n",
    "\n",
    "    remaining_nulls = out.select(pl.all().null_count()).to_dicts()[0]\n",
    "    remaining_cols = [c for c, cnt in remaining_nulls.items() if cnt > 0]\n",
    "    if remaining_cols:\n",
    "        print(\"\\nColumns still with missing values after primary imputation:\", remaining_cols)\n",
    "        for c in remaining_cols:\n",
    "            if c in numeric_values:\n",
    "                out = out.with_columns(pl.col(c).fill_null(0.0))\n",
    "            else:\n",
    "                out = out.with_columns(pl.col(c).cast(pl.Utf8).fill_null(\"__missing__\"))\n",
    "\n",
    "    ordered_cols = id_cols + [c for c in out.columns if c not in id_cols]\n",
    "    return out.select(ordered_cols)\n",
    "\n",
    "X_train_imputed = apply_imputation(X_train, numeric_impute_values, categorical_impute_values, id_cols)\n",
    "X_val_imputed   = apply_imputation(X_val,   numeric_impute_values, categorical_impute_values, id_cols)\n",
    "X_test_imputed  = apply_imputation(X_test,  numeric_impute_values, categorical_impute_values, id_cols)\n",
    "\n",
    "print(\"\\nPost-imputation missingness (train):\")\n",
    "print(X_train_imputed.select(pl.all().null_count()).transpose())\n",
    "\n",
    "import json\n",
    "impute_spec = {\n",
    "    \"numeric_impute_values\": numeric_impute_values,\n",
    "    \"categorical_impute_values\": {k: str(v) for k, v in categorical_impute_values.items()},\n",
    "    \"dropped_all_missing_cols\": all_missing_cols,\n",
    "    \"id_cols\": id_cols,\n",
    "    \"embedding_models\": embedding_models,\n",
    "    \"embedding_cols\": embedding_cols,\n",
    "}\n",
    "impute_spec_path = MODEL_DIR / \"imputation_spec.json\"\n",
    "with impute_spec_path.open(\"w\") as f:\n",
    "    json.dump(impute_spec, f, indent=2)\n",
    "print(f\"\\nüíæ Imputation spec saved to: {impute_spec_path}\")\n",
    "\n",
    "# Replace is_oa with is_not_oa (derived feature: 1 - is_oa)\n",
    "# This provides better signal for class imbalance scenarios\n",
    "if \"is_oa\" in X_train_imputed.columns:\n",
    "    print(\"\\nüîÑ Replacing is_oa with is_not_oa (derived feature)...\")\n",
    "    X_train_imputed = X_train_imputed.with_columns(\n",
    "        (1 - pl.col(\"is_oa\")).alias(\"is_not_oa\")\n",
    "    ).drop(\"is_oa\")\n",
    "    X_val_imputed = X_val_imputed.with_columns(\n",
    "        (1 - pl.col(\"is_oa\")).alias(\"is_not_oa\")\n",
    "    ).drop(\"is_oa\")\n",
    "    X_test_imputed = X_test_imputed.with_columns(\n",
    "        (1 - pl.col(\"is_oa\")).alias(\"is_not_oa\")\n",
    "    ).drop(\"is_oa\")\n",
    "    print(\"‚úÖ Replaced is_oa with is_not_oa in all datasets\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  is_oa column not found, skipping replacement\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8225930f",
   "metadata": {
    "papermill": {
     "duration": 0.00473,
     "end_time": "2025-11-18T16:06:57.046091",
     "exception": false,
     "start_time": "2025-11-18T16:06:57.041361",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Statistical analyses (correlations, chi-square, Cram√©r's V, ANOVA, Tukey's HSD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "126273de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T16:06:57.056594Z",
     "iopub.status.busy": "2025-11-18T16:06:57.056392Z",
     "iopub.status.idle": "2025-11-18T16:23:38.781305Z",
     "shell.execute_reply": "2025-11-18T16:23:38.780465Z"
    },
    "papermill": {
     "duration": 1001.731233,
     "end_time": "2025-11-18T16:23:38.781962",
     "exception": false,
     "start_time": "2025-11-18T16:06:57.050729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label diagnostics:\n",
      "  dtype        : int64\n",
      "  # unique vals: 2\n",
      "  treated as categorical: True\n",
      "\n",
      "=== 6.1 Pearson & Spearman correlations ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top correlations (by |Pearson r|):\n",
      "shape: (30, 5)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ feature          ‚îÜ pearson_r ‚îÜ pearson_p ‚îÜ spearman_r ‚îÜ spearman_p ‚îÇ\n",
      "‚îÇ ---              ‚îÜ ---       ‚îÜ ---       ‚îÜ ---        ‚îÜ ---        ‚îÇ\n",
      "‚îÇ str              ‚îÜ f64       ‚îÜ f64       ‚îÜ f64        ‚îÜ f64        ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ has_grants       ‚îÜ 0.325772  ‚îÜ 0.0       ‚îÜ 0.325772   ‚îÜ 0.0        ‚îÇ\n",
      "‚îÇ num_grants       ‚îÜ 0.274682  ‚îÜ 0.0       ‚îÜ 0.328464   ‚îÜ 0.0        ‚îÇ\n",
      "‚îÇ num_institutions ‚îÜ 0.265076  ‚îÜ 0.0       ‚îÜ 0.285334   ‚îÜ 0.0        ‚îÇ\n",
      "‚îÇ num_locations    ‚îÜ 0.25264   ‚îÜ 0.0       ‚îÜ 0.213839   ‚îÜ 0.0        ‚îÇ\n",
      "‚îÇ has_pmid         ‚îÜ 0.24681   ‚îÜ 0.0       ‚îÜ 0.24681    ‚îÜ 0.0        ‚îÇ\n",
      "‚îÇ ‚Ä¶                ‚îÜ ‚Ä¶         ‚îÜ ‚Ä¶         ‚îÜ ‚Ä¶          ‚îÜ ‚Ä¶          ‚îÇ\n",
      "‚îÇ specter2_dim_504 ‚îÜ -0.155645 ‚îÜ 0.0       ‚îÜ -0.155084  ‚îÜ 0.0        ‚îÇ\n",
      "‚îÇ scibert_dim_440  ‚îÜ -0.155306 ‚îÜ 0.0       ‚îÜ -0.168294  ‚îÜ 0.0        ‚îÇ\n",
      "‚îÇ specter2_dim_533 ‚îÜ -0.153787 ‚îÜ 0.0       ‚îÜ -0.148026  ‚îÜ 0.0        ‚îÇ\n",
      "‚îÇ scibert_dim_275  ‚îÜ 0.15274   ‚îÜ 0.0       ‚îÜ 0.16562    ‚îÜ 0.0        ‚îÇ\n",
      "‚îÇ is_in_doaj       ‚îÜ 0.152367  ‚îÜ 0.0       ‚îÜ 0.152367   ‚îÜ 0.0        ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "üíæ Correlation summary saved to: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/model_ready/feature_label_correlations.csv\n",
      "\n",
      "=== 6.2 Chi-square & Cram√©r's V (categorical features vs label) ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top categorical features by Cram√©r's V:\n",
      "shape: (3, 6)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ feature  ‚îÜ chi2         ‚îÜ p_value ‚îÜ dof    ‚îÜ cramers_v ‚îÜ n      ‚îÇ\n",
      "‚îÇ ---      ‚îÜ ---          ‚îÜ ---     ‚îÜ ---    ‚îÜ ---       ‚îÜ ---    ‚îÇ\n",
      "‚îÇ str      ‚îÜ f64          ‚îÜ f64     ‚îÜ i64    ‚îÜ f64       ‚îÜ i64    ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ id       ‚îÜ 960000.0     ‚îÜ 0.49952 ‚îÜ 959999 ‚îÜ 1.0       ‚îÜ 960000 ‚îÇ\n",
      "‚îÇ type     ‚îÜ 47319.050514 ‚îÜ 0.0     ‚îÜ 19     ‚îÜ 0.222015  ‚îÜ 960000 ‚îÇ\n",
      "‚îÇ language ‚îÜ 20000.140598 ‚îÜ 0.0     ‚îÜ 54     ‚îÜ 0.144338  ‚îÜ 960000 ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "üíæ Chi-square / Cram√©r's V summary saved to: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/model_ready/categorical_chi2_cramersv.csv\n",
      "\n",
      "=== 6.3 One-way ANOVA (numeric features across label groups) ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top features by ANOVA p-value:\n",
      "shape: (30, 3)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ feature         ‚îÜ F             ‚îÜ p_value ‚îÇ\n",
      "‚îÇ ---             ‚îÜ ---           ‚îÜ ---     ‚îÇ\n",
      "‚îÇ str             ‚îÜ f64           ‚îÜ f64     ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ has_abstract    ‚îÜ 7484.918853   ‚îÜ 0.0     ‚îÇ\n",
      "‚îÇ has_doi         ‚îÜ 14211.170527  ‚îÜ 0.0     ‚îÇ\n",
      "‚îÇ has_grants      ‚îÜ 113978.041551 ‚îÜ 0.0     ‚îÇ\n",
      "‚îÇ has_pmid        ‚îÜ 62271.670136  ‚îÜ 0.0     ‚îÇ\n",
      "‚îÇ is_in_doaj      ‚îÜ 22816.719899  ‚îÜ 0.0     ‚îÇ\n",
      "‚îÇ ‚Ä¶               ‚îÜ ‚Ä¶             ‚îÜ ‚Ä¶       ‚îÇ\n",
      "‚îÇ scibert_dim_108 ‚îÜ 18289.870082  ‚îÜ 0.0     ‚îÇ\n",
      "‚îÇ scibert_dim_109 ‚îÜ 7816.826237   ‚îÜ 0.0     ‚îÇ\n",
      "‚îÇ scibert_dim_111 ‚îÜ 5154.4229     ‚îÜ 0.0     ‚îÇ\n",
      "‚îÇ scibert_dim_113 ‚îÜ 8148.418707   ‚îÜ 0.0     ‚îÇ\n",
      "‚îÇ scibert_dim_114 ‚îÜ 6272.114133   ‚îÜ 0.0     ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "üíæ ANOVA summary saved to: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/model_ready/anova_numeric_features_vs_label.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of Tukey HSD results:\n",
      "shape: (20, 8)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ group1 ‚îÜ group2 ‚îÜ meandiff ‚îÜ p-adj ‚îÜ lower   ‚îÜ upper   ‚îÜ reject ‚îÜ feature          ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---    ‚îÜ ---      ‚îÜ ---   ‚îÜ ---     ‚îÜ ---     ‚îÜ ---    ‚îÜ ---              ‚îÇ\n",
      "‚îÇ i64    ‚îÜ i64    ‚îÜ f64      ‚îÜ f64   ‚îÜ f64     ‚îÜ f64     ‚îÜ f64    ‚îÜ str              ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ 0      ‚îÜ 1      ‚îÜ 0.1732   ‚îÜ 0.0   ‚îÜ 0.1693  ‚îÜ 0.1771  ‚îÜ 1.0    ‚îÜ has_abstract     ‚îÇ\n",
      "‚îÇ 0      ‚îÜ 1      ‚îÜ 0.1895   ‚îÜ 0.0   ‚îÜ 0.1864  ‚îÜ 0.1926  ‚îÜ 1.0    ‚îÜ has_doi          ‚îÇ\n",
      "‚îÇ 0      ‚îÜ 1      ‚îÜ 0.346    ‚îÜ 0.0   ‚îÜ 0.344   ‚îÜ 0.348   ‚îÜ 1.0    ‚îÜ has_grants       ‚îÇ\n",
      "‚îÇ 0      ‚îÜ 1      ‚îÜ 0.2301   ‚îÜ 0.0   ‚îÜ 0.2283  ‚îÜ 0.2319  ‚îÜ 1.0    ‚îÜ has_pmid         ‚îÇ\n",
      "‚îÇ 0      ‚îÜ 1      ‚îÜ 0.1611   ‚îÜ 0.0   ‚îÜ 0.159   ‚îÜ 0.1631  ‚îÜ 1.0    ‚îÜ is_in_doaj       ‚îÇ\n",
      "‚îÇ ‚Ä¶      ‚îÜ ‚Ä¶      ‚îÜ ‚Ä¶        ‚îÜ ‚Ä¶     ‚îÜ ‚Ä¶       ‚îÜ ‚Ä¶       ‚îÜ ‚Ä¶      ‚îÜ ‚Ä¶                ‚îÇ\n",
      "‚îÇ 0      ‚îÜ 1      ‚îÜ 26.248   ‚îÜ 0.0   ‚îÜ 25.9193 ‚îÜ 26.5768 ‚îÜ 1.0    ‚îÜ title_length     ‚îÇ\n",
      "‚îÇ 0      ‚îÜ 1      ‚îÜ 3.0563   ‚îÜ 0.0   ‚îÜ 3.0108  ‚îÜ 3.1017  ‚îÜ 1.0    ‚îÜ title_word_count ‚îÇ\n",
      "‚îÇ 0      ‚îÜ 1      ‚îÜ -0.1926  ‚îÜ 0.0   ‚îÜ -0.1956 ‚îÜ -0.1895 ‚îÜ 1.0    ‚îÜ scibert_dim_1    ‚îÇ\n",
      "‚îÇ 0      ‚îÜ 1      ‚îÜ -0.1047  ‚îÜ 0.0   ‚îÜ -0.1075 ‚îÜ -0.1018 ‚îÜ 1.0    ‚îÜ scibert_dim_10   ‚îÇ\n",
      "‚îÇ 0      ‚îÜ 1      ‚îÜ -0.0712  ‚îÜ 0.0   ‚îÜ -0.074  ‚îÜ -0.0685 ‚îÜ 1.0    ‚îÜ scibert_dim_100  ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "üíæ Tukey HSD results saved to: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/model_ready/tukey_hsd_top_features.csv\n"
     ]
    }
   ],
   "source": [
    "if y_train is None:\n",
    "    print(\"No y_train.npy found. Skipping label-based statistical analyses.\")\n",
    "\n",
    "else:\n",
    "    label_series = y_train\n",
    "    uniq = np.unique(label_series)\n",
    "    n_unique = len(uniq)\n",
    "    is_label_categorical = (n_unique <= 20)\n",
    "\n",
    "    print(\"\\nLabel diagnostics:\")\n",
    "    print(\"  dtype        :\", label_series.dtype)\n",
    "    print(\"  # unique vals:\", n_unique)\n",
    "    print(\"  treated as categorical:\", is_label_categorical)\n",
    "\n",
    "    print(\"\\n=== 6.1 Pearson & Spearman correlations ===\")\n",
    "    numeric_feature_cols = [\n",
    "        c for c, dt in zip(X_train_imputed.columns, X_train_imputed.dtypes)\n",
    "        if dt in {\n",
    "            pl.Int8, pl.Int16, pl.Int32, pl.Int64,\n",
    "            pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,\n",
    "            pl.Float32, pl.Float64\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    if np.issubdtype(label_series.dtype, np.number):\n",
    "        label_numeric = label_series.astype(float)\n",
    "    else:\n",
    "        _, label_numeric = np.unique(label_series, return_inverse=True)\n",
    "        label_numeric = label_numeric.astype(float)\n",
    "        print(\"Label converted to numeric codes for correlation analysis.\")\n",
    "\n",
    "    corr_records: List[Dict[str, Any]] = []\n",
    "    for col in numeric_feature_cols:\n",
    "        x = X_train_imputed.select(pl.col(col)).to_series().to_numpy().astype(float)\n",
    "        mask = ~np.isnan(x) & ~np.isnan(label_numeric)\n",
    "        if mask.sum() < 10:\n",
    "            continue\n",
    "\n",
    "        x_vals = x[mask]\n",
    "        y_vals = label_numeric[mask]\n",
    "\n",
    "        # Skip if either array is constant (all values the same)\n",
    "        if len(np.unique(x_vals)) <= 1 or len(np.unique(y_vals)) <= 1:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            pearson_r, pearson_p = stats.pearsonr(x_vals, y_vals)\n",
    "            spearman_r, spearman_p = stats.spearmanr(x_vals, y_vals)\n",
    "\n",
    "            # Handle NaN results\n",
    "            if np.isnan(pearson_r) or np.isnan(spearman_r):\n",
    "                continue\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        corr_records.append({\n",
    "            \"feature\": col,\n",
    "            \"pearson_r\": float(pearson_r),\n",
    "            \"pearson_p\": float(pearson_p),\n",
    "            \"spearman_r\": float(spearman_r),\n",
    "            \"spearman_p\": float(spearman_p),\n",
    "        })\n",
    "\n",
    "    if corr_records:\n",
    "        corr_df = pl.DataFrame(corr_records)\n",
    "        corr_df = corr_df.with_columns(\n",
    "            pl.col(\"pearson_r\").abs().alias(\"abs_pearson\")\n",
    "        ).sort(\"abs_pearson\", descending=True).drop(\"abs_pearson\")\n",
    "        print(\"\\nTop correlations (by |Pearson r|):\")\n",
    "        print(corr_df.head(30))\n",
    "        corr_path = MODEL_DIR / \"feature_label_correlations.csv\"\n",
    "        corr_df.write_csv(corr_path)\n",
    "        print(f\"üíæ Correlation summary saved to: {corr_path}\")\n",
    "    else:\n",
    "        print(\"No valid numeric feature-label correlations could be computed.\")\n",
    "\n",
    "    # Categorical Label Analyses\n",
    "    if is_label_categorical:\n",
    "        print(\"\\n=== 6.2 Chi-square & Cram√©r's V (categorical features vs label) ===\")\n",
    "        cat_cols = [\n",
    "            c for c, dt in zip(X_train_imputed.columns, X_train_imputed.dtypes)\n",
    "            if dt in {pl.Utf8, pl.Categorical}\n",
    "        ]\n",
    "        chi_records: List[Dict[str, Any]] = []\n",
    "        uniq_labels, label_codes = np.unique(label_series, return_inverse=True)\n",
    "\n",
    "        for col in cat_cols:\n",
    "            x_vals = X_train_imputed.select(pl.col(col)).to_series().to_list()\n",
    "            x_vals = np.array(x_vals, dtype=object)\n",
    "            uniq_x, x_codes = np.unique(x_vals, return_inverse=True)\n",
    "            if len(uniq_x) < 2 or len(uniq_labels) < 2:\n",
    "                continue\n",
    "\n",
    "            table = np.zeros((len(uniq_x), len(uniq_labels)), dtype=int)\n",
    "            for i in range(len(x_codes)):\n",
    "                table[x_codes[i], label_codes[i]] += 1\n",
    "\n",
    "            try:\n",
    "                chi2, p, dof, expected = stats.chi2_contingency(table)\n",
    "                n = table.sum()\n",
    "                r, k = table.shape\n",
    "                if min(r, k) > 1:\n",
    "                    cramers_v = np.sqrt(chi2 / (n * (min(r, k) - 1)))\n",
    "                else:\n",
    "                    cramers_v = np.nan\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            chi_records.append({\n",
    "                \"feature\": col,\n",
    "                \"chi2\": float(chi2),\n",
    "                \"p_value\": float(p),\n",
    "                \"dof\": int(dof),\n",
    "                \"cramers_v\": float(cramers_v),\n",
    "                \"n\": int(n),\n",
    "            })\n",
    "\n",
    "        if chi_records:\n",
    "            chi_df = pl.DataFrame(chi_records).sort(\"cramers_v\", descending=True)\n",
    "            print(\"\\nTop categorical features by Cram√©r's V:\")\n",
    "            print(chi_df.head(30))\n",
    "            chi_path = MODEL_DIR / \"categorical_chi2_cramersv.csv\"\n",
    "            chi_df.write_csv(chi_path)\n",
    "            print(f\"üíæ Chi-square / Cram√©r's V summary saved to: {chi_path}\")\n",
    "        else:\n",
    "            print(\"No suitable categorical features for chi-square / Cram√©r's V.\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nSkipping chi-square / Cram√©r's V: label not treated as categorical.\")\n",
    "\n",
    "    # ANOVA for Numeric Features\n",
    "    if is_label_categorical:\n",
    "        print(\"\\n=== 6.3 One-way ANOVA (numeric features across label groups) ===\")\n",
    "        anova_records: List[Dict[str, Any]] = []\n",
    "        uniq_labels = np.unique(label_series)\n",
    "        # Suppress ConstantInputWarning from scipy.stats.f_oneway\n",
    "        import warnings\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "            warnings.filterwarnings('ignore', message='(?i).*constant.*')\n",
    "            warnings.filterwarnings('ignore', message='(?i).*F statistic.*')\n",
    "            for col in numeric_feature_cols:\n",
    "                x = X_train_imputed.select(pl.col(col)).to_series().to_numpy().astype(float)\n",
    "                groups = []\n",
    "                for level in uniq_labels:\n",
    "                    mask = (label_series == level) & ~np.isnan(x)\n",
    "                    vals = x[mask]\n",
    "                    # Filter out constant groups (variance = 0) to avoid warnings\n",
    "                    if vals.size > 1 and np.var(vals) > 1e-10:\n",
    "                        groups.append(vals)\n",
    "                if len(groups) <= 1:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    F, p = stats.f_oneway(*groups)\n",
    "                    # Check for valid results (not inf or nan)\n",
    "                    if np.isfinite(F) and np.isfinite(p):\n",
    "                        anova_records.append({\"feature\": col, \"F\": float(F), \"p_value\": float(p)})\n",
    "                except (ValueError, RuntimeWarning):\n",
    "                    continue\n",
    "\n",
    "        if anova_records:\n",
    "            anova_df = pl.DataFrame(anova_records).sort(\"p_value\")\n",
    "            print(\"\\nTop features by ANOVA p-value:\")\n",
    "            print(anova_df.head(30))\n",
    "            anova_path = MODEL_DIR / \"anova_numeric_features_vs_label.csv\"\n",
    "            anova_df.write_csv(anova_path)\n",
    "            print(f\"üíæ ANOVA summary saved to: {anova_path}\")\n",
    "        else:\n",
    "            print(\"No valid ANOVA comparisons could be computed.\")\n",
    "    else:\n",
    "        print(\"\\nSkipping ANOVA: label not treated as categorical.\")\n",
    "\n",
    "    # Tukey's HSD Post-hoc\n",
    "    if is_label_categorical and HAS_STATSMODELS:\n",
    "        try:\n",
    "            # statsmodels is pandas-based; we only use it to compute Tukey on a few columns\n",
    "            import pandas as pd  # local import, not used for core data handling\n",
    "            anova_pdf = anova_df.to_pandas()\n",
    "            N_TOP_TUKEY = 20\n",
    "            top_features = anova_pdf.nsmallest(N_TOP_TUKEY, \"p_value\")[\"feature\"].tolist()\n",
    "            tukey_records: List[Dict[str, Any]] = []\n",
    "\n",
    "            for feat in top_features:\n",
    "                vals = X_train_imputed.select(pl.col(feat)).to_series().to_numpy().astype(float)\n",
    "                mask = ~np.isnan(vals)\n",
    "                if mask.sum() < 10:\n",
    "                    continue\n",
    "                tukey = pairwise_tukeyhsd(\n",
    "                    endog=vals[mask],\n",
    "                    groups=label_series[mask],\n",
    "                    alpha=0.05,\n",
    "                )\n",
    "                summary = tukey.summary()\n",
    "                header = summary.data[0]\n",
    "                for row in summary.data[1:]:\n",
    "                    rec = {h: v for h, v in zip(header, row)}\n",
    "                    rec[\"feature\"] = feat\n",
    "                    tukey_records.append(rec)\n",
    "\n",
    "            if tukey_records:\n",
    "                tukey_df = pl.DataFrame(tukey_records)\n",
    "                print(\"\\nSample of Tukey HSD results:\")\n",
    "                print(tukey_df.head(30))\n",
    "                tukey_path = MODEL_DIR / \"tukey_hsd_top_features.csv\"\n",
    "                tukey_df.write_csv(tukey_path)\n",
    "                print(f\"üíæ Tukey HSD results saved to: {tukey_path}\")\n",
    "            else:\n",
    "                print(\"Tukey's HSD did not produce any results (insufficient data after filtering).\")\n",
    "        except Exception as e:\n",
    "            print(f\"Tukey's HSD step encountered an error and will be skipped: {e}\")\n",
    "    elif is_label_categorical and not HAS_STATSMODELS:\n",
    "        print(\"\\nTukey's HSD skipped: statsmodels is not installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e24376d",
   "metadata": {
    "papermill": {
     "duration": 0.005033,
     "end_time": "2025-11-18T16:23:38.793450",
     "exception": false,
     "start_time": "2025-11-18T16:23:38.788417",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Prepare model-ready feature sets (keep all non-embedding features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1c27f65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T16:23:38.804536Z",
     "iopub.status.busy": "2025-11-18T16:23:38.804336Z",
     "iopub.status.idle": "2025-11-18T16:23:39.108473Z",
     "shell.execute_reply": "2025-11-18T16:23:39.107884Z"
    },
    "papermill": {
     "duration": 0.310635,
     "end_time": "2025-11-18T16:23:39.109032",
     "exception": false,
     "start_time": "2025-11-18T16:23:38.798397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature summary:\n",
      "  Embedding features (kept): 1920\n",
      "  Non-embedding numeric features (kept): 54\n",
      "  Categorical features (kept): 2\n",
      "  ID columns: 1\n",
      "\n",
      "  Total non-embedding features kept: 56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total columns in model-ready feature set (excluding label): 1977\n",
      "\n",
      "Shapes of model-ready feature matrices:\n",
      "  X_train_model_ready: (960000, 1977)\n",
      "  X_val_model_ready:   (120000, 1977)\n",
      "  X_test_model_ready:  (120000, 1977)\n"
     ]
    }
   ],
   "source": [
    "# Keep ALL non-embedding features (no reduction)\n",
    "# This preserves signals that may be sparse in the sample but informative in the full dataset\n",
    "# Scalable approach: works with any dataset size using Polars operations\n",
    "\n",
    "embedding_cols_current: List[str] = []\n",
    "for m in embedding_models:\n",
    "    prefix = f\"{m}_\"\n",
    "    embedding_cols_current.extend([c for c in X_train_imputed.columns if c.startswith(prefix)])\n",
    "embedding_cols_current = sorted(set(embedding_cols_current))\n",
    "\n",
    "# Get all non-embedding features (numeric + categorical)\n",
    "non_emb_numeric_cols = [\n",
    "    c for c, dt in zip(X_train_imputed.columns, X_train_imputed.dtypes)\n",
    "    if dt in {\n",
    "        pl.Int8, pl.Int16, pl.Int32, pl.Int64,\n",
    "        pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,\n",
    "        pl.Float32, pl.Float64\n",
    "    }\n",
    "    and c not in embedding_cols_current \n",
    "    and c not in id_cols \n",
    "    and c != \"label\"\n",
    "]\n",
    "\n",
    "categorical_cols_current = [\n",
    "    c for c, dt in zip(X_train_imputed.columns, X_train_imputed.dtypes)\n",
    "    if dt in {pl.Utf8, pl.Categorical}\n",
    "    and c not in id_cols\n",
    "]\n",
    "\n",
    "print(\"\\nFeature summary:\")\n",
    "print(f\"  Embedding features (kept): {len(embedding_cols_current)}\")\n",
    "print(f\"  Non-embedding numeric features (kept): {len(non_emb_numeric_cols)}\")\n",
    "print(f\"  Categorical features (kept): {len(categorical_cols_current)}\")\n",
    "print(f\"  ID columns: {len(id_cols)}\")\n",
    "print(f\"\\n  Total non-embedding features kept: {len(non_emb_numeric_cols) + len(categorical_cols_current)}\")\n",
    "\n",
    "# Prepare model-ready feature sets (all features)\n",
    "model_ready_feature_cols = (\n",
    "    id_cols\n",
    "    + embedding_cols_current\n",
    "    + non_emb_numeric_cols\n",
    "    + categorical_cols_current\n",
    ")\n",
    "\n",
    "# Ensure column order and existence\n",
    "seen: set[str] = set()\n",
    "ordered_model_ready_cols: List[str] = []\n",
    "for c in model_ready_feature_cols:\n",
    "    if c not in seen and c in X_train_imputed.columns:\n",
    "        ordered_model_ready_cols.append(c)\n",
    "        seen.add(c)\n",
    "\n",
    "print(f\"\\nTotal columns in model-ready feature set (excluding label): {len(ordered_model_ready_cols)}\")\n",
    "\n",
    "# Create model-ready datasets (all features, scalable with Polars)\n",
    "X_train_model_ready = X_train_imputed.select(ordered_model_ready_cols)\n",
    "X_val_model_ready   = X_val_imputed.select(ordered_model_ready_cols)\n",
    "X_test_model_ready  = X_test_imputed.select(ordered_model_ready_cols)\n",
    "\n",
    "print(\"\\nShapes of model-ready feature matrices:\")\n",
    "print(f\"  X_train_model_ready: {X_train_model_ready.shape}\")\n",
    "print(f\"  X_val_model_ready:   {X_val_model_ready.shape}\")\n",
    "print(f\"  X_test_model_ready:  {X_test_model_ready.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8087c4a1",
   "metadata": {
    "papermill": {
     "duration": 0.005198,
     "end_time": "2025-11-18T16:23:39.119540",
     "exception": false,
     "start_time": "2025-11-18T16:23:39.114342",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Save model-ready train / validation / test parquet files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0ba3b3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T16:23:39.130486Z",
     "iopub.status.busy": "2025-11-18T16:23:39.130308Z",
     "iopub.status.idle": "2025-11-18T16:27:32.036960Z",
     "shell.execute_reply": "2025-11-18T16:27:32.036074Z"
    },
    "papermill": {
     "duration": 232.918088,
     "end_time": "2025-11-18T16:27:32.042675",
     "exception": false,
     "start_time": "2025-11-18T16:23:39.124587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model-ready shapes:\n",
      "  train_model_ready: (960000, 1978)\n",
      "  val_model_ready:   (120000, 1978)\n",
      "  test_model_ready:  (120000, 1977)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saved model-ready parquet files:\n",
      "  /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/model_ready/train_model_ready.parquet\n",
      "  /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/model_ready/val_model_ready.parquet\n",
      "  /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/model_ready/test_model_ready.parquet\n"
     ]
    }
   ],
   "source": [
    "# Prepare final model-ready datasets with labels (scalable with Polars)\n",
    "train_model_ready = X_train_model_ready\n",
    "val_model_ready   = X_val_model_ready\n",
    "test_model_ready  = X_test_model_ready\n",
    "\n",
    "if y_train is not None:\n",
    "    train_model_ready = train_model_ready.with_columns(pl.Series(\"label\", y_train))\n",
    "if y_val is not None and y_val.shape[0] == val_model_ready.height:\n",
    "    val_model_ready = val_model_ready.with_columns(pl.Series(\"label\", y_val))\n",
    "\n",
    "print(\"\\nModel-ready shapes:\")\n",
    "print(f\"  train_model_ready: {train_model_ready.shape}\")\n",
    "print(f\"  val_model_ready:   {val_model_ready.shape}\")\n",
    "print(f\"  test_model_ready:  {test_model_ready.shape}\")\n",
    "\n",
    "# Save model-ready parquet files (scalable - Polars handles large datasets efficiently)\n",
    "train_path = MODEL_DIR / \"train_model_ready.parquet\"\n",
    "val_path   = MODEL_DIR / \"val_model_ready.parquet\"\n",
    "test_path  = MODEL_DIR / \"test_model_ready.parquet\"\n",
    "\n",
    "train_model_ready.write_parquet(train_path)\n",
    "val_model_ready.write_parquet(val_path)\n",
    "test_model_ready.write_parquet(test_path)\n",
    "\n",
    "print(\"\\nüíæ Saved model-ready parquet files:\")\n",
    "print(f\"  {train_path}\")\n",
    "print(f\"  {val_path}\")\n",
    "print(f\"  {test_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d249b9b",
   "metadata": {
    "papermill": {
     "duration": 0.005102,
     "end_time": "2025-11-18T16:27:32.053154",
     "exception": false,
     "start_time": "2025-11-18T16:27:32.048052",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9. Recap\n",
    "\n",
    "This notebook:\n",
    "\n",
    "- Loads the base features from `X_train.parquet`, `X_val.parquet`, `X_test.parquet`\n",
    "- Discovers and merges all available embedding parquets via `id`\n",
    "- Performs train-centric imputation with polars\n",
    "- Replaces `is_oa` with `is_not_oa` (derived feature: 1 - is_oa) for better signal\n",
    "- Computes Pearson / Spearman, chi-square / Cram√©r's V, ANOVA, optional Tukey HSD\n",
    "- Keeps **all non-embedding features** (no reduction) to preserve signals that may be sparse in sample but informative in full dataset\n",
    "- Writes model-ready splits under `data/model_ready`\n",
    "\n",
    "All dataframe logic is handled with **polars** (scalable for large datasets); pandas appears only in a small local scope if you enable Tukey HSD via statsmodels.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1694.639513,
   "end_time": "2025-11-18T16:27:37.774157",
   "environment_variables": {},
   "exception": null,
   "input_path": "src/notebooks/data_exploration_next_steps.ipynb",
   "output_path": "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/runs/data_exploration_next_steps_executed_20251118-105917.ipynb",
   "parameters": {},
   "start_time": "2025-11-18T15:59:23.134644",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}