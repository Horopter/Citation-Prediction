{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4748b24",
   "metadata": {
    "papermill": {
     "duration": 0.014874,
     "end_time": "2025-11-19T10:59:08.120995",
     "exception": false,
     "start_time": "2025-11-19T10:59:08.106121",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 6: LightGBM with All Features\n",
    "\n",
    "This notebook trains an **LightGBM** classifier on all available features (regular + all embeddings) with comprehensive preprocessing:\n",
    "- ‚úÖ All regular features\n",
    "- ‚úÖ All embedding families (PCA-compressed)\n",
    "- ‚úÖ Feature scaling (StandardScaler)\n",
    "- ‚úÖ 5-fold Cross-Validation\n",
    "- ‚úÖ Comprehensive Hyperparameter Tuning (RandomizedSearchCV)\n",
    "- ‚úÖ Threshold Fine-tuning\n",
    "- ‚úÖ Model Saving\n",
    "- ‚úÖ Submission.csv Generation\n",
    "- ‚úÖ OOM Safe with aggressive memory management\n",
    "- ‚úÖ SMOTETomek for class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a393cd0",
   "metadata": {
    "papermill": {
     "duration": 0.005571,
     "end_time": "2025-11-19T10:59:08.133663",
     "exception": false,
     "start_time": "2025-11-19T10:59:08.128092",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üìë Model 6 - Code Navigation Index\n",
    "\n",
    "## Quick Navigation\n",
    "- **[Setup](#1-setup)** - Imports, paths, device configuration, robustness utilities\n",
    "- **[Data Loading](#2-data-loading--feature-extraction)** - Load and split features\n",
    "- **[PCA Preprocessing](#3-feature-preprocessing-pca)** - Embedding compression (if applicable)\n",
    "- **[SMOTETomek](#4-class-imbalance-handling-smotetomek)** - Class imbalance resampling\n",
    "- **[Feature Scaling](#5-feature-scaling)** - StandardScaler normalization\n",
    "- **[Cross-Validation](#6-cross-validation--hyperparameter-tuning)** - Hyperparameter optimization\n",
    "- **[Threshold Tuning](#7-threshold-tuning--final-evaluation)** - Optimal threshold finding\n",
    "- **[Model Saving](#8-save-model)** - Save model weights and metadata\n",
    "- **[Submission](#9-generate-submission)** - Generate test predictions\n",
    "\n",
    "## Model Type: LightGBM (all features)\n",
    "\n",
    "## Key Features\n",
    "‚úÖ GPU-friendly with CPU fallback  \n",
    "‚úÖ Aggressive garbage collection  \n",
    "‚úÖ OOM resistant with chunked processing  \n",
    "‚úÖ Kernel panic resistant (signal handlers, checkpoints)  \n",
    "‚úÖ Polars-only (no pandas)  \n",
    "‚úÖ GPU-friendly PCA (IncrementalTorchPCA option)  \n",
    "‚úÖ SMOTETomek for class imbalance  \n",
    "‚úÖ Feature scaling & embedding normalization  \n",
    "‚úÖ Hyperparameter tuning (RandomizedSearchCV/GridSearchCV)  \n",
    "‚úÖ Fine-grained threshold optimization (120+ thresholds)  \n",
    "‚úÖ Model weights saved  \n",
    "‚úÖ Chunked/batched data processing  \n",
    "\n",
    "## Memory Management\n",
    "- `cleanup_memory()`: Aggressive GC + GPU cache clearing\n",
    "- `check_memory_safe()`: Pre-operation memory checks\n",
    "- `chunked_operation()`: Process large data in chunks\n",
    "- `safe_operation()`: Retry decorator with OOM handling\n",
    "- Signal handlers: SIGINT/SIGTERM for graceful shutdown\n",
    "- Checkpoints: Resume from failures\n",
    "\n",
    "## Device Handling\n",
    "- Automatic GPU detection with CPU fallback\n",
    "- `device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')`\n",
    "- All tensors moved to device explicitly\n",
    "- GPU cache cleared aggressively after operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b8bf12",
   "metadata": {
    "papermill": {
     "duration": 0.005439,
     "end_time": "2025-11-19T10:59:08.144673",
     "exception": false,
     "start_time": "2025-11-19T10:59:08.139234",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b312417",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T10:59:08.157713Z",
     "iopub.status.busy": "2025-11-19T10:59:08.157235Z",
     "iopub.status.idle": "2025-11-19T10:59:36.712924Z",
     "shell.execute_reply": "2025-11-19T10:59:36.711153Z"
    },
    "papermill": {
     "duration": 28.56468,
     "end_time": "2025-11-19T10:59:36.714822",
     "exception": false,
     "start_time": "2025-11-19T10:59:08.150142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import gc\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "from typing import Dict, Optional\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import signal\n",
    "import atexit\n",
    "from functools import wraps\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c553d8f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T10:59:36.732751Z",
     "iopub.status.busy": "2025-11-19T10:59:36.732418Z",
     "iopub.status.idle": "2025-11-19T10:59:37.065270Z",
     "shell.execute_reply": "2025-11-19T10:59:37.063798Z"
    },
    "papermill": {
     "duration": 0.341217,
     "end_time": "2025-11-19T10:59:37.066509",
     "exception": false,
     "start_time": "2025-11-19T10:59:36.725292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL_LIGHTGBM EXECUTION STARTED\n",
      "Start Time: 2025-11-19 05:59:36\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# STARTUP & REPRODUCIBILITY\n",
    "# =========================\n",
    "\n",
    "TOTAL_START_TIME = time.time()\n",
    "START_TIME_STR = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"MODEL_LIGHTGBM EXECUTION STARTED\")\n",
    "print(f\"Start Time: {START_TIME_STR}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c4d77cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T10:59:37.086684Z",
     "iopub.status.busy": "2025-11-19T10:59:37.086361Z",
     "iopub.status.idle": "2025-11-19T10:59:37.134581Z",
     "shell.execute_reply": "2025-11-19T10:59:37.132841Z"
    },
    "papermill": {
     "duration": 0.056936,
     "end_time": "2025-11-19T10:59:37.135999",
     "exception": false,
     "start_time": "2025-11-19T10:59:37.079063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2\n",
      "MODEL_READY_DIR: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/model_ready\n"
     ]
    }
   ],
   "source": [
    "# ==============\n",
    "# PATH MANAGEMENT\n",
    "# ==============\n",
    "\n",
    "current = Path(os.getcwd())\n",
    "PROJECT_ROOT = current\n",
    "for _ in range(5):\n",
    "    if (PROJECT_ROOT / \"data\").exists():\n",
    "        break\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "else:\n",
    "    PROJECT_ROOT = current.parent.parent\n",
    "\n",
    "MODEL_READY_DIR = PROJECT_ROOT / \"data\" / \"model_ready\"\n",
    "MODEL_SAVE_DIR = PROJECT_ROOT / \"models\" / \"saved_models\"\n",
    "SUBMISSION_DIR = PROJECT_ROOT / \"data\" / \"submission_files\"\n",
    "MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SUBMISSION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "utils_path = PROJECT_ROOT / \"src\" / \"utils\"\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"MODEL_READY_DIR:\", MODEL_READY_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bd62776",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T10:59:37.159029Z",
     "iopub.status.busy": "2025-11-19T10:59:37.158512Z",
     "iopub.status.idle": "2025-11-19T10:59:55.464895Z",
     "shell.execute_reply": "2025-11-19T10:59:55.463048Z"
    },
    "papermill": {
     "duration": 18.317462,
     "end_time": "2025-11-19T10:59:55.467320",
     "exception": false,
     "start_time": "2025-11-19T10:59:37.149858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using sklearn IncrementalPCA (memory-efficient)\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# PCA UTILITY IMPORTS\n",
    "# =======================\n",
    "USE_TORCH_PCA = False\n",
    "if utils_path.exists():\n",
    "    sys.path.insert(0, str(utils_path))\n",
    "\n",
    "if USE_TORCH_PCA:\n",
    "    try:\n",
    "        from pca_utils import IncrementalTorchPCA\n",
    "        IncrementalPCA = IncrementalTorchPCA\n",
    "        IS_TORCH_PCA = True\n",
    "        print(\"‚úÖ Using PyTorch PCA (GPU-friendly)\")\n",
    "    except ImportError:\n",
    "        from sklearn.decomposition import IncrementalPCA\n",
    "        IS_TORCH_PCA = False\n",
    "        print(\"‚ö†Ô∏è Using sklearn IncrementalPCA (CPU only)\")\n",
    "else:\n",
    "    from sklearn.decomposition import IncrementalPCA\n",
    "    IS_TORCH_PCA = False\n",
    "    print(\"‚úÖ Using sklearn IncrementalPCA (memory-efficient)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "095dafce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T10:59:55.492534Z",
     "iopub.status.busy": "2025-11-19T10:59:55.491752Z",
     "iopub.status.idle": "2025-11-19T11:00:01.852757Z",
     "shell.execute_reply": "2025-11-19T11:00:01.851034Z"
    },
    "papermill": {
     "duration": 6.372263,
     "end_time": "2025-11-19T11:00:01.854572",
     "exception": false,
     "start_time": "2025-11-19T10:59:55.482309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==========\n",
    "# ML LIBRARIES\n",
    "# ==========\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report, precision_recall_curve, roc_curve, confusion_matrix\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# ==========\n",
    "# VISUALIZATION LIBRARIES\n",
    "# ==========\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Image\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6e43a00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T11:00:01.874713Z",
     "iopub.status.busy": "2025-11-19T11:00:01.874358Z",
     "iopub.status.idle": "2025-11-19T11:00:02.049060Z",
     "shell.execute_reply": "2025-11-19T11:00:02.047123Z"
    },
    "papermill": {
     "duration": 0.183735,
     "end_time": "2025-11-19T11:00:02.050516",
     "exception": false,
     "start_time": "2025-11-19T11:00:01.866781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Memory utilities imported from shared module\n",
      "üíæ Memory: 0.62 GB (RAM) | 0.00/0.00 GB (GPU used/reserved)\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# MEMORY UTILITIES (FALLBACK DEFS)\n",
    "# ===============================\n",
    "try:\n",
    "    from model_training_utils import cleanup_memory, memory_usage, check_memory_safe\n",
    "    print(\"‚úÖ Memory utilities imported from shared module\")\n",
    "except ImportError:\n",
    "    def cleanup_memory():\n",
    "        \"\"\"Aggressive memory cleanup for both CPU and GPU.\"\"\"\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "            torch.cuda.ipc_collect()\n",
    "        gc.collect()\n",
    "    \n",
    "    def memory_usage():\n",
    "        \"\"\"Display current memory usage statistics.\"\"\"\n",
    "        try:\n",
    "            import psutil\n",
    "            process = psutil.Process(os.getpid())\n",
    "            mem_gb = process.memory_info().rss / 1024**3\n",
    "            print(f\"üíæ Memory: {mem_gb:.2f} GB (RAM)\", end=\"\")\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_mem = torch.cuda.memory_allocated() / 1024**3\n",
    "                gpu_reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "                print(f\" | {gpu_mem:.2f}/{gpu_reserved:.2f} GB (GPU used/reserved)\")\n",
    "            else:\n",
    "                print()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def check_memory_safe(ram_threshold_gb=0.85, gpu_threshold=0.80):\n",
    "        \"\"\"Check if memory usage is safe for operations.\"\"\"\n",
    "        try:\n",
    "            import psutil\n",
    "            process = psutil.Process(os.getpid())\n",
    "            ram_gb = process.memory_info().rss / 1024**3\n",
    "            total_ram = psutil.virtual_memory().total / 1024**3\n",
    "            ram_ratio = ram_gb / total_ram if total_ram > 0 else 0\n",
    "            gpu_ratio = 0\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_used = torch.cuda.memory_allocated() / 1024**3\n",
    "                gpu_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "                gpu_ratio = gpu_used / gpu_total if gpu_total > 0 else 0\n",
    "            is_safe = ram_ratio < ram_threshold_gb and gpu_ratio < gpu_threshold\n",
    "            return is_safe, {\"ram_gb\": ram_gb, \"ram_ratio\": ram_ratio, \"gpu_ratio\": gpu_ratio}\n",
    "        except:\n",
    "            return True, {}\n",
    "    \n",
    "    print(\"‚ö†Ô∏è Using fallback memory utilities\")\n",
    "\n",
    "memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e829f99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T11:00:02.071464Z",
     "iopub.status.busy": "2025-11-19T11:00:02.070951Z",
     "iopub.status.idle": "2025-11-19T11:00:02.102278Z",
     "shell.execute_reply": "2025-11-19T11:00:02.101111Z"
    },
    "papermill": {
     "duration": 0.040895,
     "end_time": "2025-11-19T11:00:02.103205",
     "exception": false,
     "start_time": "2025-11-19T11:00:02.062310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced robustness utilities loaded\n",
      "‚úÖ Training robustness wrappers loaded\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# ROBUSTNESS/CHECKPOINT UTILITIES\n",
    "# ===============================\n",
    "\n",
    "_checkpoint_state = {\n",
    "    \"pca_complete\": False,\n",
    "    \"scaling_complete\": False,\n",
    "    \"cv_complete\": False,\n",
    "    \"final_model_trained\": False,\n",
    "    \"last_saved_checkpoint\": None,\n",
    "}\n",
    "\n",
    "def save_checkpoint(state_name: str, data: dict, checkpoint_dir: Path = None):\n",
    "    \"\"\"Save checkpoint to resume from failures.\"\"\"\n",
    "    if checkpoint_dir is None:\n",
    "        checkpoint_dir = PROJECT_ROOT / \"data\" / \"checkpoints\"\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    checkpoint_path = checkpoint_dir / f\"model_lightgbm_checkpoint_{state_name}.pkl\"\n",
    "    try:\n",
    "        with open(checkpoint_path, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        _checkpoint_state[\"last_saved_checkpoint\"] = checkpoint_path\n",
    "        print(f\"‚úÖ Checkpoint saved: {checkpoint_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to save checkpoint: {e}\")\n",
    "\n",
    "def load_checkpoint(state_name: str, checkpoint_dir: Path = None):\n",
    "    \"\"\"Load checkpoint to resume from failures.\"\"\"\n",
    "    if checkpoint_dir is None:\n",
    "        checkpoint_dir = PROJECT_ROOT / \"data\" / \"checkpoints\"\n",
    "    checkpoint_path = checkpoint_dir / f\"model_lightgbm_checkpoint_{state_name}.pkl\"\n",
    "    if checkpoint_path.exists():\n",
    "        try:\n",
    "            with open(checkpoint_path, \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "            print(f\"‚úÖ Checkpoint loaded: {checkpoint_path}\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to load checkpoint: {e}\")\n",
    "    return None\n",
    "\n",
    "def safe_operation(operation_name: str, max_retries: int = 3, checkpoint_on_success: bool = False):\n",
    "    \"\"\"Decorator for safe operations with retry and checkpoint support.\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    is_safe, mem_info = check_memory_safe(ram_threshold_gb=0.80, gpu_threshold=0.75)\n",
    "                    if not is_safe:\n",
    "                        cleanup_memory()\n",
    "                        if torch.cuda.is_available():\n",
    "                            torch.cuda.empty_cache()\n",
    "                        time.sleep(1)\n",
    "                    result = func(*args, **kwargs)\n",
    "                    cleanup_memory()\n",
    "                    if checkpoint_on_success:\n",
    "                        save_checkpoint(operation_name, {\"status\": \"complete\", \"result\": result})\n",
    "                    return result\n",
    "                except (MemoryError, RuntimeError) as e:\n",
    "                    error_msg = str(e).lower()\n",
    "                    if \"out of memory\" in error_msg or \"oom\" in error_msg:\n",
    "                        if attempt < max_retries - 1:\n",
    "                            cleanup_memory()\n",
    "                            if torch.cuda.is_available():\n",
    "                                torch.cuda.empty_cache()\n",
    "                            time.sleep(2)\n",
    "                            continue\n",
    "                        else:\n",
    "                            raise\n",
    "                    else:\n",
    "                        raise\n",
    "                except Exception as e:\n",
    "                    if attempt < max_retries - 1:\n",
    "                        cleanup_memory()\n",
    "                        time.sleep(1)\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise\n",
    "            return None\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "def chunked_operation(\n",
    "    data,\n",
    "    operation_func,\n",
    "    chunk_size: int = 10000,\n",
    "    progress_every: int = 10,\n",
    "    operation_name: str = \"operation\",\n",
    "):\n",
    "    \"\"\"Execute operation on data in chunks with progress tracking.\"\"\"\n",
    "    total_chunks = (len(data) + chunk_size - 1) // chunk_size\n",
    "    results = []\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        chunk_num = i // chunk_size + 1\n",
    "        chunk = data[i : i + chunk_size]\n",
    "        try:\n",
    "            is_safe, mem_info = check_memory_safe(ram_threshold_gb=0.85, gpu_threshold=0.80)\n",
    "            if not is_safe:\n",
    "                cleanup_memory()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                time.sleep(0.5)\n",
    "            chunk_result = operation_func(chunk)\n",
    "            results.append(chunk_result)\n",
    "            if chunk_num % progress_every == 0 or chunk_num == total_chunks:\n",
    "                print(f\"  Progress: {chunk_num}/{total_chunks} chunks ({chunk_num*100//total_chunks}%)\")\n",
    "            del chunk\n",
    "            if chunk_num % 5 == 0:\n",
    "                cleanup_memory()\n",
    "        except (MemoryError, RuntimeError) as e:\n",
    "            error_msg = str(e).lower()\n",
    "            if \"out of memory\" in error_msg or \"oom\" in error_msg:\n",
    "                cleanup_memory()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                smaller_chunk_size = max(1000, chunk_size // 2)\n",
    "                if smaller_chunk_size < chunk_size:\n",
    "                    return chunked_operation(\n",
    "                        data[i:],\n",
    "                        operation_func,\n",
    "                        chunk_size=smaller_chunk_size,\n",
    "                        progress_every=progress_every,\n",
    "                        operation_name=operation_name,\n",
    "                    )\n",
    "                else:\n",
    "                    raise\n",
    "            else:\n",
    "                raise\n",
    "    return results\n",
    "\n",
    "def emergency_cleanup():\n",
    "    \"\"\"Emergency cleanup on exit.\"\"\"\n",
    "    try:\n",
    "        cleanup_memory()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        print(\"‚úÖ Emergency cleanup completed\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "atexit.register(emergency_cleanup)\n",
    "\n",
    "def signal_handler(signum, frame):\n",
    "    \"\"\"Handle signals for graceful shutdown.\"\"\"\n",
    "    print(f\"‚ö†Ô∏è Received signal {signum}, saving checkpoint...\")\n",
    "    save_checkpoint(\"emergency\", {\"status\": \"signal_received\", \"signal\": signum})\n",
    "    emergency_cleanup()\n",
    "    raise KeyboardInterrupt\n",
    "\n",
    "try:\n",
    "    signal.signal(signal.SIGINT, signal_handler)\n",
    "    signal.signal(signal.SIGTERM, signal_handler)\n",
    "except:\n",
    "    pass\n",
    "print(\"‚úÖ Enhanced robustness utilities loaded\")\n",
    "\n",
    "def safe_prediction(predict_func, *args, **kwargs):\n",
    "    \"\"\"Execute prediction with chunked processing.\"\"\"\n",
    "    try:\n",
    "        is_safe, mem_info = check_memory_safe(ram_threshold_gb=0.85, gpu_threshold=0.80)\n",
    "        if not is_safe:\n",
    "            cleanup_memory()\n",
    "        if \"X\" in kwargs and len(kwargs[\"X\"]) > 50000:\n",
    "            X = kwargs[\"X\"]\n",
    "            chunk_size = 10000\n",
    "            predictions = []\n",
    "            for i in range(0, len(X), chunk_size):\n",
    "                chunk = X[i : i + chunk_size]\n",
    "                kwargs[\"X\"] = chunk\n",
    "                chunk_preds = predict_func(*args, **kwargs)\n",
    "                predictions.append(chunk_preds)\n",
    "                del chunk, chunk_preds\n",
    "                if i % (chunk_size * 5) == 0:\n",
    "                    cleanup_memory()\n",
    "            return np.concatenate(predictions)\n",
    "        else:\n",
    "            return predict_func(*args, **kwargs)\n",
    "    except (MemoryError, RuntimeError) as e:\n",
    "        error_msg = str(e).lower()\n",
    "        if \"out of memory\" in error_msg or \"oom\" in error_msg:\n",
    "            cleanup_memory()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            if \"X\" in kwargs:\n",
    "                X = kwargs[\"X\"]\n",
    "                chunk_size = 5000\n",
    "                predictions = []\n",
    "                for i in range(0, len(X), chunk_size):\n",
    "                    chunk = X[i : i + chunk_size]\n",
    "                    kwargs[\"X\"] = chunk\n",
    "                    chunk_preds = predict_func(*args, **kwargs)\n",
    "                    predictions.append(chunk_preds)\n",
    "                    del chunk, chunk_preds\n",
    "                    cleanup_memory()\n",
    "                return np.concatenate(predictions)\n",
    "            else:\n",
    "                raise\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "print(\"‚úÖ Training robustness wrappers loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654616df",
   "metadata": {
    "papermill": {
     "duration": 0.013743,
     "end_time": "2025-11-19T11:00:02.129172",
     "exception": false,
     "start_time": "2025-11-19T11:00:02.115429",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Data Loading & Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "039f441b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T11:00:02.142910Z",
     "iopub.status.busy": "2025-11-19T11:00:02.142460Z",
     "iopub.status.idle": "2025-11-19T11:01:02.452747Z",
     "shell.execute_reply": "2025-11-19T11:01:02.451508Z"
    },
    "papermill": {
     "duration": 60.319104,
     "end_time": "2025-11-19T11:01:02.454153",
     "exception": false,
     "start_time": "2025-11-19T11:00:02.135049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 1: Data Loading\n",
      "================================================================================\n",
      "Loading train from /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/model_ready/train_model_ready.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading val from /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/model_ready/val_model_ready.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Data Summary:\n",
      "  Regular features: 54\n",
      "  Embedding sent_transformer_: 384 dims\n",
      "  Embedding scibert_: 768 dims\n",
      "  Embedding specter2_: 768 dims\n",
      "  Train samples: 960000, Positive: 65808, Negative: 894192\n",
      "  Val samples: 120000, Positive: 8075, Negative: 111925\n",
      "\n",
      "‚è±Ô∏è  Data Loading Time: 59.09 seconds (0.98 minutes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Memory: 32.76 GB (RAM) | 0.00/0.00 GB (GPU used/reserved)\n"
     ]
    }
   ],
   "source": [
    "def load_parquet_split(split: str) -> pl.DataFrame:\n",
    "    \"\"\"Load a model_ready parquet split with error handling.\"\"\"\n",
    "    try:\n",
    "        path = MODEL_READY_DIR / f\"{split}_model_ready.parquet\"\n",
    "        if not path.exists():\n",
    "            alt = MODEL_READY_DIR / f\"{split}_model_ready_reduced.parquet\"\n",
    "            if alt.exists():\n",
    "                path = alt\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Could not find {split} data\")\n",
    "        print(f\"Loading {split} from {path}\")\n",
    "        return pl.read_parquet(path)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {split}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def split_features_reg_and_all_emb(df: pl.DataFrame):\n",
    "    \"\"\"Split features into regular and embedding families.\"\"\"\n",
    "    cols = df.columns\n",
    "    dtypes = df.dtypes\n",
    "    label = df[\"label\"].to_numpy() if \"label\" in cols else None\n",
    "\n",
    "    reg_cols = []\n",
    "    EMBEDDING_FAMILY_PREFIXES = [\"sent_transformer_\", \"scibert_\", \"specter_\", \"specter2_\", \"ner_\"]\n",
    "    emb_family_to_cols = {p: [] for p in EMBEDDING_FAMILY_PREFIXES}\n",
    "\n",
    "    NUMERIC_DTYPES = {\n",
    "        pl.Int8,\n",
    "        pl.Int16,\n",
    "        pl.Int32,\n",
    "        pl.Int64,\n",
    "        pl.UInt8,\n",
    "        pl.UInt16,\n",
    "        pl.UInt32,\n",
    "        pl.UInt64,\n",
    "        pl.Float32,\n",
    "        pl.Float64,\n",
    "    }\n",
    "\n",
    "    for c, dt in zip(cols, dtypes):\n",
    "        if c in (\"id\", \"label\"):\n",
    "            continue\n",
    "        matched = False\n",
    "        for p in EMBEDDING_FAMILY_PREFIXES:\n",
    "            if c.startswith(p):\n",
    "                emb_family_to_cols[p].append(c)\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched and dt in NUMERIC_DTYPES:\n",
    "            reg_cols.append(c)\n",
    "\n",
    "    X_reg = df.select(reg_cols).to_numpy() if reg_cols else None\n",
    "    X_emb_families = {}\n",
    "    for p, clist in emb_family_to_cols.items():\n",
    "        if clist:\n",
    "            X_emb_families[p] = df.select(clist).to_numpy()\n",
    "\n",
    "    return X_reg, X_emb_families, label, reg_cols, emb_family_to_cols\n",
    "\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PHASE 1: Data Loading\")\n",
    "    print(\"=\" * 80)\n",
    "    phase_start = time.time()\n",
    "    \n",
    "    train_df = load_parquet_split(\"train\")\n",
    "    val_df = load_parquet_split(\"val\")\n",
    "\n",
    "    X_reg_train, X_emb_train_fams, y_train, reg_cols, emb_family_to_cols = (\n",
    "        split_features_reg_and_all_emb(train_df)\n",
    "    )\n",
    "    X_reg_val, X_emb_val_fams, y_val, _, _ = split_features_reg_and_all_emb(val_df)\n",
    "\n",
    "    phase_time = time.time() - phase_start\n",
    "    print(f\"\\nüìä Data Summary:\")\n",
    "    print(f\"  Regular features: {len(reg_cols)}\")\n",
    "    for fam, arr in X_emb_train_fams.items():\n",
    "        print(f\"  Embedding {fam}: {arr.shape[1]} dims\")\n",
    "    print(\n",
    "        f\"  Train samples: {len(y_train)}, Positive: {y_train.sum()}, Negative: {(y_train==0).sum()}\"\n",
    "    )\n",
    "    print(f\"  Val samples: {len(y_val)}, Positive: {y_val.sum()}, Negative: {(y_val==0).sum()}\")\n",
    "    print(f\"\\n‚è±Ô∏è  Data Loading Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\")\n",
    "\n",
    "    del train_df, val_df\n",
    "    cleanup_memory()\n",
    "    memory_usage()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ff764f",
   "metadata": {
    "papermill": {
     "duration": 0.007001,
     "end_time": "2025-11-19T11:01:02.474379",
     "exception": false,
     "start_time": "2025-11-19T11:01:02.467378",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Feature Preprocessing: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce6bb3ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T11:01:02.488077Z",
     "iopub.status.busy": "2025-11-19T11:01:02.487647Z",
     "iopub.status.idle": "2025-11-19T11:01:02.493575Z",
     "shell.execute_reply": "2025-11-19T11:01:02.492164Z"
    },
    "papermill": {
     "duration": 0.015362,
     "end_time": "2025-11-19T11:01:02.495606",
     "exception": false,
     "start_time": "2025-11-19T11:01:02.480244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PCA compression per embedding family\n",
    "PCA_COMPONENTS_PER_FAMILY = {\n",
    "    \"sent_transformer_\": 32,\n",
    "    \"scibert_\": 32,\n",
    "    \"specter_\": 32,\n",
    "    \"specter2_\": 32,\n",
    "    \"ner_\": 16,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04c98fae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T11:01:02.517039Z",
     "iopub.status.busy": "2025-11-19T11:01:02.516531Z",
     "iopub.status.idle": "2025-11-19T11:06:13.602652Z",
     "shell.execute_reply": "2025-11-19T11:06:13.601476Z"
    },
    "papermill": {
     "duration": 311.096784,
     "end_time": "2025-11-19T11:06:13.604471",
     "exception": false,
     "start_time": "2025-11-19T11:01:02.507687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 2: PCA Preprocessing\n",
      "================================================================================\n",
      "\n",
      "üìä Applying IncrementalPCA to embedding families...\n",
      "  sent_transformer_: 384 dims ‚Üí 32 components\n",
      "  scibert_: 768 dims ‚Üí 32 components\n",
      "  specter2_: 768 dims ‚Üí 32 components\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting PCA on subset (288000/960000 samples) for sent_transformer_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting PCA on subset (288000/960000 samples) for scibert_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting PCA on subset (288000/960000 samples) for specter2_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint saved: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/checkpoints/model_lightgbm_checkpoint_pca.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint saved: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/checkpoints/model_lightgbm_checkpoint_pca.pkl\n",
      "\n",
      "üìä After IncrementalPCA:\n",
      "  Train embeddings: (960000, 96)\n",
      "  Val embeddings: (120000, 96)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Combined train: (960000, 150)\n",
      "  Combined val: (120000, 150)\n",
      "\n",
      "‚è±Ô∏è  PCA Preprocessing Time: 308.84 seconds (5.15 minutes)\n",
      "üíæ Memory: 34.08 GB (RAM) | 0.00/0.00 GB (GPU used/reserved)\n"
     ]
    }
   ],
   "source": [
    "@safe_operation(\"pca\", max_retries=3, checkpoint_on_success=True)\n",
    "def apply_pca_to_embeddings(\n",
    "    X_emb_fams: Dict[str, np.ndarray], fit_on_train: bool = True, pca_models: Optional[Dict] = None\n",
    "):\n",
    "    \"\"\"Apply IncrementalPCA to each embedding family (GPU-friendly, OOM-resistant).\"\"\"\n",
    "    X_emb_pca_list = []\n",
    "    new_pca_models = {}\n",
    "\n",
    "    cleanup_memory()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # Memory safety check before PCA\n",
    "    is_safe, mem_info = check_memory_safe(ram_threshold_gb=0.75, gpu_threshold=0.70)\n",
    "    if not is_safe:\n",
    "        print(f\"‚ö†Ô∏è Memory usage high before PCA: {mem_info}\")\n",
    "        cleanup_memory()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    for fam, X_emb in X_emb_fams.items():\n",
    "        n_components = PCA_COMPONENTS_PER_FAMILY.get(fam, 32)\n",
    "        try:\n",
    "            cleanup_memory()\n",
    "\n",
    "            # Fit PCA model if training or no model given\n",
    "            if fit_on_train or pca_models is None:\n",
    "                ipca_args = {\n",
    "                    \"n_components\": min(n_components, X_emb.shape[1]),\n",
    "                    \"batch_size\": 2000\n",
    "                }\n",
    "                if IS_TORCH_PCA:\n",
    "                    ipca_args[\"device\"] = device\n",
    "                ipca = IncrementalPCA(**ipca_args)\n",
    "\n",
    "                # Fit PCA on subset for big data\n",
    "                max_pca_rows = int(X_emb.shape[0] * 0.3)\n",
    "                if X_emb.shape[0] > max_pca_rows:\n",
    "                    print(f\"  Fitting PCA on subset ({max_pca_rows}/{X_emb.shape[0]} samples) for {fam}\")\n",
    "                    idx = np.random.choice(X_emb.shape[0], size=max_pca_rows, replace=False)\n",
    "                    X_emb_subset = X_emb[idx].copy()\n",
    "                    del idx\n",
    "                    cleanup_memory()\n",
    "                    ipca.fit(X_emb_subset)\n",
    "                    del X_emb_subset\n",
    "                    cleanup_memory()\n",
    "                else:\n",
    "                    X_emb_copy = X_emb.copy() if not X_emb.flags[\"OWNDATA\"] else X_emb\n",
    "                    ipca.fit(X_emb_copy)\n",
    "                    if X_emb_copy is not X_emb:\n",
    "                        del X_emb_copy\n",
    "                    cleanup_memory()\n",
    "\n",
    "                new_pca_models[fam] = ipca\n",
    "            else:\n",
    "                ipca = pca_models[fam]\n",
    "\n",
    "            # Transform in chunks to avoid OOM\n",
    "            chunk_size = 5000\n",
    "            if X_emb.shape[0] > chunk_size:\n",
    "                X_emb_pca_chunks = []\n",
    "                for i in range(0, X_emb.shape[0], chunk_size):\n",
    "                    chunk = X_emb[i : i + chunk_size].copy()\n",
    "                    chunk_pca = ipca.transform(chunk)\n",
    "                    X_emb_pca_chunks.append(chunk_pca)\n",
    "                    del chunk, chunk_pca\n",
    "                    cleanup_memory()\n",
    "                    if i % (chunk_size * 5) == 0 and torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                X_emb_pca = np.vstack(X_emb_pca_chunks)\n",
    "                del X_emb_pca_chunks\n",
    "                cleanup_memory()\n",
    "            else:\n",
    "                X_emb_copy = X_emb.copy() if not X_emb.flags[\"OWNDATA\"] else X_emb\n",
    "                X_emb_pca = ipca.transform(X_emb_copy)\n",
    "                if X_emb_copy is not X_emb:\n",
    "                    del X_emb_copy\n",
    "                cleanup_memory()\n",
    "\n",
    "            X_emb_pca_list.append(X_emb_pca)\n",
    "            del X_emb_pca\n",
    "            cleanup_memory()\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower() or \"OOM\" in str(e).upper():\n",
    "                print(f\"‚ùå OOM error processing {fam}\")\n",
    "                cleanup_memory()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    torch.cuda.synchronize()\n",
    "                raise\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    cleanup_memory()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    X_emb_combined = np.hstack(X_emb_pca_list) if X_emb_pca_list else None\n",
    "    return X_emb_combined, new_pca_models if fit_on_train else pca_models\n",
    "\n",
    "# Apply IncrementalPCA to embeddings\n",
    "try:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PHASE 2: PCA Preprocessing\")\n",
    "    print(\"=\" * 80)\n",
    "    phase_start = time.time()\n",
    "    \n",
    "    cleanup_memory()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    print(\"\\nüìä Applying IncrementalPCA to embedding families...\")\n",
    "    for fam in X_emb_train_fams.keys():\n",
    "        n_comp = PCA_COMPONENTS_PER_FAMILY.get(fam, 32)\n",
    "        print(f\"  {fam}: {X_emb_train_fams[fam].shape[1]} dims ‚Üí {n_comp} components\")\n",
    "\n",
    "    # Train PCA models on train\n",
    "    X_emb_train_pca, pca_models_train = apply_pca_to_embeddings(\n",
    "        X_emb_train_fams, fit_on_train=True\n",
    "    )\n",
    "\n",
    "    cleanup_memory()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Transform validation using trained PCA models\n",
    "    X_emb_val_pca, _ = apply_pca_to_embeddings(\n",
    "        X_emb_val_fams, fit_on_train=False, pca_models=pca_models_train\n",
    "    )\n",
    "\n",
    "    phase_time = time.time() - phase_start\n",
    "    print(f\"\\nüìä After IncrementalPCA:\")\n",
    "    print(f\"  Train embeddings: {X_emb_train_pca.shape}\")\n",
    "    print(f\"  Val embeddings: {X_emb_val_pca.shape}\")\n",
    "\n",
    "    # Combine regular + embedding features\n",
    "    if X_reg_train is not None:\n",
    "        X_train = np.hstack([X_reg_train, X_emb_train_pca])\n",
    "        X_val = np.hstack([X_reg_val, X_emb_val_pca])\n",
    "    else:\n",
    "        X_train = X_emb_train_pca\n",
    "        X_val = X_emb_val_pca\n",
    "\n",
    "    print(f\"  Combined train: {X_train.shape}\")\n",
    "    print(f\"  Combined val: {X_val.shape}\")\n",
    "    print(f\"\\n‚è±Ô∏è  PCA Preprocessing Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\")\n",
    "\n",
    "    del X_reg_train, X_reg_val, X_emb_train_fams, X_emb_val_fams, X_emb_train_pca, X_emb_val_pca\n",
    "    cleanup_memory()\n",
    "    memory_usage()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in PCA: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32f167b",
   "metadata": {
    "papermill": {
     "duration": 0.00668,
     "end_time": "2025-11-19T11:06:13.625162",
     "exception": false,
     "start_time": "2025-11-19T11:06:13.618482",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Class Imbalance Handling: SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eb70b37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T11:06:13.639623Z",
     "iopub.status.busy": "2025-11-19T11:06:13.639200Z",
     "iopub.status.idle": "2025-11-19T11:32:23.475973Z",
     "shell.execute_reply": "2025-11-19T11:32:23.474778Z"
    },
    "papermill": {
     "duration": 1569.861018,
     "end_time": "2025-11-19T11:32:23.492407",
     "exception": false,
     "start_time": "2025-11-19T11:06:13.631389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 3: SMOTETomek Resampling\n",
      "================================================================================\n",
      "\n",
      "üìä Checking class imbalance and applying SMOTETomek resampling...\n",
      "  Before: 960000 samples, Positive: 65808, Negative: 894192\n",
      "  Imbalance ratio: 13.59:1\n",
      "  ‚ö†Ô∏è Large dataset detected (960,000 samples), using sampling_strategy=0.2 for memory efficiency\n",
      "  Fitting SMOTETomek...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  After: 1068602 samples, Positive: 176624, Negative: 891978\n",
      "  Balance ratio: 5.05:1\n",
      "\n",
      "‚è±Ô∏è  SMOTETomek Resampling Time: 1564.99 seconds (26.08 minutes)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Skip SMOTETomek for very large datasets (>100k samples) - use class_weight instead\n",
    "USE_CLASS_WEIGHT = False  # Will be set based on dataset size\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 3: SMOTETomek Resampling\")\n",
    "print(\"=\" * 80)\n",
    "phase_start = time.time()\n",
    "\n",
    "print(\"\\nüìä Checking class imbalance and applying SMOTETomek resampling...\")\n",
    "print(f\"  Before: {len(X_train)} samples, Positive: {y_train.sum()}, Negative: {(y_train == 0).sum()}\")\n",
    "print(f\"  Imbalance ratio: {(y_train == 0).sum() / max(y_train.sum(), 1):.2f}:1\")\n",
    "\n",
    "try:\n",
    "    # SMOTETomek is REQUIRED - use adaptive strategy for large datasets\n",
    "    if len(X_train) > 500_000:\n",
    "        print(f\"  ‚ö†Ô∏è Large dataset detected ({len(X_train):,} samples), using sampling_strategy=0.2 for memory efficiency\")\n",
    "        smt = SMOTETomek(random_state=42, sampling_strategy=0.2, n_jobs=-1)\n",
    "    else:\n",
    "        smt = SMOTETomek(random_state=42, sampling_strategy=0.4, n_jobs=-1)\n",
    "    \n",
    "    # Fit and resample with memory cleanup\n",
    "    print(\"  Fitting SMOTETomek...\")\n",
    "    cleanup_memory()\n",
    "    X_train_resampled, y_train_resampled = smt.fit_resample(X_train, y_train)\n",
    "    cleanup_memory()\n",
    "\n",
    "    phase_time = time.time() - phase_start\n",
    "    print(f\"  After: {len(X_train_resampled)} samples, Positive: {y_train_resampled.sum()}, Negative: {(y_train_resampled == 0).sum()}\")\n",
    "    print(f\"  Balance ratio: {(y_train_resampled == 0).sum() / max(y_train_resampled.sum(), 1):.2f}:1\")\n",
    "    print(f\"\\n‚è±Ô∏è  SMOTETomek Resampling Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\")\n",
    "\n",
    "    X_train = X_train_resampled\n",
    "    y_train = y_train_resampled\n",
    "\n",
    "    del X_train_resampled, y_train_resampled\n",
    "    cleanup_memory()\n",
    "except Exception as e:\n",
    "    phase_time = time.time() - phase_start\n",
    "    print(f\"  ‚ö†Ô∏è SMOTETomek failed: {e}\")\n",
    "    print(\"  Continuing with original training data...\")\n",
    "    print(f\"\\n‚è±Ô∏è  SMOTETomek Time (failed): {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\")\n",
    "    cleanup_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9ac6c7",
   "metadata": {
    "papermill": {
     "duration": 0.005877,
     "end_time": "2025-11-19T11:32:23.504798",
     "exception": false,
     "start_time": "2025-11-19T11:32:23.498921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89b3ad4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T11:32:23.518058Z",
     "iopub.status.busy": "2025-11-19T11:32:23.517617Z",
     "iopub.status.idle": "2025-11-19T11:32:33.049086Z",
     "shell.execute_reply": "2025-11-19T11:32:33.047873Z"
    },
    "papermill": {
     "duration": 9.540222,
     "end_time": "2025-11-19T11:32:33.050529",
     "exception": false,
     "start_time": "2025-11-19T11:32:23.510307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 4: Feature Scaling\n",
      "================================================================================\n",
      "\n",
      "üìä Applying Feature Scaling to combined features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting scaler on sample (50000 samples) for OOM protection...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transforming train data in chunks (size=50000)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Transforming val data in chunks (size=50000)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Scaling complete!\n",
      "\n",
      "‚è±Ô∏è  Feature Scaling Time: 7.79 seconds (0.13 minutes)\n",
      "üíæ Memory: 35.06 GB (RAM) | 0.00/0.00 GB (GPU used/reserved)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 4: Feature Scaling\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "phase_start = time.time()\n",
    "print(\"\\nüìä Applying Feature Scaling to combined features...\")\n",
    "\n",
    "# Store raw (unscaled) data for CV Pipeline (scaler will be fit per fold)\n",
    "X_train_raw = X_train.copy()\n",
    "X_val_raw = X_val.copy()\n",
    "y_train_raw = y_train.copy()\n",
    "y_val_raw = y_val.copy()\n",
    "\n",
    "# Use StandardScaler (RobustScaler doesn't support partial_fit)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# For large datasets, fit on sample then transform in chunks\n",
    "CHUNK_SIZE = 50000\n",
    "\n",
    "if X_train.shape[0] > CHUNK_SIZE:\n",
    "    print(f\"  Fitting scaler on sample ({min(CHUNK_SIZE, X_train.shape[0])} samples) for OOM protection...\")\n",
    "    sample_indices = np.random.choice(X_train.shape[0], size=min(CHUNK_SIZE, X_train.shape[0]), replace=False)\n",
    "    scaler.fit(X_train[sample_indices])\n",
    "    del sample_indices\n",
    "    cleanup_memory()\n",
    "\n",
    "    # Transform train in chunks\n",
    "    print(f\"  Transforming train data in chunks (size={CHUNK_SIZE})...\")\n",
    "    X_train_chunks = []\n",
    "    for i in range(0, X_train.shape[0], CHUNK_SIZE):\n",
    "        chunk = scaler.transform(X_train[i:i + CHUNK_SIZE])\n",
    "        X_train_chunks.append(chunk)\n",
    "        del chunk\n",
    "        if i % (CHUNK_SIZE * 5) == 0:\n",
    "            cleanup_memory()\n",
    "    X_train = np.vstack(X_train_chunks)\n",
    "    del X_train_chunks\n",
    "    cleanup_memory()\n",
    "\n",
    "    # Transform val in chunks\n",
    "    if X_val.shape[0] > CHUNK_SIZE:\n",
    "        print(f\"  Transforming val data in chunks (size={CHUNK_SIZE})...\")\n",
    "        X_val_chunks = []\n",
    "        for i in range(0, X_val.shape[0], CHUNK_SIZE):\n",
    "            chunk = scaler.transform(X_val[i:i + CHUNK_SIZE])\n",
    "            X_val_chunks.append(chunk)\n",
    "            del chunk\n",
    "        X_val = np.vstack(X_val_chunks)\n",
    "        del X_val_chunks\n",
    "    else:\n",
    "        X_val = scaler.transform(X_val)\n",
    "else:\n",
    "    # Small dataset - fit and transform normally\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "cleanup_memory()\n",
    "phase_time = time.time() - phase_start\n",
    "print(\"  ‚úÖ Scaling complete!\")\n",
    "print(f\"\\n‚è±Ô∏è  Feature Scaling Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\")\n",
    "memory_usage()\n",
    "\n",
    "# Store raw (unscaled) data again for safety (if further processing needed)\n",
    "X_train_raw = X_train.copy()\n",
    "X_val_raw = X_val.copy()\n",
    "y_train_raw = y_train.copy()\n",
    "y_val_raw = y_val.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e319f05f",
   "metadata": {
    "papermill": {
     "duration": 0.006896,
     "end_time": "2025-11-19T11:32:33.071384",
     "exception": false,
     "start_time": "2025-11-19T11:32:33.064488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Cross-Validation & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a164ee0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T11:32:33.084978Z",
     "iopub.status.busy": "2025-11-19T11:32:33.084543Z",
     "iopub.status.idle": "2025-11-19T11:32:37.703950Z",
     "shell.execute_reply": "2025-11-19T11:32:37.702845Z"
    },
    "papermill": {
     "duration": 4.628644,
     "end_time": "2025-11-19T11:32:37.705802",
     "exception": false,
     "start_time": "2025-11-19T11:32:33.077158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 5: Hyperparameter Tuning Setup\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Dataset too large (1188602 samples), using subset (25000 samples) for CV\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using 25000 samples for hyperparameter tuning\n",
      "\n",
      "üìä Full dataset for CV: (25000, 150), labels: (25000,)\n",
      "  Positive samples: 3885, Negative: 21115\n",
      "\n",
      "üîç Hyperparameter tuning:\n",
      "  Method: RandomizedSearchCV\n",
      "  CV folds: 3\n",
      "  Random iterations: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Memory: 34.68 GB (RAM) | 0.00/0.00 GB (GPU used/reserved)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 5: Hyperparameter Tuning Setup\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Combine train and val for CV\n",
    "# Use RAW (unscaled) data for CV ‚Äì Pipeline will scale per fold\n",
    "X_full = np.vstack([X_train_raw, X_val_raw])\n",
    "y_full = np.hstack([y_train_raw, y_val_raw])\n",
    "\n",
    "# Use subset for hyperparameter tuning if dataset is too large (memory optimization)\n",
    "MAX_SAMPLES_FOR_CV = 25000  # Limit to 25k samples for CV\n",
    "if len(X_full) > MAX_SAMPLES_FOR_CV:\n",
    "    print(f\"‚ö†Ô∏è Dataset too large ({len(X_full)} samples), using subset ({MAX_SAMPLES_FOR_CV} samples) for CV\")\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_full, _, y_full, _ = train_test_split(\n",
    "        X_full, y_full,\n",
    "        train_size=MAX_SAMPLES_FOR_CV,\n",
    "        stratify=y_full,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    print(f\"  Using {len(X_full)} samples for hyperparameter tuning\")\n",
    "    cleanup_memory()\n",
    "\n",
    "print(f\"\\nüìä Full dataset for CV: {X_full.shape}, labels: {y_full.shape}\")\n",
    "print(f\"  Positive samples: {y_full.sum()}, Negative: {(y_full == 0).sum()}\")\n",
    "\n",
    "# Setup 3-fold Stratified CV\n",
    "N_FOLDS = 3\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# LightGBM hyperparameter grid (optimized to prevent warnings)\n",
    "# Key fixes:\n",
    "# - Remove min_split_gain from grid (set as fixed parameter = 0.0 to prevent \"no splits\" warnings)\n",
    "# - Lower min_child_samples to allow more granular splits\n",
    "# - Use feature_fraction ONLY (never colsample_bytree) to avoid parameter conflict\n",
    "# - Add num_leaves for better tree control\n",
    "# - Add force_col_wise to remove threading overhead warning\n",
    "LGBM_PARAM_GRID = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [3, 5, 7, -1],  # -1 means no limit\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    \"subsample\": [0.8, 0.9, 1.0],\n",
    "    \"feature_fraction\": [0.8, 0.9, 1.0],  # Use feature_fraction, NEVER colsample_bytree\n",
    "    \"min_child_samples\": [5, 10, 20],     # Lowered to allow more splits\n",
    "    # min_split_gain removed from grid - set as fixed parameter = 0.0\n",
    "    \"num_leaves\": [31, 50, 100],          # Control tree complexity\n",
    "    \"reg_alpha\": [0, 0.1, 0.5],\n",
    "    \"reg_lambda\": [1, 1.5, 2.0],\n",
    "    \"scale_pos_weight\": [1, (y_full == 0).sum() / max((y_full == 1).sum(), 1)],\n",
    "}\n",
    "\n",
    "# Ensure colsample_bytree is NEVER in the grid\n",
    "assert 'colsample_bytree' not in LGBM_PARAM_GRID, \"colsample_bytree should not be in parameter grid for LightGBM\"\n",
    "\n",
    "# Use RandomizedSearchCV for faster tuning\n",
    "USE_RANDOMIZED_SEARCH = True\n",
    "\n",
    "# Reduce iterations for large datasets\n",
    "if len(X_full) > 100_000:\n",
    "    N_ITER_RANDOM = 10  # Fewer iterations for large datasets\n",
    "else:\n",
    "    N_ITER_RANDOM = 20\n",
    "N_ITER_RANDOM = 10  # Force 10 for memory efficiency\n",
    "\n",
    "print(f\"\\nüîç Hyperparameter tuning:\")\n",
    "print(f\"  Method: {'RandomizedSearchCV' if USE_RANDOMIZED_SEARCH else 'GridSearchCV'}\")\n",
    "print(f\"  CV folds: {N_FOLDS}\")\n",
    "if USE_RANDOMIZED_SEARCH:\n",
    "    print(f\"  Random iterations: {N_ITER_RANDOM}\")\n",
    "\n",
    "cleanup_memory()\n",
    "memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "hyperparam_tuning_cell",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T11:32:37.729893Z",
     "iopub.status.busy": "2025-11-19T11:32:37.729580Z",
     "iopub.status.idle": "2025-11-19T13:32:39.179897Z"
    },
    "papermill": {
     "duration": 7200.009709,
     "end_time": "2025-11-19T13:32:37.730509",
     "exception": false,
     "start_time": "2025-11-19T11:32:37.720800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 5B: Hyperparameter Tuning\n",
      "================================================================================\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Fit with error handling\u001b[39;00m\n\u001b[32m     52\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[43msearch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_full\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m elapsed_time = time.time() - start_time\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Extract best model and parameters from pipeline\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/venv/lib/python3.11/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/venv/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/venv/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1992\u001b[39m, in \u001b[36mRandomizedSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1990\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1991\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1992\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1993\u001b[39m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1994\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[32m   1995\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1996\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/venv/lib/python3.11/site-packages/sklearn/model_selection/_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/venv/lib/python3.11/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/venv/lib/python3.11/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/venv/lib/python3.11/site-packages/joblib/parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning with Pipeline (prevents data leakage)\n",
    "best_model = None\n",
    "best_params = None\n",
    "best_cv_score = 0.0\n",
    "\n",
    "try:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PHASE 5B: Hyperparameter Tuning\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Base LightGBM model\n",
    "    base_model = LGBMClassifier(\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "        objective='binary',\n",
    "        verbose=-1,\n",
    "        min_split_gain=0.0,  # Fixed to prevent warnings\n",
    "        force_col_wise=True,\n",
    "    )\n",
    "\n",
    "    # Create Pipeline: scaler -> model (scaler fit per CV fold)\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', base_model)\n",
    "    ])\n",
    "\n",
    "    # Transform parameter grid for Pipeline\n",
    "    pipeline_param_grid = {f'model__{k}': v for k, v in LGBM_PARAM_GRID.items()}\n",
    "\n",
    "    if USE_RANDOMIZED_SEARCH:\n",
    "        search = RandomizedSearchCV(\n",
    "            pipeline,\n",
    "            pipeline_param_grid,\n",
    "            cv=skf,\n",
    "            scoring='f1',\n",
    "            n_iter=N_ITER_RANDOM,\n",
    "            random_state=SEED,\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "        )\n",
    "    else:\n",
    "        search = GridSearchCV(\n",
    "            pipeline,\n",
    "            pipeline_param_grid,\n",
    "            cv=skf,\n",
    "            scoring='f1',\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "    # Fit with error handling\n",
    "    start_time = time.time()\n",
    "    search.fit(X_full, y_full)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Extract best model and parameters from pipeline\n",
    "    best_pipeline = search.best_estimator_\n",
    "    best_model = best_pipeline.named_steps['model']\n",
    "    # Extract model parameters (remove 'model__' prefix)\n",
    "    best_params = {k.replace('model__', ''): v for k, v in search.best_params_.items()}\n",
    "    best_cv_score = search.best_score_\n",
    "\n",
    "    print(f\"\\n‚úÖ Hyperparameter tuning complete ({elapsed_time/60:.1f} min)\")\n",
    "    print(f\"  Best CV F1: {best_cv_score:.4f}\")\n",
    "    print(f\"  Best parameters:\")\n",
    "    for key, value in best_params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "\n",
    "    cleanup_memory()\n",
    "    memory_usage()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in hyperparameter tuning: {e}\")\n",
    "    print(\"‚ö†Ô∏è Using default parameters...\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    best_params = {}\n",
    "    best_cv_score = 0.0\n",
    "    cleanup_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0125edda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:54:37.817800Z",
     "iopub.status.busy": "2025-11-19T09:54:37.817655Z",
     "iopub.status.idle": "2025-11-19T09:54:38.543501Z",
     "shell.execute_reply": "2025-11-19T09:54:38.542038Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZATION: CV Performance Curves\n",
    "# ============================================\n",
    "try:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PLOTTING: Cross-Validation Performance Curves\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Get predictions from best model on CV data for visualization\n",
    "    if ('search' in locals() and hasattr(search, 'best_estimator_')) or best_model is not None:\n",
    "        # Use the best pipeline to get predictions\n",
    "        # Use the best pipeline from RandomizedSearchCV\n",
    "        if 'search' in locals() and hasattr(search, 'best_estimator_'):\n",
    "            best_pipeline = search.best_estimator_\n",
    "        elif best_model is not None:\n",
    "            # Fallback: create pipeline with best model\n",
    "            from sklearn.pipeline import Pipeline\n",
    "            best_pipeline = Pipeline([('scaler', StandardScaler().fit(X_full)), ('model', best_model)])\n",
    "        else:\n",
    "            raise ValueError(\"No best model available\")\n",
    "        y_cv_proba = best_pipeline.predict_proba(X_full)[:, 1]\n",
    "        \n",
    "        # Calculate PR and ROC curves\n",
    "        precision, recall, pr_thresholds = precision_recall_curve(y_full, y_cv_proba)\n",
    "        fpr, tpr, roc_thresholds = roc_curve(y_full, y_cv_proba)\n",
    "        roc_auc = roc_auc_score(y_full, y_cv_proba)\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # PR Curve\n",
    "        axes[0].plot(recall, precision, color='darkorange', lw=2, \n",
    "                    label=f'PR Curve (AUC = {np.trapz(precision, recall):.4f})')\n",
    "        axes[0].axhline(y=y_full.mean(), color='navy', linestyle='--', \n",
    "                        label=f'Baseline (Precision = {y_full.mean():.4f})')\n",
    "        axes[0].set_xlabel('Recall', fontsize=12)\n",
    "        axes[0].set_ylabel('Precision', fontsize=12)\n",
    "        axes[0].set_title('Precision-Recall Curve (CV)', fontsize=14, fontweight='bold')\n",
    "        axes[0].legend(loc='best')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # ROC Curve\n",
    "        axes[1].plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                    label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "        axes[1].plot([0, 1], [0, 1], color='navy', linestyle='--', label='Random Classifier')\n",
    "        axes[1].set_xlabel('False Positive Rate', fontsize=12)\n",
    "        axes[1].set_ylabel('True Positive Rate', fontsize=12)\n",
    "        axes[1].set_title('ROC Curve (CV)', fontsize=14, fontweight='bold')\n",
    "        axes[1].legend(loc='best')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        cv_plot_path = PROJECT_ROOT / 'logs' / 'model_lightgbm_cv_curves.png'\n",
    "        plt.savefig(cv_plot_path, dpi=150, bbox_inches='tight')\n",
    "        plt.show()  # Display plot in notebook\n",
    "        display(Image(str(cv_plot_path)))  # Display inline\n",
    "        print(\"‚úÖ CV curves saved to: logs/model_lightgbm_cv_curves.png\")\n",
    "        plt.close()\n",
    "        cleanup_memory()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Skipping CV visualization (best_model or data not available)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error plotting CV curves: {e}\")\n",
    "    cleanup_memory()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73de6174",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Threshold Tuning & Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45df782",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:54:38.570272Z",
     "iopub.status.busy": "2025-11-19T09:54:38.570116Z",
     "iopub.status.idle": "2025-11-19T09:54:44.211049Z",
     "shell.execute_reply": "2025-11-19T09:54:44.208958Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train final model on full data\n",
    "try:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PHASE 6: Final Model Training & Threshold Tuning\")\n",
    "    print(\"=\" * 80)\n",
    "    phase_start = time.time()\n",
    "    print(\"Training Final Model on Full Dataset...\")\n",
    "\n",
    "    # Use best parameters, or defaults if best_params is None\n",
    "    # Initialize best_params if not set\n",
    "    if 'best_params' not in locals():\n",
    "        best_params = {}\n",
    "        best_cv_score = 0.0\n",
    "        print(\"  ‚ö†Ô∏è best_params not found, using defaults\")\n",
    "\n",
    "    # Initialize best_params if not set\n",
    "    try:\n",
    "        _ = best_params\n",
    "    except NameError:\n",
    "        best_params = {}\n",
    "        best_cv_score = 0.0\n",
    "        print(\"  ‚ö†Ô∏è best_params not found, using defaults\")\n",
    "\n",
    "    # Use best parameters, or defaults if best_params is None\n",
    "    # Initialize best_params if not set (from hyperparameter tuning)\n",
    "    if 'best_params' not in globals():\n",
    "        best_params = {}\n",
    "        best_cv_score = 0.0\n",
    "        print(\"  ‚ö†Ô∏è best_params not found, using defaults\")\n",
    "\n",
    "    # Use best parameters, or defaults if best_params is None\n",
    "    final_params = best_params.copy() if best_params else {}\n",
    "\n",
    "    # Remove colsample_bytree if present (should never be, but double-check)\n",
    "    if 'colsample_bytree' in final_params:\n",
    "        print(\"  ‚ö†Ô∏è Removing colsample_bytree from final_params (using feature_fraction instead)\")\n",
    "        del final_params['colsample_bytree']\n",
    "\n",
    "    # Set safe defaults to prevent warnings\n",
    "    final_params['min_split_gain'] = 0.0\n",
    "    final_params.setdefault('min_child_samples', 5)\n",
    "    final_params['force_col_wise'] = True\n",
    "\n",
    "    final_model = LGBMClassifier(\n",
    "        **final_params,\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "        objective='binary',\n",
    "        verbose=-1,  # Suppress info messages (warnings still shown)\n",
    "    )\n",
    "\n",
    "    # Aggressive cleanup before final training\n",
    "    cleanup_memory()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"  Memory cleaned before final training\")\n",
    "\n",
    "    final_model.fit(X_train, y_train)\n",
    "\n",
    "    # Get predictions on validation set (original split)\n",
    "    y_val_proba = safe_prediction(final_model.predict_proba, X=X_val)[:, 1]\n",
    "\n",
    "    # Find optimal threshold using precision-recall curve\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(y_val, y_val_proba)\n",
    "    f1_scores_pr = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    best_pr_idx = np.argmax(f1_scores_pr)\n",
    "    best_pr_threshold = pr_thresholds[best_pr_idx] if best_pr_idx < len(pr_thresholds) else 0.5\n",
    "    best_pr_f1 = f1_scores_pr[best_pr_idx]\n",
    "\n",
    "    # Manual fine-grained search in optimal region\n",
    "    thresholds = np.concatenate([\n",
    "        np.linspace(0.01, 0.05, 20),\n",
    "        np.linspace(0.05, 0.15, 50),\n",
    "        np.linspace(0.15, 0.3, 30),\n",
    "        np.linspace(0.3, 0.9, 20)\n",
    "    ])\n",
    "\n",
    "    best_threshold = best_pr_threshold\n",
    "    best_f1 = best_pr_f1\n",
    "    for thr in thresholds:\n",
    "        y_pred = (y_val_proba >= thr).astype(int)\n",
    "        f1 = f1_score(y_val, y_pred, pos_label=1, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = thr\n",
    "\n",
    "    phase_time = time.time() - phase_start\n",
    "    print(f\"\\n‚úÖ Final Optimal Threshold: {best_threshold:.4f}\")\n",
    "    print(f\"‚úÖ Final Validation F1: {best_f1:.4f}\")\n",
    "    print(f\"\\n‚è±Ô∏è  Final Model Training & Threshold Tuning Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\")\n",
    "\n",
    "    # Classification report\n",
    "    y_val_pred = (y_val_proba >= best_threshold).astype(int)\n",
    "    print(\"\\nüìä Classification Report:\")\n",
    "    print(classification_report(y_val, y_val_pred, digits=4, zero_division=0))\n",
    "\n",
    "    cleanup_memory()\n",
    "    memory_usage()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in final training: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ae8b44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:54:44.230893Z",
     "iopub.status.busy": "2025-11-19T09:54:44.230745Z",
     "iopub.status.idle": "2025-11-19T09:54:45.810063Z",
     "shell.execute_reply": "2025-11-19T09:54:45.807120Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZATION: Final Model Performance\n",
    "# ============================================\n",
    "try:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PLOTTING: Final Model Performance Visualizations\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if 'final_model' in locals() and 'y_val_proba' in locals() and 'y_val' in locals():\n",
    "        # Calculate curves\n",
    "        precision, recall, pr_thresholds = precision_recall_curve(y_val, y_val_proba)\n",
    "        fpr, tpr, roc_thresholds = roc_curve(y_val, y_val_proba)\n",
    "        roc_auc = roc_auc_score(y_val, y_val_proba)\n",
    "        pr_auc = np.trapz(precision, recall)\n",
    "        \n",
    "        # Create comprehensive figure\n",
    "        fig = plt.figure(figsize=(16, 12))\n",
    "        \n",
    "        # 1. PR Curve\n",
    "        ax1 = plt.subplot(2, 3, 1)\n",
    "        ax1.plot(recall, precision, color='darkorange', lw=2, \n",
    "                label=f'PR Curve (AUC = {pr_auc:.4f})')\n",
    "        ax1.axhline(y=y_val.mean(), color='navy', linestyle='--', \n",
    "                   label=f'Baseline (Precision = {y_val.mean():.4f})')\n",
    "        ax1.axvline(x=recall[np.argmax(precision * recall)], color='red', linestyle=':', \n",
    "                   alpha=0.7, label=f'Optimal Point')\n",
    "        ax1.set_xlabel('Recall', fontsize=11)\n",
    "        ax1.set_ylabel('Precision', fontsize=11)\n",
    "        ax1.set_title('Precision-Recall Curve', fontsize=12, fontweight='bold')\n",
    "        ax1.legend(loc='best', fontsize=9)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. ROC Curve\n",
    "        ax2 = plt.subplot(2, 3, 2)\n",
    "        ax2.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "        ax2.plot([0, 1], [0, 1], color='navy', linestyle='--', label='Random Classifier')\n",
    "        ax2.set_xlabel('False Positive Rate', fontsize=11)\n",
    "        ax2.set_ylabel('True Positive Rate', fontsize=11)\n",
    "        ax2.set_title('ROC Curve', fontsize=12, fontweight='bold')\n",
    "        ax2.legend(loc='best', fontsize=9)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Confusion Matrix\n",
    "        ax3 = plt.subplot(2, 3, 3)\n",
    "        cm = confusion_matrix(y_val, y_val_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax3, \n",
    "                   xticklabels=['Negative', 'Positive'], \n",
    "                   yticklabels=['Negative', 'Positive'])\n",
    "        ax3.set_ylabel('True Label', fontsize=11)\n",
    "        ax3.set_xlabel('Predicted Label', fontsize=11)\n",
    "        ax3.set_title(f'Confusion Matrix\\n(Threshold = {best_threshold:.4f})', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # 4. Feature Importance (Top 20)\n",
    "        ax4 = plt.subplot(2, 3, 4)\n",
    "        if hasattr(final_model, 'feature_importances_'):\n",
    "            feature_importance = final_model.feature_importances_\n",
    "            top_n = min(20, len(feature_importance))\n",
    "            top_indices = np.argsort(feature_importance)[-top_n:][::-1]\n",
    "            top_importance = feature_importance[top_indices]\n",
    "            ax4.barh(range(top_n), top_importance, color='steelblue')\n",
    "            ax4.set_yticks(range(top_n))\n",
    "            ax4.set_yticklabels([f'Feature {i}' for i in top_indices], fontsize=8)\n",
    "            ax4.set_xlabel('Importance', fontsize=11)\n",
    "            ax4.set_title(f'Top {top_n} Feature Importances', fontsize=12, fontweight='bold')\n",
    "            ax4.grid(True, alpha=0.3, axis='x')\n",
    "        else:\n",
    "            ax4.text(0.5, 0.5, 'Feature importance\\nnot available', \n",
    "                    ha='center', va='center', transform=ax4.transAxes, fontsize=12)\n",
    "            ax4.set_title('Feature Importance', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # 5. Threshold Analysis\n",
    "        ax5 = plt.subplot(2, 3, 5)\n",
    "        thresholds_fine = np.linspace(0, 1, 100)\n",
    "        f1_scores_fine = []\n",
    "        for thr in thresholds_fine:\n",
    "            y_pred_thr = (y_val_proba >= thr).astype(int)\n",
    "            f1_thr = f1_score(y_val, y_pred_thr, pos_label=1, zero_division=0)\n",
    "            f1_scores_fine.append(f1_thr)\n",
    "        ax5.plot(thresholds_fine, f1_scores_fine, color='darkgreen', lw=2, label='F1 Score')\n",
    "        ax5.axvline(x=best_threshold, color='red', linestyle='--', lw=2, \n",
    "                   label=f'Optimal Threshold = {best_threshold:.4f}')\n",
    "        ax5.axhline(y=best_f1, color='red', linestyle=':', alpha=0.7, \n",
    "                   label=f'Best F1 = {best_f1:.4f}')\n",
    "        ax5.set_xlabel('Threshold', fontsize=11)\n",
    "        ax5.set_ylabel('F1 Score', fontsize=11)\n",
    "        ax5.set_title('F1 Score vs Threshold', fontsize=12, fontweight='bold')\n",
    "        ax5.legend(loc='best', fontsize=9)\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 6. Prediction Distribution\n",
    "        ax6 = plt.subplot(2, 3, 6)\n",
    "        ax6.hist(y_val_proba[y_val == 0], bins=50, alpha=0.6, label='Negative', \n",
    "                color='blue', density=True)\n",
    "        ax6.hist(y_val_proba[y_val == 1], bins=50, alpha=0.6, label='Positive', \n",
    "                color='red', density=True)\n",
    "        ax6.axvline(x=best_threshold, color='green', linestyle='--', lw=2, \n",
    "                   label=f'Threshold = {best_threshold:.4f}')\n",
    "        ax6.set_xlabel('Predicted Probability', fontsize=11)\n",
    "        ax6.set_ylabel('Density', fontsize=11)\n",
    "        ax6.set_title('Prediction Probability Distribution', fontsize=12, fontweight='bold')\n",
    "        ax6.legend(loc='best', fontsize=9)\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle('Model LightGBM: LightGBM Performance Analysis', fontsize=14, fontweight='bold', y=0.995)\n",
    "        plt.tight_layout()\n",
    "        final_plot_path = PROJECT_ROOT / 'logs' / 'model_lightgbm_final_performance.png'\n",
    "        plt.savefig(final_plot_path, dpi=150, bbox_inches='tight')\n",
    "        plt.show()  # Display plot in notebook\n",
    "        display(Image(str(final_plot_path)))  # Display inline\n",
    "        print(\"‚úÖ Final performance plots saved to: logs/model_lightgbm_final_performance.png\")\n",
    "        plt.close()\n",
    "        cleanup_memory()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Skipping final visualization (final_model or predictions not available)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error plotting final performance: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    cleanup_memory()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5a6cb2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995db9b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:54:45.877239Z",
     "iopub.status.busy": "2025-11-19T09:54:45.876905Z",
     "iopub.status.idle": "2025-11-19T09:54:45.937490Z",
     "shell.execute_reply": "2025-11-19T09:54:45.936554Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "try:\n",
    "    model_save_path = MODEL_SAVE_DIR / \"model_lightgbm_all_features_best.pkl\"\n",
    "\n",
    "    save_dict = {\n",
    "        \"model\": final_model,\n",
    "        \"scaler\": scaler if 'scaler' in locals() else None,\n",
    "        \"pca_models\": pca_models_train if 'pca_models_train' in locals() else None,\n",
    "        \"best_params\": best_params,\n",
    "        \"best_cv_score\": best_cv_score,\n",
    "        \"best_threshold\": best_threshold,\n",
    "        \"best_f1\": best_f1,\n",
    "        \"reg_cols\": reg_cols,\n",
    "        \"emb_family_to_cols\": emb_family_to_cols,\n",
    "    }\n",
    "\n",
    "    with open(model_save_path, \"wb\") as f:\n",
    "        pickle.dump(save_dict, f)\n",
    "\n",
    "    print(f\"\\nüíæ Model saved to: {model_save_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving model: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eaca92",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9. Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e847f217",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:54:45.982270Z",
     "iopub.status.busy": "2025-11-19T09:54:45.982057Z",
     "iopub.status.idle": "2025-11-19T09:54:50.553840Z",
     "shell.execute_reply": "2025-11-19T09:54:50.552005Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_work_id(id_value: str) -> str:\n",
    "    \"\"\"Extract work_id from URL or return as is if already just ID.\"\"\"\n",
    "    id_str = str(id_value)\n",
    "    # If it already looks like a work ID, just return it\n",
    "    if id_str.startswith('W') and len(id_str) > 1 and '/' not in id_str:\n",
    "        return id_str\n",
    "    # Otherwise, extract from URL or string\n",
    "    match = re.search(r'W\\d+', id_str)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return id_str\n",
    "\n",
    "# Load test data and generate predictions\n",
    "try:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PHASE 7: Test Predictions\")\n",
    "    print(\"=\" * 80)\n",
    "    phase_start = time.time()\n",
    "    print(\"Generating Test Predictions...\")\n",
    "\n",
    "    test_df = load_parquet_split(\"test\")\n",
    "    test_ids = test_df[\"id\"].to_numpy()\n",
    "\n",
    "    # Process test data same as train\n",
    "    X_reg_test, X_emb_test_fams, _, _, _ = split_features_reg_and_all_emb(test_df)\n",
    "    del test_df\n",
    "\n",
    "    # Apply PCA\n",
    "    X_emb_test_pca, _ = apply_pca_to_embeddings(\n",
    "        X_emb_test_fams, fit_on_train=False, pca_models=pca_models_train\n",
    "    )\n",
    "\n",
    "    # Combine features\n",
    "    if X_reg_test is not None:\n",
    "        X_test = np.hstack([X_reg_test, X_emb_test_pca])\n",
    "    else:\n",
    "        X_test = X_emb_test_pca\n",
    "\n",
    "    del X_reg_test, X_emb_test_fams, X_emb_test_pca\n",
    "    cleanup_memory()\n",
    "\n",
    "    # Scale\n",
    "    if \"scaler\" in locals():\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Predict in chunks for Out Of Memory protection\n",
    "    chunk_size = 10000\n",
    "    if X_test.shape[0] > chunk_size:\n",
    "        print(f\"  Predicting in chunks (size={chunk_size}) for OOM protection...\")\n",
    "        y_test_proba_chunks = []\n",
    "        for i in range(0, X_test.shape[0], chunk_size):\n",
    "            chunk_proba = safe_prediction(final_model.predict_proba, X=X_test[i : i + chunk_size])[:, 1]\n",
    "            y_test_proba_chunks.append(chunk_proba)\n",
    "            del chunk_proba\n",
    "            cleanup_memory()\n",
    "        y_test_proba = np.concatenate(y_test_proba_chunks)\n",
    "        del y_test_proba_chunks\n",
    "    else:\n",
    "        y_test_proba = safe_prediction(final_model.predict_proba, X=X_test)[:, 1]\n",
    "\n",
    "    y_test_pred = (y_test_proba >= best_threshold).astype(int)\n",
    "\n",
    "    # Create submission using Polars\n",
    "    work_ids = np.array([extract_work_id(str(id_val)) for id_val in test_ids])\n",
    "    submission_df = pl.DataFrame({\"work_id\": work_ids, \"label\": y_test_pred})\n",
    "\n",
    "    submission_path = SUBMISSION_DIR / \"submission_model_lightgbm.csv\"\n",
    "    submission_df.write_csv(submission_path)\n",
    "\n",
    "    phase_time = time.time() - phase_start\n",
    "    print(f\"\\n‚úÖ Submission saved to: {submission_path}\")\n",
    "    print(f\"  Test predictions: {len(y_test_pred)}, Positive: {y_test_pred.sum()}, Negative: {(y_test_pred==0).sum()}\")\n",
    "    print(f\"\\n‚è±Ô∏è  Test Predictions Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\")\n",
    "\n",
    "    cleanup_memory()\n",
    "    memory_usage()\n",
    "    \n",
    "    # Print total execution time summary\n",
    "    total_time = time.time() - TOTAL_START_TIME\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    # Print total execution time summary\n",
    "    total_time = time.time() - TOTAL_START_TIME\n",
    "    END_TIME_STR = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL_LIGHTGBM EXECUTION COMPLETED\")\n",
    "    print(f\"Start Time: {START_TIME_STR}\")\n",
    "    print(f\"End Time: {END_TIME_STR}\")\n",
    "    print(f\"Total Execution Time: {total_time:.2f} seconds ({total_time/60:.2f} minutes / {total_time/3600:.2f} hours)\")\n",
    "    print(f\"Final Validation F1 Score: {best_f1:.4f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(f\"End Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Final Validation F1 Score: {best_f1:.4f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    total_time = time.time() - TOTAL_START_TIME\n",
    "    print(f\"\\n‚ùå Error generating submission: {e}\")\n",
    "    print(f\"Execution failed after {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9218.740579,
   "end_time": "2025-11-19T13:32:43.643736",
   "environment_variables": {},
   "exception": null,
   "input_path": "src/notebooks/model_lightgbm_all_features.ipynb",
   "output_path": "/gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/runs/model_lightgbm_executed_20251119-055855.ipynb",
   "parameters": {},
   "start_time": "2025-11-19T10:59:04.903157",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}