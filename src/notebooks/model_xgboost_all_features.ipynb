{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4748b24",
   "metadata": {
    "papermill": {
     "duration": 0.007626,
     "end_time": "2025-11-19T09:52:49.560562",
     "exception": false,
     "start_time": "2025-11-19T09:52:49.552936",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 5: XGBoost with All Features\n",
    "\n",
    "This notebook trains an **XGBoost** classifier on all available features (regular + all embeddings) with comprehensive preprocessing:\n",
    "- âœ… All regular features\n",
    "- âœ… All embedding families (PCA-compressed)\n",
    "- âœ… Feature scaling (StandardScaler)\n",
    "- âœ… 5-fold Cross-Validation\n",
    "- âœ… Comprehensive Hyperparameter Tuning (RandomizedSearchCV)\n",
    "- âœ… Threshold Fine-tuning\n",
    "- âœ… Model Saving\n",
    "- âœ… Submission.csv Generation\n",
    "- âœ… OOM Safe with aggressive memory management\n",
    "- âœ… Incremental SMOTE (ENN/Tomek) for class imbalance - OOM-resistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a393cd0",
   "metadata": {
    "papermill": {
     "duration": 0.005052,
     "end_time": "2025-11-19T09:52:49.571903",
     "exception": false,
     "start_time": "2025-11-19T09:52:49.566851",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ðŸ“‘ Model 5 - Code Navigation Index\n",
    "\n",
    "## Quick Navigation\n",
    "- **[Setup](#1-setup)** - Imports, paths, device configuration, robustness utilities\n",
    "- **[Data Loading](#2-data-loading--feature-extraction)** - Load and split features\n",
    "- **[PCA Preprocessing](#3-feature-preprocessing-pca)** - Embedding compression (if applicable)\n",
    "- **[SMOTETomek](#4-class-imbalance-handling-smotetomek)** - Class imbalance resampling\n",
    "- **[Feature Scaling](#5-feature-scaling)** - StandardScaler normalization\n",
    "- **[Cross-Validation](#6-cross-validation--hyperparameter-tuning)** - Hyperparameter optimization\n",
    "- **[Threshold Tuning](#7-threshold-tuning--final-evaluation)** - Optimal threshold finding\n",
    "- **[Model Saving](#8-save-model)** - Save model weights and metadata\n",
    "- **[Submission](#9-generate-submission)** - Generate test predictions\n",
    "\n",
    "## Model Type: XGBoost (all features)\n",
    "\n",
    "## Key Features\n",
    "âœ… GPU-friendly with CPU fallback  \n",
    "âœ… Aggressive garbage collection  \n",
    "âœ… OOM resistant with chunked processing  \n",
    "âœ… Kernel panic resistant (signal handlers, checkpoints)  \n",
    "âœ… Polars-only (no pandas)  \n",
    "âœ… GPU-friendly PCA (IncrementalTorchPCA option)  \n",
    "âœ… Incremental SMOTE (ENN/Tomek) for class imbalance - OOM-resistant  \n",
    "âœ… Feature scaling & embedding normalization  \n",
    "âœ… Hyperparameter tuning (RandomizedSearchCV/GridSearchCV)  \n",
    "âœ… Fine-grained threshold optimization (120+ thresholds)  \n",
    "âœ… Model weights saved  \n",
    "âœ… Chunked/batched data processing  \n",
    "\n",
    "## Memory Management\n",
    "- `cleanup_memory()`: Aggressive GC + GPU cache clearing\n",
    "- `check_memory_safe()`: Pre-operation memory checks\n",
    "- `chunked_operation()`: Process large data in chunks\n",
    "- `safe_operation()`: Retry decorator with OOM handling\n",
    "- Signal handlers: SIGINT/SIGTERM for graceful shutdown\n",
    "- Checkpoints: Resume from failures\n",
    "\n",
    "## Device Handling\n",
    "- Automatic GPU detection with CPU fallback\n",
    "- `device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')`\n",
    "- All tensors moved to device explicitly\n",
    "- GPU cache cleared aggressively after operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b8bf12",
   "metadata": {
    "papermill": {
     "duration": 0.0036,
     "end_time": "2025-11-19T09:52:49.579687",
     "exception": false,
     "start_time": "2025-11-19T09:52:49.576087",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b312417",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:52:49.586524Z",
     "iopub.status.busy": "2025-11-19T09:52:49.586364Z",
     "iopub.status.idle": "2025-11-19T09:52:50.411869Z",
     "shell.execute_reply": "2025-11-19T09:52:50.411421Z"
    },
    "papermill": {
     "duration": 0.829607,
     "end_time": "2025-11-19T09:52:50.412446",
     "exception": false,
     "start_time": "2025-11-19T09:52:49.582839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import gc\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "from typing import Dict, Optional\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import signal\n",
    "import atexit\n",
    "from functools import wraps\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c553d8f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:52:50.416321Z",
     "iopub.status.busy": "2025-11-19T09:52:50.416192Z",
     "iopub.status.idle": "2025-11-19T09:52:50.438202Z",
     "shell.execute_reply": "2025-11-19T09:52:50.437703Z"
    },
    "papermill": {
     "duration": 0.024629,
     "end_time": "2025-11-19T09:52:50.438890",
     "exception": false,
     "start_time": "2025-11-19T09:52:50.414261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STARTUP & REPRODUCIBILITY\n",
    "# =========================\n",
    "\n",
    "TOTAL_START_TIME = time.time()\n",
    "START_TIME_STR = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"MODEL_XGBOOST EXECUTION STARTED\")\n",
    "print(f\"Start Time: {START_TIME_STR}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4d77cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:52:50.446438Z",
     "iopub.status.busy": "2025-11-19T09:52:50.446265Z",
     "iopub.status.idle": "2025-11-19T09:52:50.449511Z",
     "shell.execute_reply": "2025-11-19T09:52:50.448945Z"
    },
    "papermill": {
     "duration": 0.008277,
     "end_time": "2025-11-19T09:52:50.450203",
     "exception": false,
     "start_time": "2025-11-19T09:52:50.441926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==============\n",
    "# PATH MANAGEMENT\n",
    "# ==============\n",
    "\n",
    "current = Path(os.getcwd())\n",
    "PROJECT_ROOT = current\n",
    "for _ in range(5):\n",
    "    if (PROJECT_ROOT / \"data\").exists():\n",
    "        break\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "else:\n",
    "    PROJECT_ROOT = current.parent.parent\n",
    "\n",
    "MODEL_READY_DIR = PROJECT_ROOT / \"data\" / \"model_ready\"\n",
    "MODEL_SAVE_DIR = PROJECT_ROOT / \"models\" / \"saved_models\"\n",
    "SUBMISSION_DIR = PROJECT_ROOT / \"data\" / \"submission_files\"\n",
    "MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SUBMISSION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "utils_path = PROJECT_ROOT / \"src\" / \"utils\"\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"MODEL_READY_DIR:\", MODEL_READY_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd62776",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:52:50.454481Z",
     "iopub.status.busy": "2025-11-19T09:52:50.454320Z",
     "iopub.status.idle": "2025-11-19T09:52:51.475489Z",
     "shell.execute_reply": "2025-11-19T09:52:51.474606Z"
    },
    "papermill": {
     "duration": 1.02595,
     "end_time": "2025-11-19T09:52:51.478002",
     "exception": false,
     "start_time": "2025-11-19T09:52:50.452052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# PCA UTILITY IMPORTS\n",
    "# =======================\n",
    "USE_TORCH_PCA = False\n",
    "if utils_path.exists():\n",
    "    sys.path.insert(0, str(utils_path))\n",
    "\n",
    "if USE_TORCH_PCA:\n",
    "    try:\n",
    "        from pca_utils import IncrementalTorchPCA\n",
    "        IncrementalPCA = IncrementalTorchPCA\n",
    "        IS_TORCH_PCA = True\n",
    "        print(\"âœ… Using PyTorch PCA (GPU-friendly)\")\n",
    "    except ImportError:\n",
    "        from sklearn.decomposition import IncrementalPCA\n",
    "        IS_TORCH_PCA = False\n",
    "        print(\"âš ï¸ Using sklearn IncrementalPCA (CPU only)\")\n",
    "else:\n",
    "    from sklearn.decomposition import IncrementalPCA\n",
    "    IS_TORCH_PCA = False\n",
    "    print(\"âœ… Using sklearn IncrementalPCA (memory-efficient)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095dafce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:52:51.486141Z",
     "iopub.status.busy": "2025-11-19T09:52:51.485925Z",
     "iopub.status.idle": "2025-11-19T09:52:51.901075Z",
     "shell.execute_reply": "2025-11-19T09:52:51.900453Z"
    },
    "papermill": {
     "duration": 0.418605,
     "end_time": "2025-11-19T09:52:51.901873",
     "exception": false,
     "start_time": "2025-11-19T09:52:51.483268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==========\n",
    "# ML LIBRARIES\n",
    "# ==========\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report, precision_recall_curve, roc_curve, confusion_matrix\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ==========\n",
    "# VISUALIZATION LIBRARIES\n",
    "# ==========\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Image\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e43a00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:52:51.906492Z",
     "iopub.status.busy": "2025-11-19T09:52:51.906319Z",
     "iopub.status.idle": "2025-11-19T09:52:51.911412Z",
     "shell.execute_reply": "2025-11-19T09:52:51.911042Z"
    },
    "papermill": {
     "duration": 0.008269,
     "end_time": "2025-11-19T09:52:51.912276",
     "exception": false,
     "start_time": "2025-11-19T09:52:51.904007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# MEMORY UTILITIES (FALLBACK DEFS)\n",
    "# ===============================\n",
    "try:\n",
    "    from model_training_utils import cleanup_memory, memory_usage, check_memory_safe\n",
    "    print(\"âœ… Memory utilities imported from shared module\")\n",
    "except ImportError:\n",
    "    def cleanup_memory():\n",
    "        \"\"\"Aggressive memory cleanup for both CPU and GPU.\"\"\"\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "            torch.cuda.ipc_collect()\n",
    "        gc.collect()\n",
    "    \n",
    "    def memory_usage():\n",
    "        \"\"\"Display current memory usage statistics.\"\"\"\n",
    "        try:\n",
    "            import psutil\n",
    "            process = psutil.Process(os.getpid())\n",
    "            mem_gb = process.memory_info().rss / 1024**3\n",
    "            print(f\"ðŸ’¾ Memory: {mem_gb:.2f} GB (RAM)\", end=\"\")\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_mem = torch.cuda.memory_allocated() / 1024**3\n",
    "                gpu_reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "                print(f\" | {gpu_mem:.2f}/{gpu_reserved:.2f} GB (GPU used/reserved)\")\n",
    "            else:\n",
    "                print()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def check_memory_safe(ram_threshold_gb=0.85, gpu_threshold=0.80):\n",
    "        \"\"\"Check if memory usage is safe for operations.\"\"\"\n",
    "        try:\n",
    "            import psutil\n",
    "            process = psutil.Process(os.getpid())\n",
    "            ram_gb = process.memory_info().rss / 1024**3\n",
    "            total_ram = psutil.virtual_memory().total / 1024**3\n",
    "            ram_ratio = ram_gb / total_ram if total_ram > 0 else 0\n",
    "            gpu_ratio = 0\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_used = torch.cuda.memory_allocated() / 1024**3\n",
    "                gpu_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "                gpu_ratio = gpu_used / gpu_total if gpu_total > 0 else 0\n",
    "            is_safe = ram_ratio < ram_threshold_gb and gpu_ratio < gpu_threshold\n",
    "            return is_safe, {\"ram_gb\": ram_gb, \"ram_ratio\": ram_ratio, \"gpu_ratio\": gpu_ratio}\n",
    "        except:\n",
    "            return True, {}\n",
    "    \n",
    "    print(\"âš ï¸ Using fallback memory utilities\")\n",
    "\n",
    "memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e829f99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:52:51.919238Z",
     "iopub.status.busy": "2025-11-19T09:52:51.919027Z",
     "iopub.status.idle": "2025-11-19T09:52:51.938393Z",
     "shell.execute_reply": "2025-11-19T09:52:51.937551Z"
    },
    "papermill": {
     "duration": 0.025076,
     "end_time": "2025-11-19T09:52:51.939542",
     "exception": false,
     "start_time": "2025-11-19T09:52:51.914466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# ROBUSTNESS/CHECKPOINT UTILITIES\n",
    "# ===============================\n",
    "\n",
    "_checkpoint_state = {\n",
    "    \"pca_complete\": False,\n",
    "    \"scaling_complete\": False,\n",
    "    \"cv_complete\": False,\n",
    "    \"final_model_trained\": False,\n",
    "    \"last_saved_checkpoint\": None,\n",
    "}\n",
    "\n",
    "def save_checkpoint(state_name: str, data: dict, checkpoint_dir: Path = None):\n",
    "    \"\"\"Save checkpoint to resume from failures.\"\"\"\n",
    "    if checkpoint_dir is None:\n",
    "        checkpoint_dir = PROJECT_ROOT / \"data\" / \"checkpoints\"\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    checkpoint_path = checkpoint_dir / f\"model_xgboost_checkpoint_{state_name}.pkl\"\n",
    "    try:\n",
    "        with open(checkpoint_path, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        _checkpoint_state[\"last_saved_checkpoint\"] = checkpoint_path\n",
    "        print(f\"âœ… Checkpoint saved: {checkpoint_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Failed to save checkpoint: {e}\")\n",
    "\n",
    "def load_checkpoint(state_name: str, checkpoint_dir: Path = None):\n",
    "    \"\"\"Load checkpoint to resume from failures.\"\"\"\n",
    "    if checkpoint_dir is None:\n",
    "        checkpoint_dir = PROJECT_ROOT / \"data\" / \"checkpoints\"\n",
    "    checkpoint_path = checkpoint_dir / f\"model_xgboost_checkpoint_{state_name}.pkl\"\n",
    "    if checkpoint_path.exists():\n",
    "        try:\n",
    "            with open(checkpoint_path, \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "            print(f\"âœ… Checkpoint loaded: {checkpoint_path}\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Failed to load checkpoint: {e}\")\n",
    "    return None\n",
    "\n",
    "def safe_operation(operation_name: str, max_retries: int = 3, checkpoint_on_success: bool = False):\n",
    "    \"\"\"Decorator for safe operations with retry and checkpoint support.\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    is_safe, mem_info = check_memory_safe(ram_threshold_gb=0.80, gpu_threshold=0.75)\n",
    "                    if not is_safe:\n",
    "                        cleanup_memory()\n",
    "                        if torch.cuda.is_available():\n",
    "                            torch.cuda.empty_cache()\n",
    "                        time.sleep(1)\n",
    "                    result = func(*args, **kwargs)\n",
    "                    cleanup_memory()\n",
    "                    if checkpoint_on_success:\n",
    "                        save_checkpoint(operation_name, {\"status\": \"complete\", \"result\": result})\n",
    "                    return result\n",
    "                except (MemoryError, RuntimeError) as e:\n",
    "                    error_msg = str(e).lower()\n",
    "                    if \"out of memory\" in error_msg or \"oom\" in error_msg:\n",
    "                        if attempt < max_retries - 1:\n",
    "                            cleanup_memory()\n",
    "                            if torch.cuda.is_available():\n",
    "                                torch.cuda.empty_cache()\n",
    "                            time.sleep(2)\n",
    "                            continue\n",
    "                        else:\n",
    "                            raise\n",
    "                    else:\n",
    "                        raise\n",
    "                except Exception as e:\n",
    "                    if attempt < max_retries - 1:\n",
    "                        cleanup_memory()\n",
    "                        time.sleep(1)\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise\n",
    "            return None\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "def chunked_operation(\n",
    "    data,\n",
    "    operation_func,\n",
    "    chunk_size: int = 10000,\n",
    "    progress_every: int = 10,\n",
    "    operation_name: str = \"operation\",\n",
    "):\n",
    "    \"\"\"Execute operation on data in chunks with progress tracking.\"\"\"\n",
    "    total_chunks = (len(data) + chunk_size - 1) // chunk_size\n",
    "    results = []\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        chunk_num = i // chunk_size + 1\n",
    "        chunk = data[i : i + chunk_size]\n",
    "        try:\n",
    "            is_safe, mem_info = check_memory_safe(ram_threshold_gb=0.85, gpu_threshold=0.80)\n",
    "            if not is_safe:\n",
    "                cleanup_memory()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                time.sleep(0.5)\n",
    "            chunk_result = operation_func(chunk)\n",
    "            results.append(chunk_result)\n",
    "            if chunk_num % progress_every == 0 or chunk_num == total_chunks:\n",
    "                print(f\"  Progress: {chunk_num}/{total_chunks} chunks ({chunk_num*100//total_chunks}%)\")\n",
    "            del chunk\n",
    "            if chunk_num % 5 == 0:\n",
    "                cleanup_memory()\n",
    "        except (MemoryError, RuntimeError) as e:\n",
    "            error_msg = str(e).lower()\n",
    "            if \"out of memory\" in error_msg or \"oom\" in error_msg:\n",
    "                cleanup_memory()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                smaller_chunk_size = max(1000, chunk_size // 2)\n",
    "                if smaller_chunk_size < chunk_size:\n",
    "                    return chunked_operation(\n",
    "                        data[i:],\n",
    "                        operation_func,\n",
    "                        chunk_size=smaller_chunk_size,\n",
    "                        progress_every=progress_every,\n",
    "                        operation_name=operation_name,\n",
    "                    )\n",
    "                else:\n",
    "                    raise\n",
    "            else:\n",
    "                raise\n",
    "    return results\n",
    "\n",
    "def emergency_cleanup():\n",
    "    \"\"\"Emergency cleanup on exit.\"\"\"\n",
    "    try:\n",
    "        cleanup_memory()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        print(\"âœ… Emergency cleanup completed\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "atexit.register(emergency_cleanup)\n",
    "\n",
    "def signal_handler(signum, frame):\n",
    "    \"\"\"Handle signals for graceful shutdown.\"\"\"\n",
    "    print(f\"âš ï¸ Received signal {signum}, saving checkpoint...\")\n",
    "    save_checkpoint(\"emergency\", {\"status\": \"signal_received\", \"signal\": signum})\n",
    "    emergency_cleanup()\n",
    "    raise KeyboardInterrupt\n",
    "\n",
    "try:\n",
    "    signal.signal(signal.SIGINT, signal_handler)\n",
    "    signal.signal(signal.SIGTERM, signal_handler)\n",
    "except:\n",
    "    pass\n",
    "print(\"âœ… Enhanced robustness utilities loaded\")\n",
    "\n",
    "def safe_prediction(predict_func, *args, **kwargs):\n",
    "    \"\"\"Execute prediction with chunked processing.\"\"\"\n",
    "    try:\n",
    "        is_safe, mem_info = check_memory_safe(ram_threshold_gb=0.85, gpu_threshold=0.80)\n",
    "        if not is_safe:\n",
    "            cleanup_memory()\n",
    "        if \"X\" in kwargs and len(kwargs[\"X\"]) > 50000:\n",
    "            X = kwargs[\"X\"]\n",
    "            chunk_size = 10000\n",
    "            predictions = []\n",
    "            for i in range(0, len(X), chunk_size):\n",
    "                chunk = X[i : i + chunk_size]\n",
    "                kwargs[\"X\"] = chunk\n",
    "                chunk_preds = predict_func(*args, **kwargs)\n",
    "                predictions.append(chunk_preds)\n",
    "                del chunk, chunk_preds\n",
    "                if i % (chunk_size * 5) == 0:\n",
    "                    cleanup_memory()\n",
    "            return np.concatenate(predictions)\n",
    "        else:\n",
    "            return predict_func(*args, **kwargs)\n",
    "    except (MemoryError, RuntimeError) as e:\n",
    "        error_msg = str(e).lower()\n",
    "        if \"out of memory\" in error_msg or \"oom\" in error_msg:\n",
    "            cleanup_memory()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            if \"X\" in kwargs:\n",
    "                X = kwargs[\"X\"]\n",
    "                chunk_size = 5000\n",
    "                predictions = []\n",
    "                for i in range(0, len(X), chunk_size):\n",
    "                    chunk = X[i : i + chunk_size]\n",
    "                    kwargs[\"X\"] = chunk\n",
    "                    chunk_preds = predict_func(*args, **kwargs)\n",
    "                    predictions.append(chunk_preds)\n",
    "                    del chunk, chunk_preds\n",
    "                    cleanup_memory()\n",
    "                return np.concatenate(predictions)\n",
    "            else:\n",
    "                raise\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "print(\"âœ… Training robustness wrappers loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654616df",
   "metadata": {
    "papermill": {
     "duration": 0.002296,
     "end_time": "2025-11-19T09:52:51.944654",
     "exception": false,
     "start_time": "2025-11-19T09:52:51.942358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Data Loading & Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f441b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:52:51.951220Z",
     "iopub.status.busy": "2025-11-19T09:52:51.951087Z",
     "iopub.status.idle": "2025-11-19T09:52:52.673339Z",
     "shell.execute_reply": "2025-11-19T09:52:52.672505Z"
    },
    "papermill": {
     "duration": 0.727788,
     "end_time": "2025-11-19T09:52:52.674937",
     "exception": false,
     "start_time": "2025-11-19T09:52:51.947149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_parquet_split(split: str) -> pl.DataFrame:\n",
    "    \"\"\"Load a model_ready parquet split with error handling.\"\"\"\n",
    "    try:\n",
    "        path = MODEL_READY_DIR / f\"{split}_model_ready.parquet\"\n",
    "        if not path.exists():\n",
    "            alt = MODEL_READY_DIR / f\"{split}_model_ready_reduced.parquet\"\n",
    "            if alt.exists():\n",
    "                path = alt\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Could not find {split} data\")\n",
    "        print(f\"Loading {split} from {path}\")\n",
    "        return pl.read_parquet(path)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading {split}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def split_features_reg_and_all_emb(df: pl.DataFrame):\n",
    "    \"\"\"Split features into regular and embedding families.\"\"\"\n",
    "    cols = df.columns\n",
    "    dtypes = df.dtypes\n",
    "    label = df[\"label\"].to_numpy() if \"label\" in cols else None\n",
    "\n",
    "    reg_cols = []\n",
    "    EMBEDDING_FAMILY_PREFIXES = [\"sent_transformer_\", \"scibert_\", \"specter_\", \"specter2_\", \"ner_\"]\n",
    "    emb_family_to_cols = {p: [] for p in EMBEDDING_FAMILY_PREFIXES}\n",
    "\n",
    "    NUMERIC_DTYPES = {\n",
    "        pl.Int8,\n",
    "        pl.Int16,\n",
    "        pl.Int32,\n",
    "        pl.Int64,\n",
    "        pl.UInt8,\n",
    "        pl.UInt16,\n",
    "        pl.UInt32,\n",
    "        pl.UInt64,\n",
    "        pl.Float32,\n",
    "        pl.Float64,\n",
    "    }\n",
    "\n",
    "    for c, dt in zip(cols, dtypes):\n",
    "        if c in (\"id\", \"label\"):\n",
    "            continue\n",
    "        matched = False\n",
    "        for p in EMBEDDING_FAMILY_PREFIXES:\n",
    "            if c.startswith(p):\n",
    "                emb_family_to_cols[p].append(c)\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched and dt in NUMERIC_DTYPES:\n",
    "            reg_cols.append(c)\n",
    "\n",
    "    X_reg = df.select(reg_cols).to_numpy() if reg_cols else None\n",
    "    X_emb_families = {}\n",
    "    for p, clist in emb_family_to_cols.items():\n",
    "        if clist:\n",
    "            X_emb_families[p] = df.select(clist).to_numpy()\n",
    "\n",
    "    return X_reg, X_emb_families, label, reg_cols, emb_family_to_cols\n",
    "\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PHASE 1: Data Loading\")\n",
    "    print(\"=\" * 80)\n",
    "    phase_start = time.time()\n",
    "    \n",
    "    train_df = load_parquet_split(\"train\")\n",
    "    val_df = load_parquet_split(\"val\")\n",
    "\n",
    "    X_reg_train, X_emb_train_fams, y_train, reg_cols, emb_family_to_cols = (\n",
    "        split_features_reg_and_all_emb(train_df)\n",
    "    )\n",
    "    X_reg_val, X_emb_val_fams, y_val, _, _ = split_features_reg_and_all_emb(val_df)\n",
    "\n",
    "    phase_time = time.time() - phase_start\n",
    "    print(f\"\\nðŸ“Š Data Summary:\")\n",
    "    print(f\"  Regular features: {len(reg_cols)}\")\n",
    "    for fam, arr in X_emb_train_fams.items():\n",
    "        print(f\"  Embedding {fam}: {arr.shape[1]} dims\")\n",
    "    print(\n",
    "        f\"  Train samples: {len(y_train)}, Positive: {y_train.sum()}, Negative: {(y_train==0).sum()}\"\n",
    "    )\n",
    "    print(f\"  Val samples: {len(y_val)}, Positive: {y_val.sum()}, Negative: {(y_val==0).sum()}\")\n",
    "    print(f\"\\nâ±ï¸  Data Loading Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\")\n",
    "\n",
    "    del train_df, val_df\n",
    "    cleanup_memory()\n",
    "    memory_usage()\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ff764f",
   "metadata": {
    "papermill": {
     "duration": 0.00177,
     "end_time": "2025-11-19T09:52:52.678971",
     "exception": false,
     "start_time": "2025-11-19T09:52:52.677201",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Feature Preprocessing: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6bb3ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:52:52.684655Z",
     "iopub.status.busy": "2025-11-19T09:52:52.684456Z",
     "iopub.status.idle": "2025-11-19T09:52:52.687038Z",
     "shell.execute_reply": "2025-11-19T09:52:52.686380Z"
    },
    "papermill": {
     "duration": 0.006053,
     "end_time": "2025-11-19T09:52:52.687731",
     "exception": false,
     "start_time": "2025-11-19T09:52:52.681678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PCA OPTION: Set to False to use all embeddings without PCA compression\n",
    "USE_PCA = False  # Set to False to skip PCA and use all features (like PyTorch MLP)\n",
    "\n",
    "# PCA compression per embedding family (only used if USE_PCA=True)\n",
    "PCA_COMPONENTS_PER_FAMILY = {\n",
    "    \"sent_transformer_\": 32,\n",
    "    \"scibert_\": 32,\n",
    "    \"specter_\": 32,\n",
    "    \"specter2_\": 32,\n",
    "    \"ner_\": 16,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c98fae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:52:52.692404Z",
     "iopub.status.busy": "2025-11-19T09:52:52.692275Z",
     "iopub.status.idle": "2025-11-19T09:53:01.417306Z",
     "shell.execute_reply": "2025-11-19T09:53:01.416822Z"
    },
    "papermill": {
     "duration": 8.731428,
     "end_time": "2025-11-19T09:53:01.421097",
     "exception": false,
     "start_time": "2025-11-19T09:52:52.689669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@safe_operation(\"pca\", max_retries=3, checkpoint_on_success=True)\n",
    "def apply_pca_to_embeddings(\n",
    "    X_emb_fams: Dict[str, np.ndarray], fit_on_train: bool = True, pca_models: Optional[Dict] = None\n",
    "):\n",
    "    \"\"\"Apply IncrementalPCA to each embedding family (GPU-friendly, OOM-resistant).\"\"\"\n",
    "    X_emb_pca_list = []\n",
    "    new_pca_models = {}\n",
    "\n",
    "    cleanup_memory()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # Memory safety check before PCA\n",
    "    is_safe, mem_info = check_memory_safe(ram_threshold_gb=0.75, gpu_threshold=0.70)\n",
    "    if not is_safe:\n",
    "        print(f\"âš ï¸ Memory usage high before PCA: {mem_info}\")\n",
    "        cleanup_memory()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    for fam, X_emb in X_emb_fams.items():\n",
    "        n_components = PCA_COMPONENTS_PER_FAMILY.get(fam, 32)\n",
    "        try:\n",
    "            cleanup_memory()\n",
    "\n",
    "            # Fit PCA model if training or no model given\n",
    "            if fit_on_train or pca_models is None:\n",
    "                ipca_args = {\n",
    "                    \"n_components\": min(n_components, X_emb.shape[1]),\n",
    "                    \"batch_size\": 2000\n",
    "                }\n",
    "                if IS_TORCH_PCA:\n",
    "                    ipca_args[\"device\"] = device\n",
    "                ipca = IncrementalPCA(**ipca_args)\n",
    "\n",
    "                # Fit PCA on subset for big data\n",
    "                max_pca_rows = int(X_emb.shape[0] * 0.3)\n",
    "                if X_emb.shape[0] > max_pca_rows:\n",
    "                    print(f\"  Fitting PCA on subset ({max_pca_rows}/{X_emb.shape[0]} samples) for {fam}\")\n",
    "                    idx = np.random.choice(X_emb.shape[0], size=max_pca_rows, replace=False)\n",
    "                    X_emb_subset = X_emb[idx].copy()\n",
    "                    del idx\n",
    "                    cleanup_memory()\n",
    "                    ipca.fit(X_emb_subset)\n",
    "                    del X_emb_subset\n",
    "                    cleanup_memory()\n",
    "                else:\n",
    "                    X_emb_copy = X_emb.copy() if not X_emb.flags[\"OWNDATA\"] else X_emb\n",
    "                    ipca.fit(X_emb_copy)\n",
    "                    if X_emb_copy is not X_emb:\n",
    "                        del X_emb_copy\n",
    "                    cleanup_memory()\n",
    "\n",
    "                new_pca_models[fam] = ipca\n",
    "            else:\n",
    "                ipca = pca_models[fam]\n",
    "\n",
    "            # Transform in chunks for memory efficiency\n",
    "            chunk_size = 2000\n",
    "            if X_emb.shape[0] > chunk_size:\n",
    "                X_emb_pca_chunks = []\n",
    "                for i in range(0, X_emb.shape[0], chunk_size):\n",
    "                    chunk = X_emb[i : i + chunk_size].copy()\n",
    "                    chunk_pca = ipca.transform(chunk)\n",
    "                    X_emb_pca_chunks.append(chunk_pca)\n",
    "                    del chunk, chunk_pca\n",
    "                    cleanup_memory()\n",
    "                    if i % (chunk_size * 5) == 0 and torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                X_emb_pca = np.vstack(X_emb_pca_chunks)\n",
    "                del X_emb_pca_chunks\n",
    "                cleanup_memory()\n",
    "            else:\n",
    "                X_emb_copy = X_emb.copy() if not X_emb.flags[\"OWNDATA\"] else X_emb\n",
    "                X_emb_pca = ipca.transform(X_emb_copy)\n",
    "                if X_emb_copy is not X_emb:\n",
    "                    del X_emb_copy\n",
    "                cleanup_memory()\n",
    "\n",
    "            X_emb_pca_list.append(X_emb_pca)\n",
    "            del X_emb_pca\n",
    "            cleanup_memory()\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower() or \"OOM\" in str(e).upper():\n",
    "                print(f\"âŒ OOM error processing {fam}\")\n",
    "                cleanup_memory()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    torch.cuda.synchronize()\n",
    "                raise\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    cleanup_memory()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    X_emb_combined = np.hstack(X_emb_pca_list) if X_emb_pca_list else None\n",
    "    return X_emb_combined, new_pca_models if fit_on_train else pca_models\n",
    "\n",
    "# Apply PCA to embeddings (if enabled) or use embeddings directly\n",
    "try:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    if USE_PCA:\n",
    "        print(\"PHASE 2: PCA Preprocessing\")\n",
    "    else:\n",
    "        print(\"PHASE 2: Feature Preprocessing (No PCA)\")\n",
    "    print(\"=\" * 80)\n",
    "    phase_start = time.time()\n",
    "    \n",
    "    cleanup_memory()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    if USE_PCA:\n",
    "        print(\"\\nðŸ“Š Applying IncrementalPCA to embedding families...\")\n",
    "        for fam in X_emb_train_fams.keys():\n",
    "            n_comp = PCA_COMPONENTS_PER_FAMILY.get(fam, 32)\n",
    "            print(f\"  {fam}: {X_emb_train_fams[fam].shape[1]} dims â†’ {n_comp} components\")\n",
    "\n",
    "        # Train PCA models on train\n",
    "        X_emb_train_pca, pca_models_train = apply_pca_to_embeddings(\n",
    "            X_emb_train_fams, fit_on_train=True\n",
    "        )\n",
    "\n",
    "        cleanup_memory()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Transform validation using trained PCA models\n",
    "        X_emb_val_pca, _ = apply_pca_to_embeddings(\n",
    "            X_emb_val_fams, fit_on_train=False, pca_models=pca_models_train\n",
    "        )\n",
    "\n",
    "        phase_time = time.time() - phase_start\n",
    "        print(f\"\\nðŸ“Š After IncrementalPCA:\")\n",
    "        print(f\"  Train embeddings: {X_emb_train_pca.shape}\")\n",
    "        print(f\"  Val embeddings: {X_emb_val_pca.shape}\")\n",
    "\n",
    "        # Combine regular + embedding features\n",
    "        if X_reg_train is not None:\n",
    "            X_train = np.hstack([X_reg_train, X_emb_train_pca])\n",
    "            X_val = np.hstack([X_reg_val, X_emb_val_pca])\n",
    "        else:\n",
    "            X_train = X_emb_train_pca\n",
    "            X_val = X_emb_val_pca\n",
    "        \n",
    "        # Store PCA models for later (model saving)\n",
    "        pca_models = pca_models_train\n",
    "        \n",
    "        print(f\"  Combined train: {X_train.shape}\")\n",
    "        print(f\"  Combined val: {X_val.shape}\")\n",
    "        print(f\"\\nâ±ï¸  PCA Preprocessing Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\")\n",
    "\n",
    "        del X_reg_train, X_reg_val, X_emb_train_fams, X_emb_val_fams, X_emb_train_pca, X_emb_val_pca\n",
    "    else:\n",
    "        print(\"\\nðŸ“Š Using all embeddings without PCA compression...\")\n",
    "        for fam in X_emb_train_fams.keys():\n",
    "            print(f\"  {fam}: {X_emb_train_fams[fam].shape[1]} dims (no compression)\")\n",
    "\n",
    "        # Combine all embeddings directly (no PCA)\n",
    "        X_emb_train_list = [X_emb_train_fams[fam] for fam in X_emb_train_fams.keys()]\n",
    "        X_emb_val_list = [X_emb_val_fams[fam] for fam in X_emb_val_fams.keys()]\n",
    "        \n",
    "        X_emb_train_combined = np.hstack(X_emb_train_list) if X_emb_train_list else None\n",
    "        X_emb_val_combined = np.hstack(X_emb_val_list) if X_emb_val_list else None\n",
    "\n",
    "        phase_time = time.time() - phase_start\n",
    "        print(f\"\\nðŸ“Š After combining embeddings:\")\n",
    "        print(f\"  Train embeddings: {X_emb_train_combined.shape}\")\n",
    "        print(f\"  Val embeddings: {X_emb_val_combined.shape}\")\n",
    "\n",
    "        # Combine regular + embedding features\n",
    "        if X_reg_train is not None:\n",
    "            X_train = np.hstack([X_reg_train, X_emb_train_combined])\n",
    "            X_val = np.hstack([X_reg_val, X_emb_val_combined])\n",
    "        else:\n",
    "            X_train = X_emb_train_combined\n",
    "            X_val = X_emb_val_combined\n",
    "        \n",
    "        # No PCA models to store\n",
    "        pca_models = {}\n",
    "        \n",
    "        print(f\"  Combined train: {X_train.shape}\")\n",
    "        print(f\"  Combined val: {X_val.shape}\")\n",
    "        print(f\"\\nâ±ï¸  Feature Preprocessing Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\")\n",
    "\n",
    "        del X_reg_train, X_reg_val, X_emb_train_fams, X_emb_val_fams, X_emb_train_list, X_emb_val_list, X_emb_train_combined, X_emb_val_combined\n",
    "    \n",
    "    cleanup_memory()\n",
    "    memory_usage()\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error in preprocessing: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32f167b",
   "metadata": {
    "papermill": {
     "duration": 0.004673,
     "end_time": "2025-11-19T09:53:01.432838",
     "exception": false,
     "start_time": "2025-11-19T09:53:01.428165",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Class Imbalance Handling: SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb70b37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:53:01.442578Z",
     "iopub.status.busy": "2025-11-19T09:53:01.442146Z",
     "iopub.status.idle": "2025-11-19T09:53:02.420974Z",
     "shell.execute_reply": "2025-11-19T09:53:02.420619Z"
    },
    "papermill": {
     "duration": 0.985196,
     "end_time": "2025-11-19T09:53:02.421600",
     "exception": false,
     "start_time": "2025-11-19T09:53:01.436404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Incremental SMOTE configuration for OOM-resistant processing\n",
    "# We always apply SMOTE - never skip it, even for large datasets\n",
    "USE_INCREMENTAL_SMOTE = True  # Always use chunked processing for better memory management\n",
    "SMOTE_CHUNK_SIZE = 10_000  # Process in chunks of 10k samples for optimal memory usage\n",
    "SMOTE_SAMPLE_RATIO = 0.5  # Apply SMOTE to 50% of the data to balance effectiveness and memory\n",
    "\n",
    "# SMOTE method selection: \"enn\" (default) or \"tomek\"\n",
    "# ENN (Edited Nearest Neighbours) is more aggressive and better for noisy data\n",
    "# Tomek links is less aggressive and preserves more original samples\n",
    "SMOTE_METHOD = \"enn\"  # Default: ENN for better noise handling\n",
    "\n",
    "method_name = \"SMOTEENN\" if SMOTE_METHOD == \"enn\" else \"SMOTETomek\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"PHASE 3: {method_name} Resampling (OOM-Resistant)\")\n",
    "print(\"=\" * 80)\n",
    "phase_start = time.time()\n",
    "\n",
    "print(\"\\nðŸ“Š Checking class imbalance and applying incremental SMOTE resampling...\")\n",
    "print(f\"  Before: {len(X_train)} samples, Positive: {y_train.sum()}, Negative: {(y_train == 0).sum()}\")\n",
    "print(f\"  Imbalance ratio: {(y_train == 0).sum() / max(y_train.sum(), 1):.2f}:1\")\n",
    "\n",
    "# Check memory before SMOTE and clean up if needed\n",
    "is_safe, mem_info = check_memory_safe(ram_threshold_gb=0.65, gpu_threshold=0.65)\n",
    "if not is_safe:\n",
    "    print(f\"  âš ï¸  Memory usage high before SMOTE: {mem_info}\")\n",
    "    print(\"  Performing aggressive cleanup...\")\n",
    "    cleanup_memory()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# We always apply SMOTE - no skipping allowed\n",
    "USE_CLASS_WEIGHT = False\n",
    "\n",
    "# Sample a portion of data for SMOTE (stratified to preserve class distribution)\n",
    "# This helps manage memory while still applying SMOTE to a representative sample\n",
    "print(f\"\\n  Sampling {SMOTE_SAMPLE_RATIO*100:.0f}% of data for {method_name} resampling...\")\n",
    "X_train_smote, X_train_keep, y_train_smote, y_train_keep = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=1 - SMOTE_SAMPLE_RATIO,\n",
    "    stratify=y_train,\n",
    "    random_state=42\n",
    ")\n",
    "print(f\"  SMOTE sample: {len(X_train_smote)} samples (pos: {y_train_smote.sum()}, neg: {(y_train_smote==0).sum()})\")\n",
    "print(f\"  Keep as-is: {len(X_train_keep)} samples (pos: {y_train_keep.sum()}, neg: {(y_train_keep==0).sum()})\")\n",
    "\n",
    "# Cleanup original data to free memory\n",
    "del X_train, y_train\n",
    "cleanup_memory()\n",
    "\n",
    "# Always use incremental SMOTE for better memory management\n",
    "if USE_INCREMENTAL_SMOTE and len(X_train_smote) > SMOTE_CHUNK_SIZE:\n",
    "    print(f\"\\n  Using incremental {method_name} with chunk size: {SMOTE_CHUNK_SIZE:,} samples\")\n",
    "    \n",
    "    # Determine sampling strategy based on dataset size\n",
    "    # Smaller strategy values mean less aggressive resampling, which uses less memory\n",
    "    if len(X_train_smote) > 100_000:\n",
    "        sampling_strategy = 0.15  # Moderate resampling for very large datasets\n",
    "    elif len(X_train_smote) > 50_000:\n",
    "        sampling_strategy = 0.25  # Balanced resampling for large datasets\n",
    "    else:\n",
    "        sampling_strategy = 0.35  # More aggressive resampling for smaller datasets\n",
    "    \n",
    "    print(f\"  Sampling strategy: {sampling_strategy} (target minority/majority ratio)\")\n",
    "    \n",
    "    # Shuffle data to ensure good class mixing across chunks\n",
    "    # This helps each chunk have a representative sample of both classes\n",
    "    print(f\"  Shuffling data to ensure good class distribution across chunks...\")\n",
    "    X_train_shuffled, y_train_shuffled = shuffle(X_train_smote, y_train_smote, random_state=42)\n",
    "    del X_train_smote, y_train_smote\n",
    "    cleanup_memory()\n",
    "    \n",
    "    n_chunks = (len(X_train_shuffled) + SMOTE_CHUNK_SIZE - 1) // SMOTE_CHUNK_SIZE\n",
    "    print(f\"  Processing {n_chunks} chunks of up to {SMOTE_CHUNK_SIZE:,} samples each...\")\n",
    "    \n",
    "    X_resampled_chunks = []\n",
    "    y_resampled_chunks = []\n",
    "    \n",
    "    # Process each chunk sequentially with robust error handling\n",
    "    for chunk_idx in range(n_chunks):\n",
    "        start_idx = chunk_idx * SMOTE_CHUNK_SIZE\n",
    "        end_idx = min(start_idx + SMOTE_CHUNK_SIZE, len(X_train_shuffled))\n",
    "        \n",
    "        X_chunk = X_train_shuffled[start_idx:end_idx].copy()\n",
    "        y_chunk = y_train_shuffled[start_idx:end_idx].copy()\n",
    "        \n",
    "        # Only apply SMOTE if chunk has both classes (required for resampling)\n",
    "        if y_chunk.sum() > 0 and (y_chunk == 0).sum() > 0:\n",
    "            print(f\"    Chunk {chunk_idx + 1}/{n_chunks}: {len(X_chunk)} samples (pos: {y_chunk.sum()}, neg: {(y_chunk==0).sum()})\")\n",
    "            \n",
    "            # Create SMOTE instance based on selected method\n",
    "            # Using n_jobs=1 to avoid memory spikes from parallel processing\n",
    "            if SMOTE_METHOD == \"enn\":\n",
    "                smote = SMOTEENN(\n",
    "                    random_state=42 + chunk_idx,  # Different seed per chunk for diversity\n",
    "                    sampling_strategy=sampling_strategy,\n",
    "                    n_jobs=1  # Single job to minimize memory usage\n",
    "                )\n",
    "            else:  # tovek\n",
    "                smote = SMOTETomek(\n",
    "                    random_state=42 + chunk_idx,\n",
    "                    sampling_strategy=sampling_strategy,\n",
    "                    n_jobs=1\n",
    "                )\n",
    "            \n",
    "            # Apply SMOTE with retry logic for OOM resilience\n",
    "            max_retries = 3\n",
    "            retry_chunk_size = len(X_chunk)\n",
    "            chunk_resampled = False\n",
    "            \n",
    "            for retry in range(max_retries):\n",
    "                try:\n",
    "                    # Check memory before SMOTE\n",
    "                    is_safe, _ = check_memory_safe(ram_threshold_gb=0.75, gpu_threshold=0.75)\n",
    "                    if not is_safe:\n",
    "                        cleanup_memory()\n",
    "                        if torch.cuda.is_available():\n",
    "                            torch.cuda.empty_cache()\n",
    "                        time.sleep(0.5)  # Brief pause to let memory settle\n",
    "                    \n",
    "                    # Apply SMOTE resampling\n",
    "                    X_chunk_resampled, y_chunk_resampled = smote.fit_resample(X_chunk, y_chunk)\n",
    "                    X_resampled_chunks.append(X_chunk_resampled)\n",
    "                    y_resampled_chunks.append(y_chunk_resampled)\n",
    "                    \n",
    "                    print(f\"      âœ“ Resampled to {len(X_chunk_resampled)} samples\")\n",
    "                    chunk_resampled = True\n",
    "                    \n",
    "                    # Cleanup immediately after successful resampling\n",
    "                    del smote, X_chunk_resampled, y_chunk_resampled\n",
    "                    cleanup_memory()\n",
    "                    break\n",
    "                    \n",
    "                except (MemoryError, RuntimeError) as e:\n",
    "                    error_msg = str(e).lower()\n",
    "                    if (\"out of memory\" in error_msg or \"oom\" in error_msg) and retry < max_retries - 1:\n",
    "                        # If OOM, try with a smaller subset of the chunk\n",
    "                        print(f\"      âš ï¸  Memory error on retry {retry + 1}, trying smaller subset...\")\n",
    "                        cleanup_memory()\n",
    "                        if torch.cuda.is_available():\n",
    "                            torch.cuda.empty_cache()\n",
    "                        \n",
    "                        # Use a smaller subset for next retry\n",
    "                        subset_size = retry_chunk_size // 2\n",
    "                        if subset_size < 1000:  # Minimum viable chunk size\n",
    "                            print(f\"      âš ï¸  Chunk too small, using original chunk\")\n",
    "                            X_resampled_chunks.append(X_chunk)\n",
    "                            y_resampled_chunks.append(y_chunk)\n",
    "                            chunk_resampled = True\n",
    "                            break\n",
    "                        \n",
    "                        # Try with smaller subset\n",
    "                        idx_subset = np.random.choice(len(X_chunk), size=subset_size, replace=False)\n",
    "                        X_chunk = X_chunk[idx_subset]\n",
    "                        y_chunk = y_chunk[idx_subset]\n",
    "                        retry_chunk_size = subset_size\n",
    "                        time.sleep(1)  # Wait before retry\n",
    "                    else:\n",
    "                        # Final retry failed or non-OOM error - use original chunk\n",
    "                        print(f\"      âš ï¸  SMOTE failed after {retry + 1} attempts, using original chunk\")\n",
    "                        X_resampled_chunks.append(X_chunk)\n",
    "                        y_resampled_chunks.append(y_chunk)\n",
    "                        chunk_resampled = True\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    # Other errors - use original chunk and continue\n",
    "                    print(f\"      âš ï¸  Unexpected error: {e}, using original chunk\")\n",
    "                    X_resampled_chunks.append(X_chunk)\n",
    "                    y_resampled_chunks.append(y_chunk)\n",
    "                    chunk_resampled = True\n",
    "                    break\n",
    "            \n",
    "            # Ensure chunk was processed\n",
    "            if not chunk_resampled:\n",
    "                X_resampled_chunks.append(X_chunk)\n",
    "                y_resampled_chunks.append(y_chunk)\n",
    "            \n",
    "            # Cleanup chunk data\n",
    "            del X_chunk, y_chunk\n",
    "            cleanup_memory()\n",
    "            \n",
    "        else:\n",
    "            # Chunk has only one class - keep as is (SMOTE requires both classes)\n",
    "            print(f\"    Chunk {chunk_idx + 1}/{n_chunks}: {len(X_chunk)} samples (single class, keeping as-is)\")\n",
    "            X_resampled_chunks.append(X_chunk)\n",
    "            y_resampled_chunks.append(y_chunk)\n",
    "            del X_chunk, y_chunk\n",
    "        \n",
    "        # Periodic cleanup every few chunks to prevent memory accumulation\n",
    "        if (chunk_idx + 1) % 3 == 0:\n",
    "            cleanup_memory()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # Combine all resampled chunks into final training set\n",
    "    print(f\"\\n  Combining {len(X_resampled_chunks)} resampled chunks...\")\n",
    "    X_train_smote_resampled = np.vstack(X_resampled_chunks)\n",
    "    y_train_smote_resampled = np.hstack(y_resampled_chunks)\n",
    "    \n",
    "    del X_resampled_chunks, y_resampled_chunks, X_train_shuffled, y_train_shuffled\n",
    "    cleanup_memory()\n",
    "    \n",
    "    # Combine resampled SMOTE sample with the kept portion\n",
    "    print(f\"  Combining resampled data with kept portion...\")\n",
    "    X_train = np.vstack([X_train_smote_resampled, X_train_keep])\n",
    "    y_train = np.hstack([y_train_smote_resampled, y_train_keep])\n",
    "    \n",
    "    del X_train_smote_resampled, y_train_smote_resampled, X_train_keep, y_train_keep\n",
    "    cleanup_memory()\n",
    "    \n",
    "    phase_time = time.time() - phase_start\n",
    "    print(f\"\\n  âœ… After resampling: {len(X_train)} samples\")\n",
    "    print(f\"     Positive: {y_train.sum()}, Negative: {(y_train == 0).sum()}\")\n",
    "    print(f\"     Balance ratio: {(y_train == 0).sum() / max(y_train.sum(), 1):.2f}:1\")\n",
    "    print(f\"\\nâ±ï¸  Incremental {method_name} Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\")\n",
    "    \n",
    "else:\n",
    "    # For smaller datasets that fit in memory, use standard SMOTE\n",
    "    # Still chunked if needed, but can process larger chunks\n",
    "    print(f\"\\n  Using standard {method_name} (dataset fits in memory)\")\n",
    "    \n",
    "    # Determine parameters based on dataset size\n",
    "    if len(X_train_smote) > 50_000:\n",
    "        sampling_strategy = 0.25\n",
    "        n_jobs_smote = 1  # Single job for memory safety\n",
    "    else:\n",
    "        sampling_strategy = 0.35\n",
    "        n_jobs_smote = 1  # Still single job to be safe\n",
    "    \n",
    "    # Create SMOTE instance\n",
    "    if SMOTE_METHOD == \"enn\":\n",
    "        smote = SMOTEENN(random_state=42, sampling_strategy=sampling_strategy, n_jobs=n_jobs_smote)\n",
    "    else:  # tovek\n",
    "        smote = SMOTETomek(random_state=42, sampling_strategy=sampling_strategy, n_jobs=n_jobs_smote)\n",
    "    \n",
    "    print(f\"  Fitting {method_name} on sample...\")\n",
    "    cleanup_memory()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Apply SMOTE with error handling - fallback to incremental if needed\n",
    "    try:\n",
    "        X_train_smote_resampled, y_train_smote_resampled = smote.fit_resample(X_train_smote, y_train_smote)\n",
    "    except (MemoryError, RuntimeError) as e:\n",
    "        error_msg = str(e).lower()\n",
    "        if \"out of memory\" in error_msg or \"oom\" in error_msg:\n",
    "            print(f\"  âš ï¸  Memory error, falling back to incremental processing...\")\n",
    "            # Fall back to incremental approach with smaller chunks\n",
    "            X_train_shuffled, y_train_shuffled = shuffle(X_train_smote, y_train_smote, random_state=42)\n",
    "            del X_train_smote, y_train_smote\n",
    "            cleanup_memory()\n",
    "            \n",
    "            # Process in smaller chunks\n",
    "            n_chunks = (len(X_train_shuffled) + SMOTE_CHUNK_SIZE - 1) // SMOTE_CHUNK_SIZE\n",
    "            X_resampled_chunks = []\n",
    "            y_resampled_chunks = []\n",
    "            \n",
    "            for chunk_idx in range(n_chunks):\n",
    "                start_idx = chunk_idx * SMOTE_CHUNK_SIZE\n",
    "                end_idx = min(start_idx + SMOTE_CHUNK_SIZE, len(X_train_shuffled))\n",
    "                X_chunk = X_train_shuffled[start_idx:end_idx]\n",
    "                y_chunk = y_train_shuffled[start_idx:end_idx]\n",
    "                \n",
    "                if y_chunk.sum() > 0 and (y_chunk == 0).sum() > 0:\n",
    "                    if SMOTE_METHOD == \"enn\":\n",
    "                        smote_chunk = SMOTEENN(random_state=42 + chunk_idx, sampling_strategy=sampling_strategy, n_jobs=1)\n",
    "                    else:\n",
    "                        smote_chunk = SMOTETomek(random_state=42 + chunk_idx, sampling_strategy=sampling_strategy, n_jobs=1)\n",
    "                    \n",
    "                    try:\n",
    "                        X_chunk_resampled, y_chunk_resampled = smote_chunk.fit_resample(X_chunk, y_chunk)\n",
    "                        X_resampled_chunks.append(X_chunk_resampled)\n",
    "                        y_resampled_chunks.append(y_chunk_resampled)\n",
    "                    except:\n",
    "                        X_resampled_chunks.append(X_chunk)\n",
    "                        y_resampled_chunks.append(y_chunk)\n",
    "                    finally:\n",
    "                        del X_chunk, y_chunk\n",
    "                        cleanup_memory()\n",
    "                else:\n",
    "                    X_resampled_chunks.append(X_chunk)\n",
    "                    y_resampled_chunks.append(y_chunk)\n",
    "            \n",
    "            X_train_smote_resampled = np.vstack(X_resampled_chunks)\n",
    "            y_train_smote_resampled = np.hstack(y_resampled_chunks)\n",
    "            del X_resampled_chunks, y_resampled_chunks, X_train_shuffled, y_train_shuffled\n",
    "\n",
    "    del smote, X_train_smote, y_train_smote\n",
    "    cleanup_memory()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Combine resampled SMOTE sample with the kept portion\n",
    "    X_train = np.vstack([X_train_smote_resampled, X_train_keep])\n",
    "    y_train = np.hstack([y_train_smote_resampled, y_train_keep])\n",
    "    \n",
    "    del X_train_smote_resampled, y_train_smote_resampled, X_train_keep, y_train_keep\n",
    "    cleanup_memory()\n",
    "\n",
    "    phase_time = time.time() - phase_start\n",
    "    print(f\"\\n  âœ… After resampling: {len(X_train)} samples\")\n",
    "    print(f\"     Positive: {y_train.sum()}, Negative: {(y_train == 0).sum()}\")\n",
    "    print(f\"     Balance ratio: {(y_train == 0).sum() / max(y_train.sum(), 1):.2f}:1\")\n",
    "    print(f\"\\nâ±ï¸  {method_name} Resampling Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\")\n",
    "\n",
    "# Final cleanup\n",
    "cleanup_memory()\n",
    "memory_usage()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9ac6c7",
   "metadata": {
    "papermill": {
     "duration": 0.002111,
     "end_time": "2025-11-19T09:53:02.425755",
     "exception": false,
     "start_time": "2025-11-19T09:53:02.423644",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b3ad4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:53:02.430182Z",
     "iopub.status.busy": "2025-11-19T09:53:02.430071Z",
     "iopub.status.idle": "2025-11-19T09:53:02.577129Z",
     "shell.execute_reply": "2025-11-19T09:53:02.576780Z"
    },
    "papermill": {
     "duration": 0.150073,
     "end_time": "2025-11-19T09:53:02.577718",
     "exception": false,
     "start_time": "2025-11-19T09:53:02.427645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 4: Feature Scaling\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "phase_start = time.time()\n",
    "print(\"\\nðŸ“Š Applying Feature Scaling to combined features...\")\n",
    "\n",
    "# Store raw (unscaled) data for CV Pipeline (scaler will be fit per fold)\n",
    "X_train_raw = X_train.copy()\n",
    "X_val_raw = X_val.copy()\n",
    "y_train_raw = y_train.copy()\n",
    "y_val_raw = y_val.copy()\n",
    "cleanup_memory()\n",
    "\n",
    "# Use StandardScaler (RobustScaler doesn't support partial_fit)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Process in chunks for memory efficiency\n",
    "CHUNK_SIZE = 20000\n",
    "\n",
    "if X_train.shape[0] > CHUNK_SIZE:\n",
    "    print(f\"  Fitting scaler on sample ({min(CHUNK_SIZE, X_train.shape[0])} samples)...\")\n",
    "    sample_indices = np.random.choice(X_train.shape[0], size=min(CHUNK_SIZE, X_train.shape[0]), replace=False)\n",
    "    scaler.fit(X_train[sample_indices])\n",
    "    del sample_indices\n",
    "    cleanup_memory()\n",
    "\n",
    "    # Transform train in chunks\n",
    "    print(f\"  Transforming train data in chunks (size={CHUNK_SIZE})...\")\n",
    "    X_train_chunks = []\n",
    "    for i in range(0, X_train.shape[0], CHUNK_SIZE):\n",
    "        chunk = scaler.transform(X_train[i:i + CHUNK_SIZE])\n",
    "        X_train_chunks.append(chunk)\n",
    "        del chunk\n",
    "        if i % (CHUNK_SIZE * 5) == 0:\n",
    "            cleanup_memory()\n",
    "    X_train = np.vstack(X_train_chunks)\n",
    "    del X_train_chunks\n",
    "    cleanup_memory()\n",
    "\n",
    "    # Transform val in chunks\n",
    "    if X_val.shape[0] > CHUNK_SIZE:\n",
    "        print(f\"  Transforming val data in chunks (size={CHUNK_SIZE})...\")\n",
    "        X_val_chunks = []\n",
    "        for i in range(0, X_val.shape[0], CHUNK_SIZE):\n",
    "            chunk = scaler.transform(X_val[i:i + CHUNK_SIZE])\n",
    "            X_val_chunks.append(chunk)\n",
    "            del chunk\n",
    "        X_val = np.vstack(X_val_chunks)\n",
    "        del X_val_chunks\n",
    "    else:\n",
    "        X_val = scaler.transform(X_val)\n",
    "else:\n",
    "    # Small dataset - fit and transform normally\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "cleanup_memory()\n",
    "phase_time = time.time() - phase_start\n",
    "print(\"  âœ… Scaling complete!\")\n",
    "print(f\"\\nâ±ï¸  Feature Scaling Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\")\n",
    "memory_usage()\n",
    "\n",
    "# Store raw (unscaled) data again for CV\n",
    "X_train_raw = X_train.copy()\n",
    "X_val_raw = X_val.copy()\n",
    "y_train_raw = y_train.copy()\n",
    "y_val_raw = y_val.copy()\n",
    "cleanup_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e319f05f",
   "metadata": {
    "papermill": {
     "duration": 0.001876,
     "end_time": "2025-11-19T09:53:02.581903",
     "exception": false,
     "start_time": "2025-11-19T09:53:02.580027",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Cross-Validation & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164ee0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:53:02.586051Z",
     "iopub.status.busy": "2025-11-19T09:53:02.585939Z",
     "iopub.status.idle": "2025-11-19T09:53:02.723036Z",
     "shell.execute_reply": "2025-11-19T09:53:02.722718Z"
    },
    "papermill": {
     "duration": 0.140245,
     "end_time": "2025-11-19T09:53:02.723871",
     "exception": false,
     "start_time": "2025-11-19T09:53:02.583626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 5: Hyperparameter Tuning Setup\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Combine train and val for CV\n",
    "# Use RAW (unscaled) data for CV â€“ Pipeline will scale per fold\n",
    "X_full = np.vstack([X_train_raw, X_val_raw])\n",
    "y_full = np.hstack([y_train_raw, y_val_raw])\n",
    "\n",
    "# Use subset for hyperparameter tuning if dataset is too large\n",
    "MAX_SAMPLES_FOR_CV = 20000\n",
    "if len(X_full) > MAX_SAMPLES_FOR_CV:\n",
    "    print(f\"âš ï¸ Dataset too large ({len(X_full)} samples), using subset ({MAX_SAMPLES_FOR_CV} samples) for CV\")\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_full, _, y_full, _ = train_test_split(\n",
    "        X_full, y_full,\n",
    "        train_size=MAX_SAMPLES_FOR_CV,\n",
    "        stratify=y_full,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    print(f\"  Using {len(X_full)} samples for hyperparameter tuning\")\n",
    "    cleanup_memory()\n",
    "\n",
    "print(f\"\\nðŸ“Š Full dataset for CV: {X_full.shape}, labels: {y_full.shape}\")\n",
    "print(f\"  Positive samples: {y_full.sum()}, Negative: {(y_full == 0).sum()}\")\n",
    "\n",
    "# Setup 5-fold Stratified CV\n",
    "N_FOLDS = 5\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# XGBoost hyperparameter grid (optimized to prevent warnings)\n",
    "# Key fixes:\n",
    "# - Remove min_split_gain from grid (set as fixed parameter = 0.0 to prevent \"no splits\" warnings)\n",
    "# - Lower min_child_samples to allow more granular splits\n",
    "# - Use colsample_bytree ONLY (never colsample_bytree) to avoid parameter conflict\n",
    "# - Add num_leaves for better tree control\n",
    "# - Add force_col_wise to remove threading overhead warning\n",
    "XGB_PARAM_GRID = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [3, 5, 7, -1],  # -1 means no limit\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    \"subsample\": [0.8, 0.9, 1.0],\n",
    "    \"colsample_bytree\": [0.8, 0.9, 1.0],  # Use colsample_bytree, NEVER colsample_bytree\n",
    "    \"min_child_samples\": [5, 10, 20],     # Lowered to allow more splits\n",
    "    # min_split_gain removed from grid - set as fixed parameter = 0.0\n",
    "    \"num_leaves\": [31, 50, 100],          # Control tree complexity\n",
    "    \"reg_alpha\": [0, 0.1, 0.5],\n",
    "    \"reg_lambda\": [1, 1.5, 2.0],\n",
    "    \"scale_pos_weight\": [1, (y_full == 0).sum() / max((y_full == 1).sum(), 1)],\n",
    "}\n",
    "\n",
    "# Ensure colsample_bytree is NEVER in the grid\n",
    "\n",
    "# Use RandomizedSearchCV for faster tuning\n",
    "USE_RANDOMIZED_SEARCH = True\n",
    "\n",
    "# Adjust iterations based on dataset size\n",
    "if len(X_full) > 50_000:\n",
    "    N_ITER_RANDOM = 10\n",
    "elif len(X_full) > 20_000:\n",
    "    N_ITER_RANDOM = 15\n",
    "else:\n",
    "    N_ITER_RANDOM = 20\n",
    "\n",
    "print(f\"\\nðŸ” Hyperparameter tuning:\")\n",
    "print(f\"  Method: {'RandomizedSearchCV' if USE_RANDOMIZED_SEARCH else 'GridSearchCV'}\")\n",
    "print(f\"  CV folds: {N_FOLDS}\")\n",
    "if USE_RANDOMIZED_SEARCH:\n",
    "    print(f\"  Random iterations: {N_ITER_RANDOM}\")\n",
    "\n",
    "cleanup_memory()\n",
    "memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hyperparam_tuning_cell",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:53:02.728310Z",
     "iopub.status.busy": "2025-11-19T09:53:02.728229Z",
     "iopub.status.idle": "2025-11-19T09:53:28.354563Z",
     "shell.execute_reply": "2025-11-19T09:53:28.353724Z"
    },
    "papermill": {
     "duration": 25.633221,
     "end_time": "2025-11-19T09:53:28.359216",
     "exception": false,
     "start_time": "2025-11-19T09:53:02.725995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with Pipeline (prevents data leakage)\n",
    "best_model = None\n",
    "best_params = None\n",
    "best_cv_score = 0.0\n",
    "\n",
    "try:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PHASE 5B: Hyperparameter Tuning\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Base XGBoost model\n",
    "    # Calculate balanced scale_pos_weight\n",
    "    scale_pos_weight = (y_full == 0).sum() / max((y_full == 1).sum(), 1)\n",
    "    base_model = XGBClassifier(\n",
    "        tree_method=\"hist\",\n",
    "        eval_metric=\"logloss\",\n",
    "        verbosity=0,\n",
    "        random_state=SEED,\n",
    "        n_jobs=2,\n",
    "        objective='binary:logistic',\n",
    "        verbose=-1,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        max_bin=256,\n",
    "    )\n",
    "\n",
    "    # Create Pipeline: scaler -> model (scaler fit per CV fold)\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', base_model)\n",
    "    ])\n",
    "\n",
    "    # Transform parameter grid for Pipeline\n",
    "    pipeline_param_grid = {f'model__{k}': v for k, v in XGB_PARAM_GRID.items()}\n",
    "\n",
    "    if USE_RANDOMIZED_SEARCH:\n",
    "        search = RandomizedSearchCV(\n",
    "            pipeline,\n",
    "            pipeline_param_grid,\n",
    "            cv=skf,\n",
    "            scoring='f1',\n",
    "            n_iter=N_ITER_RANDOM,\n",
    "            random_state=SEED,\n",
    "            n_jobs=1,  # Sequential processing for memory efficiency\n",
    "            verbose=1,\n",
    "        )\n",
    "    else:\n",
    "        search = GridSearchCV(\n",
    "            pipeline,\n",
    "            pipeline_param_grid,\n",
    "            cv=skf,\n",
    "            scoring='f1',\n",
    "            n_jobs=1,  # Sequential processing for memory efficiency\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "    # Fit with error handling\n",
    "    start_time = time.time()\n",
    "    search.fit(X_full, y_full)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Extract best model and parameters from pipeline\n",
    "    best_pipeline = search.best_estimator_\n",
    "    best_model = best_pipeline.named_steps['model']\n",
    "    # Extract model parameters (remove 'model__' prefix)\n",
    "    best_params = {k.replace('model__', ''): v for k, v in search.best_params_.items()}\n",
    "    best_cv_score = search.best_score_\n",
    "\n",
    "    print(f\"\\nâœ… Hyperparameter tuning complete ({elapsed_time/60:.1f} min)\")\n",
    "    print(f\"  Best CV F1: {best_cv_score:.4f}\")\n",
    "    print(f\"  Best parameters:\")\n",
    "    for key, value in best_params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "\n",
    "    cleanup_memory()\n",
    "    memory_usage()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error in hyperparameter tuning: {e}\")\n",
    "    print(\"âš ï¸ Using default parameters...\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    best_params = {}\n",
    "    best_cv_score = 0.0\n",
    "    cleanup_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0125edda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:53:28.369765Z",
     "iopub.status.busy": "2025-11-19T09:53:28.369634Z",
     "iopub.status.idle": "2025-11-19T09:53:28.847727Z",
     "shell.execute_reply": "2025-11-19T09:53:28.847369Z"
    },
    "papermill": {
     "duration": 0.482811,
     "end_time": "2025-11-19T09:53:28.849244",
     "exception": false,
     "start_time": "2025-11-19T09:53:28.366433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZATION: CV Performance Curves\n",
    "# ============================================\n",
    "try:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PLOTTING: Cross-Validation Performance Curves\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Get predictions from best model on CV data for visualization\n",
    "    if ('search' in locals() and hasattr(search, 'best_estimator_')) or best_model is not None:\n",
    "        # Use the best pipeline to get predictions\n",
    "        # Use the best pipeline from RandomizedSearchCV\n",
    "        if 'search' in locals() and hasattr(search, 'best_estimator_'):\n",
    "            best_pipeline = search.best_estimator_\n",
    "        elif best_model is not None:\n",
    "            # Fallback: create pipeline with best model\n",
    "            from sklearn.pipeline import Pipeline\n",
    "            best_pipeline = Pipeline([('scaler', StandardScaler().fit(X_full)), ('model', best_model)])\n",
    "        else:\n",
    "            raise ValueError(\"No best model available\")\n",
    "        y_cv_proba = best_pipeline.predict_proba(X_full)[:, 1]\n",
    "        \n",
    "        # Calculate PR and ROC curves\n",
    "        precision, recall, pr_thresholds = precision_recall_curve(y_full, y_cv_proba)\n",
    "        fpr, tpr, roc_thresholds = roc_curve(y_full, y_cv_proba)\n",
    "        roc_auc = roc_auc_score(y_full, y_cv_proba)\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # PR Curve\n",
    "        axes[0].plot(recall, precision, color='darkorange', lw=2, \n",
    "                    label=f'PR Curve (AUC = {np.trapz(precision, recall):.4f})')\n",
    "        axes[0].axhline(y=y_full.mean(), color='navy', linestyle='--', \n",
    "                        label=f'Baseline (Precision = {y_full.mean():.4f})')\n",
    "        axes[0].set_xlabel('Recall', fontsize=12)\n",
    "        axes[0].set_ylabel('Precision', fontsize=12)\n",
    "        axes[0].set_title('Precision-Recall Curve (CV)', fontsize=14, fontweight='bold')\n",
    "        axes[0].legend(loc='best')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # ROC Curve\n",
    "        axes[1].plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                    label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "        axes[1].plot([0, 1], [0, 1], color='navy', linestyle='--', label='Random Classifier')\n",
    "        axes[1].set_xlabel('False Positive Rate', fontsize=12)\n",
    "        axes[1].set_ylabel('True Positive Rate', fontsize=12)\n",
    "        axes[1].set_title('ROC Curve (CV)', fontsize=14, fontweight='bold')\n",
    "        axes[1].legend(loc='best')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        cv_plot_path = PROJECT_ROOT / 'logs' / 'model_xgboost_cv_curves.png'\n",
    "        plt.savefig(cv_plot_path, dpi=150, bbox_inches='tight')\n",
    "        plt.show()  # Display plot in notebook\n",
    "        display(Image(str(cv_plot_path)))  # Display inline\n",
    "        print(\"âœ… CV curves saved to: logs/model_xgboost_cv_curves.png\")\n",
    "        plt.close()\n",
    "        cleanup_memory()\n",
    "    else:\n",
    "        print(\"âš ï¸ Skipping CV visualization (best_model or data not available)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Error plotting CV curves: {e}\")\n",
    "    cleanup_memory()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73de6174",
   "metadata": {
    "papermill": {
     "duration": 0.002559,
     "end_time": "2025-11-19T09:53:28.854696",
     "exception": false,
     "start_time": "2025-11-19T09:53:28.852137",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Threshold Tuning & Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45df782",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:53:28.860664Z",
     "iopub.status.busy": "2025-11-19T09:53:28.860558Z",
     "iopub.status.idle": "2025-11-19T09:53:31.534830Z",
     "shell.execute_reply": "2025-11-19T09:53:31.534388Z"
    },
    "papermill": {
     "duration": 2.678747,
     "end_time": "2025-11-19T09:53:31.535899",
     "exception": false,
     "start_time": "2025-11-19T09:53:28.857152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train final model on full data\n",
    "try:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PHASE 6: Final Model Training & Threshold Tuning\")\n",
    "    print(\"=\" * 80)\n",
    "    phase_start = time.time()\n",
    "    print(\"Training Final Model on Full Dataset...\")\n",
    "\n",
    "    # Use best parameters, or defaults if best_params is None\n",
    "    # Initialize best_params if not set\n",
    "    if 'best_params' not in locals():\n",
    "        best_params = {}\n",
    "        best_cv_score = 0.0\n",
    "        print(\"  âš ï¸ best_params not found, using defaults\")\n",
    "\n",
    "    # Initialize best_params if not set\n",
    "    try:\n",
    "        _ = best_params\n",
    "    except NameError:\n",
    "        best_params = {}\n",
    "        best_cv_score = 0.0\n",
    "        print(\"  âš ï¸ best_params not found, using defaults\")\n",
    "\n",
    "    # Use best parameters, or defaults if best_params is None\n",
    "    # Initialize best_params if not set (from hyperparameter tuning)\n",
    "    if 'best_params' not in globals():\n",
    "        best_params = {}\n",
    "        best_cv_score = 0.0\n",
    "        print(\"  âš ï¸ best_params not found, using defaults\")\n",
    "\n",
    "    # Use best parameters, or defaults if best_params is None\n",
    "    final_params = best_params.copy() if best_params else {}\n",
    "\n",
    "    # Set safe defaults to prevent warnings\n",
    "    final_params['min_split_gain'] = 0.0\n",
    "    final_params.setdefault('min_child_samples', 5)\n",
    "    \n",
    "    # Remove scale_pos_weight from final_params if present (we'll set it explicitly)\n",
    "    final_params.pop('scale_pos_weight', None)\n",
    "    \n",
    "    # Calculate balanced scale_pos_weight for final model\n",
    "    scale_pos_weight_final = (y_train == 0).sum() / max((y_train == 1).sum(), 1)\n",
    "\n",
    "    # Use class_weight if SMOTE was skipped\n",
    "    use_class_weight = globals().get('USE_CLASS_WEIGHT', False)\n",
    "    if use_class_weight:\n",
    "        scale_pos_weight_final = (y_train == 0).sum() / max((y_train == 1).sum(), 1)\n",
    "        print(f\"  Using class_weight (SMOTE skipped): scale_pos_weight={scale_pos_weight_final:.4f}\")\n",
    "    else:\n",
    "        scale_pos_weight_final = final_params.pop('scale_pos_weight', None) or ((y_train == 0).sum() / max((y_train == 1).sum(), 1))\n",
    "    \n",
    "    final_model = XGBClassifier(\n",
    "        tree_method=\"hist\",\n",
    "        eval_metric=\"logloss\",\n",
    "        verbosity=0,\n",
    "        **final_params,\n",
    "        random_state=SEED,\n",
    "        n_jobs=2,\n",
    "        objective='binary:logistic',\n",
    "        verbose=-1,\n",
    "        scale_pos_weight=scale_pos_weight_final,\n",
    "        max_bin=256,\n",
    "    )\n",
    "\n",
    "    # Aggressive cleanup before final training\n",
    "    cleanup_memory()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"  Memory cleaned before final training\")\n",
    "\n",
    "    # Train with early stopping\n",
    "    print(\"  Training with early stopping...\")\n",
    "    try:\n",
    "        # Try new XGBoost API (2.0+) with callbacks\n",
    "        from xgboost import callback\n",
    "        final_model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            callbacks=[callback.EarlyStopping(rounds=50, save_best=True)],\n",
    "            verbose=False\n",
    "        )\n",
    "    except (TypeError, AttributeError):\n",
    "        # Fallback to old API or no early stopping\n",
    "        try:\n",
    "            final_model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                early_stopping_rounds=50,\n",
    "                verbose=False\n",
    "            )\n",
    "        except TypeError:\n",
    "            # If both fail, train without early stopping\n",
    "            print(\"  âš ï¸ Early stopping not supported, training without it\")\n",
    "            final_model.fit(X_train, y_train)\n",
    "\n",
    "    # Get predictions on validation set (original split)\n",
    "    y_val_proba = safe_prediction(final_model.predict_proba, X=X_val)[:, 1]\n",
    "\n",
    "    # Find optimal threshold using precision-recall curve\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(y_val, y_val_proba)\n",
    "    f1_scores_pr = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    best_pr_idx = np.argmax(f1_scores_pr)\n",
    "    best_pr_threshold = pr_thresholds[best_pr_idx] if best_pr_idx < len(pr_thresholds) else 0.5\n",
    "    best_pr_f1 = f1_scores_pr[best_pr_idx]\n",
    "\n",
    "    # Manual fine-grained search in optimal region\n",
    "    thresholds = np.concatenate([\n",
    "        np.linspace(0.01, 0.05, 20),\n",
    "        np.linspace(0.05, 0.15, 50),\n",
    "        np.linspace(0.15, 0.3, 30),\n",
    "        np.linspace(0.3, 0.9, 20)\n",
    "    ])\n",
    "\n",
    "    best_threshold = best_pr_threshold\n",
    "    best_f1 = best_pr_f1\n",
    "    for thr in thresholds:\n",
    "        y_pred = (y_val_proba >= thr).astype(int)\n",
    "        f1 = f1_score(y_val, y_pred, pos_label=1, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = thr\n",
    "\n",
    "    phase_time = time.time() - phase_start\n",
    "    print(f\"\\nâœ… Final Optimal Threshold: {best_threshold:.4f}\")\n",
    "    print(f\"âœ… Final Validation F1: {best_f1:.4f}\")\n",
    "    print(f\"\\nâ±ï¸  Final Model Training & Threshold Tuning Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\")\n",
    "\n",
    "    # Classification report\n",
    "    y_val_pred = (y_val_proba >= best_threshold).astype(int)\n",
    "    print(\"\\nðŸ“Š Classification Report:\")\n",
    "    print(classification_report(y_val, y_val_pred, digits=4, zero_division=0))\n",
    "\n",
    "    cleanup_memory()\n",
    "    memory_usage()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error in final training: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ae8b44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:53:31.547008Z",
     "iopub.status.busy": "2025-11-19T09:53:31.546879Z",
     "iopub.status.idle": "2025-11-19T09:53:32.900334Z",
     "shell.execute_reply": "2025-11-19T09:53:32.899567Z"
    },
    "papermill": {
     "duration": 1.36162,
     "end_time": "2025-11-19T09:53:32.904460",
     "exception": false,
     "start_time": "2025-11-19T09:53:31.542840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZATION: Final Model Performance\n",
    "# ============================================\n",
    "try:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PLOTTING: Final Model Performance Visualizations\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if 'final_model' in locals() and 'y_val_proba' in locals() and 'y_val' in locals():\n",
    "        # Calculate curves\n",
    "        precision, recall, pr_thresholds = precision_recall_curve(y_val, y_val_proba)\n",
    "        fpr, tpr, roc_thresholds = roc_curve(y_val, y_val_proba)\n",
    "        roc_auc = roc_auc_score(y_val, y_val_proba)\n",
    "        pr_auc = np.trapz(precision, recall)\n",
    "        \n",
    "        # Create comprehensive figure\n",
    "        fig = plt.figure(figsize=(16, 12))\n",
    "        \n",
    "        # 1. PR Curve\n",
    "        ax1 = plt.subplot(2, 3, 1)\n",
    "        ax1.plot(recall, precision, color='darkorange', lw=2, \n",
    "                label=f'PR Curve (AUC = {pr_auc:.4f})')\n",
    "        ax1.axhline(y=y_val.mean(), color='navy', linestyle='--', \n",
    "                   label=f'Baseline (Precision = {y_val.mean():.4f})')\n",
    "        ax1.axvline(x=recall[np.argmax(precision * recall)], color='red', linestyle=':', \n",
    "                   alpha=0.7, label=f'Optimal Point')\n",
    "        ax1.set_xlabel('Recall', fontsize=11)\n",
    "        ax1.set_ylabel('Precision', fontsize=11)\n",
    "        ax1.set_title('Precision-Recall Curve', fontsize=12, fontweight='bold')\n",
    "        ax1.legend(loc='best', fontsize=9)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. ROC Curve\n",
    "        ax2 = plt.subplot(2, 3, 2)\n",
    "        ax2.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "        ax2.plot([0, 1], [0, 1], color='navy', linestyle='--', label='Random Classifier')\n",
    "        ax2.set_xlabel('False Positive Rate', fontsize=11)\n",
    "        ax2.set_ylabel('True Positive Rate', fontsize=11)\n",
    "        ax2.set_title('ROC Curve', fontsize=12, fontweight='bold')\n",
    "        ax2.legend(loc='best', fontsize=9)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Confusion Matrix\n",
    "        ax3 = plt.subplot(2, 3, 3)\n",
    "        cm = confusion_matrix(y_val, y_val_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax3, \n",
    "                   xticklabels=['Negative', 'Positive'], \n",
    "                   yticklabels=['Negative', 'Positive'])\n",
    "        ax3.set_ylabel('True Label', fontsize=11)\n",
    "        ax3.set_xlabel('Predicted Label', fontsize=11)\n",
    "        ax3.set_title(f'Confusion Matrix\\n(Threshold = {best_threshold:.4f})', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # 4. Feature Importance (Top 20)\n",
    "        ax4 = plt.subplot(2, 3, 4)\n",
    "        if hasattr(final_model, 'feature_importances_'):\n",
    "            feature_importance = final_model.feature_importances_\n",
    "            top_n = min(20, len(feature_importance))\n",
    "            top_indices = np.argsort(feature_importance)[-top_n:][::-1]\n",
    "            top_importance = feature_importance[top_indices]\n",
    "            ax4.barh(range(top_n), top_importance, color='steelblue')\n",
    "            ax4.set_yticks(range(top_n))\n",
    "            ax4.set_yticklabels([f'Feature {i}' for i in top_indices], fontsize=8)\n",
    "            ax4.set_xlabel('Importance', fontsize=11)\n",
    "            ax4.set_title(f'Top {top_n} Feature Importances', fontsize=12, fontweight='bold')\n",
    "            ax4.grid(True, alpha=0.3, axis='x')\n",
    "        else:\n",
    "            ax4.text(0.5, 0.5, 'Feature importance\\nnot available', \n",
    "                    ha='center', va='center', transform=ax4.transAxes, fontsize=12)\n",
    "            ax4.set_title('Feature Importance', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # 5. Threshold Analysis\n",
    "        ax5 = plt.subplot(2, 3, 5)\n",
    "        thresholds_fine = np.linspace(0, 1, 100)\n",
    "        f1_scores_fine = []\n",
    "        for thr in thresholds_fine:\n",
    "            y_pred_thr = (y_val_proba >= thr).astype(int)\n",
    "            f1_thr = f1_score(y_val, y_pred_thr, pos_label=1, zero_division=0)\n",
    "            f1_scores_fine.append(f1_thr)\n",
    "        ax5.plot(thresholds_fine, f1_scores_fine, color='darkgreen', lw=2, label='F1 Score')\n",
    "        ax5.axvline(x=best_threshold, color='red', linestyle='--', lw=2, \n",
    "                   label=f'Optimal Threshold = {best_threshold:.4f}')\n",
    "        ax5.axhline(y=best_f1, color='red', linestyle=':', alpha=0.7, \n",
    "                   label=f'Best F1 = {best_f1:.4f}')\n",
    "        ax5.set_xlabel('Threshold', fontsize=11)\n",
    "        ax5.set_ylabel('F1 Score', fontsize=11)\n",
    "        ax5.set_title('F1 Score vs Threshold', fontsize=12, fontweight='bold')\n",
    "        ax5.legend(loc='best', fontsize=9)\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 6. Prediction Distribution\n",
    "        ax6 = plt.subplot(2, 3, 6)\n",
    "        ax6.hist(y_val_proba[y_val == 0], bins=50, alpha=0.6, label='Negative', \n",
    "                color='blue', density=True)\n",
    "        ax6.hist(y_val_proba[y_val == 1], bins=50, alpha=0.6, label='Positive', \n",
    "                color='red', density=True)\n",
    "        ax6.axvline(x=best_threshold, color='green', linestyle='--', lw=2, \n",
    "                   label=f'Threshold = {best_threshold:.4f}')\n",
    "        ax6.set_xlabel('Predicted Probability', fontsize=11)\n",
    "        ax6.set_ylabel('Density', fontsize=11)\n",
    "        ax6.set_title('Prediction Probability Distribution', fontsize=12, fontweight='bold')\n",
    "        ax6.legend(loc='best', fontsize=9)\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle('Model XGBoost: XGBoost Performance Analysis', fontsize=14, fontweight='bold', y=0.995)\n",
    "        plt.tight_layout()\n",
    "        final_plot_path = PROJECT_ROOT / 'logs' / 'model_xgboost_final_performance.png'\n",
    "        plt.savefig(final_plot_path, dpi=150, bbox_inches='tight')\n",
    "        plt.show()  # Display plot in notebook\n",
    "        display(Image(str(final_plot_path)))  # Display inline\n",
    "        print(\"âœ… Final performance plots saved to: logs/model_xgboost_final_performance.png\")\n",
    "        plt.close()\n",
    "        cleanup_memory()\n",
    "    else:\n",
    "        print(\"âš ï¸ Skipping final visualization (final_model or predictions not available)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Error plotting final performance: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    cleanup_memory()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5a6cb2",
   "metadata": {
    "papermill": {
     "duration": 0.010949,
     "end_time": "2025-11-19T09:53:32.928518",
     "exception": false,
     "start_time": "2025-11-19T09:53:32.917569",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995db9b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:53:32.952354Z",
     "iopub.status.busy": "2025-11-19T09:53:32.952077Z",
     "iopub.status.idle": "2025-11-19T09:53:32.975800Z",
     "shell.execute_reply": "2025-11-19T09:53:32.973922Z"
    },
    "papermill": {
     "duration": 0.037419,
     "end_time": "2025-11-19T09:53:32.976780",
     "exception": false,
     "start_time": "2025-11-19T09:53:32.939361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save model with calibration\n",
    "try:\n",
    "    from sklearn.calibration import CalibratedClassifierCV\n",
    "    \n",
    "    model_save_path = MODEL_SAVE_DIR / \"model_xgboost_all_features_best.pkl\"\n",
    "\n",
    "    # Calibrate model probabilities before saving\n",
    "    print(\"\\nðŸ“Š Calibrating model probabilities...\")\n",
    "    calibrated_model = CalibratedClassifierCV(\n",
    "        final_model,\n",
    "        method='isotonic',\n",
    "        cv=3,\n",
    "        n_jobs=1  # Sequential processing for memory efficiency\n",
    "    )\n",
    "    calibrated_model.fit(X_train, y_train)\n",
    "    print(\"  âœ… Model calibrated\")\n",
    "    \n",
    "    # Use calibrated model for predictions\n",
    "    y_val_proba_calibrated = calibrated_model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Re-optimize threshold on calibrated predictions\n",
    "    precision_cal, recall_cal, pr_thresholds_cal = precision_recall_curve(y_val, y_val_proba_calibrated)\n",
    "    f1_scores_cal = 2 * (precision_cal * recall_cal) / (precision_cal + recall_cal + 1e-10)\n",
    "    best_cal_idx = np.argmax(f1_scores_cal)\n",
    "    best_threshold_cal = pr_thresholds_cal[best_cal_idx] if best_cal_idx < len(pr_thresholds_cal) else 0.5\n",
    "    best_f1_cal = f1_scores_cal[best_cal_idx]\n",
    "    \n",
    "    print(f\"  Calibrated F1: {best_f1_cal:.4f} (vs original {best_f1:.4f})\")\n",
    "    print(f\"  Calibrated threshold: {best_threshold_cal:.4f}\")\n",
    "\n",
    "    save_dict = {\n",
    "        \"model\": calibrated_model,  # Save calibrated model\n",
    "        \"scaler\": scaler if 'scaler' in locals() else None,\n",
    "        \"pca_models\": pca_models if 'pca_models' in locals() else None,\n",
    "        \"best_params\": best_params,\n",
    "        \"best_cv_score\": best_cv_score,\n",
    "        \"best_threshold\": best_threshold_cal,  # Use calibrated threshold\n",
    "        \"best_f1\": best_f1_cal,  # Use calibrated F1\n",
    "        \"reg_cols\": reg_cols,\n",
    "        \"emb_family_to_cols\": emb_family_to_cols,\n",
    "    }\n",
    "\n",
    "    with open(model_save_path, \"wb\") as f:\n",
    "        pickle.dump(save_dict, f)\n",
    "\n",
    "    print(f\"\\nðŸ’¾ Model saved to: {model_save_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error saving model: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eaca92",
   "metadata": {
    "papermill": {
     "duration": 0.009938,
     "end_time": "2025-11-19T09:53:32.996716",
     "exception": false,
     "start_time": "2025-11-19T09:53:32.986778",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9. Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e847f217",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:53:33.017628Z",
     "iopub.status.busy": "2025-11-19T09:53:33.017517Z",
     "iopub.status.idle": "2025-11-19T09:53:36.481762Z",
     "shell.execute_reply": "2025-11-19T09:53:36.480968Z"
    },
    "papermill": {
     "duration": 3.484958,
     "end_time": "2025-11-19T09:53:36.489972",
     "exception": false,
     "start_time": "2025-11-19T09:53:33.005014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_work_id(id_value: str) -> str:\n",
    "    \"\"\"Extract work_id from URL or return as is if already just ID.\"\"\"\n",
    "    id_str = str(id_value)\n",
    "    # If it already looks like a work ID, just return it\n",
    "    if id_str.startswith('W') and len(id_str) > 1 and '/' not in id_str:\n",
    "        return id_str\n",
    "    # Otherwise, extract from URL or string\n",
    "    match = re.search(r'W\\d+', id_str)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return id_str\n",
    "\n",
    "# Load test data and generate predictions\n",
    "try:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PHASE 7: Test Predictions\")\n",
    "    print(\"=\" * 80)\n",
    "    phase_start = time.time()\n",
    "    print(\"Generating Test Predictions...\")\n",
    "\n",
    "    test_df = load_parquet_split(\"test\")\n",
    "    test_ids = test_df[\"id\"].to_numpy()\n",
    "\n",
    "    # Process test data same as train\n",
    "    X_reg_test, X_emb_test_fams, _, _, _ = split_features_reg_and_all_emb(test_df)\n",
    "    del test_df\n",
    "\n",
    "    # Apply PCA (if enabled) or use embeddings directly\n",
    "    if USE_PCA and 'pca_models' in locals() and pca_models:\n",
    "        # Apply PCA using trained models\n",
    "        X_emb_test_pca, _ = apply_pca_to_embeddings(\n",
    "            X_emb_test_fams, fit_on_train=False, pca_models=pca_models\n",
    "        )\n",
    "        \n",
    "        # Combine features\n",
    "        if X_reg_test is not None:\n",
    "            X_test = np.hstack([X_reg_test, X_emb_test_pca])\n",
    "        else:\n",
    "            X_test = X_emb_test_pca\n",
    "        \n",
    "        del X_emb_test_pca\n",
    "    else:\n",
    "        # No PCA - combine embeddings directly\n",
    "        X_emb_test_list = [X_emb_test_fams[fam] for fam in X_emb_test_fams.keys()]\n",
    "        X_emb_test_combined = np.hstack(X_emb_test_list) if X_emb_test_list else None\n",
    "        \n",
    "        # Combine features\n",
    "        if X_reg_test is not None:\n",
    "            X_test = np.hstack([X_reg_test, X_emb_test_combined])\n",
    "        else:\n",
    "            X_test = X_emb_test_combined\n",
    "        \n",
    "        del X_emb_test_list, X_emb_test_combined\n",
    "\n",
    "    del X_reg_test, X_emb_test_fams\n",
    "    cleanup_memory()\n",
    "\n",
    "    # Scale\n",
    "    if \"scaler\" in locals():\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Predict in chunks for Out Of Memory protection\n",
    "    chunk_size = 10000\n",
    "    if X_test.shape[0] > chunk_size:\n",
    "        print(f\"  Predicting in chunks (size={chunk_size}) for OOM protection...\")\n",
    "        y_test_proba_chunks = []\n",
    "        for i in range(0, X_test.shape[0], chunk_size):\n",
    "            chunk_proba = safe_prediction(final_model.predict_proba, X=X_test[i : i + chunk_size])[:, 1]\n",
    "            y_test_proba_chunks.append(chunk_proba)\n",
    "            del chunk_proba\n",
    "            cleanup_memory()\n",
    "        y_test_proba = np.concatenate(y_test_proba_chunks)\n",
    "        del y_test_proba_chunks\n",
    "    else:\n",
    "        y_test_proba = safe_prediction(final_model.predict_proba, X=X_test)[:, 1]\n",
    "\n",
    "    y_test_pred = (y_test_proba >= best_threshold).astype(int)\n",
    "\n",
    "    # Create submission using Polars\n",
    "    work_ids = np.array([extract_work_id(str(id_val)) for id_val in test_ids])\n",
    "    submission_df = pl.DataFrame({\"work_id\": work_ids, \"label\": y_test_pred})\n",
    "\n",
    "    submission_path = SUBMISSION_DIR / \"submission_model_xgboost.csv\"\n",
    "    submission_df.write_csv(submission_path)\n",
    "\n",
    "    phase_time = time.time() - phase_start\n",
    "    print(f\"\\nâœ… Submission saved to: {submission_path}\")\n",
    "    print(f\"  Test predictions: {len(y_test_pred)}, Positive: {y_test_pred.sum()}, Negative: {(y_test_pred==0).sum()}\")\n",
    "    print(f\"\\nâ±ï¸  Test Predictions Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\")\n",
    "\n",
    "    cleanup_memory()\n",
    "    memory_usage()\n",
    "    \n",
    "    # Print total execution time summary\n",
    "    total_time = time.time() - TOTAL_START_TIME\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    # Print total execution time summary\n",
    "    total_time = time.time() - TOTAL_START_TIME\n",
    "    END_TIME_STR = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MODEL_XGBOOST EXECUTION COMPLETED\")\n",
    "    print(f\"Start Time: {START_TIME_STR}\")\n",
    "    print(f\"End Time: {END_TIME_STR}\")\n",
    "    print(f\"Total Execution Time: {total_time:.2f} seconds ({total_time/60:.2f} minutes / {total_time/3600:.2f} hours)\")\n",
    "    print(f\"Final Validation F1 Score: {best_f1:.4f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(f\"End Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Final Validation F1 Score: {best_f1:.4f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    total_time = time.time() - TOTAL_START_TIME\n",
    "    print(f\"\\nâŒ Error generating submission: {e}\")\n",
    "    print(f\"Execution failed after {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
