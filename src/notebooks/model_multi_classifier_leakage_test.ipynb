{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Classifier Pipeline: Leakage Test\n",
    "\n",
    "This notebook combines:\n",
    "- ‚úÖ **XGBoost features**: All regular features + embeddings (PCA-compressed) from `data/results/`\n",
    "- ‚úÖ **Temporal features**: Date-derived features (days_since_publication, days_since_updated, num_years_after_publication)\n",
    "- ‚úÖ **Multiple classifiers**: BernoulliNB, LogisticRegression, DecisionTree, RandomForest, GradientBoosting, ExtraTrees, AdaBoost, LGBMClassifier, XGBClassifier, CatBoostClassifier, BaggingClassifier, Perceptron, QuadraticDiscriminantAnalysis, GaussianNB, LinearDiscriminantAnalysis, ExtraTreeClassifier, SGDClassifier, DummyClassifier\n",
    "- ‚úÖ **RandomUnderSampler**: For class imbalance handling\n",
    "- ‚úÖ **Selective scaling**: Numeric columns only, preserves binary features\n",
    "- ‚úÖ **Feature review**: Keeps both original and engineered features for comparison\n",
    "- ‚úÖ **Duplicate elimination**: Removes absolute duplicate features\n",
    "- ‚úÖ Threshold optimization and submission generation\n",
    "- ‚úÖ OOM Safe with aggressive memory management\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìë Multi-Classifier Pipeline - Code Navigation Index\n",
    "\n",
    "## Quick Navigation\n",
    "- **[Setup](#1-setup)** - Imports, paths, device configuration, robustness utilities\n",
    "- **[Data Loading](#2-data-loading)** - Load base features and embeddings from `data/results/`\n",
    "- **[Temporal Feature Engineering](#3-temporal-feature-engineering)** - Add temporal date features\n",
    "- **[Feature Combination](#4-feature-combination)** - Combine regular + embeddings (PCA) + temporal features\n",
    "- **[Duplicate Elimination](#5-duplicate-elimination)** - Remove absolute duplicate features\n",
    "- **[Feature Review](#6-feature-review)** - Display original vs engineered features\n",
    "- **[Selective Scaling](#7-selective-scaling)** - Scale numeric columns only\n",
    "- **[Class Imbalance](#8-class-imbalance)** - RandomUnderSampler\n",
    "- **[Model Training](#9-model-training)** - Train multiple classifiers\n",
    "- **[Model Comparison](#10-model-comparison)** - Evaluate and compare all models\n",
    "- **[Threshold Tuning](#11-threshold-tuning)** - Optimal threshold finding\n",
    "- **[Model Saving](#12-save-model)** - Save best model\n",
    "- **[Submission](#13-generate-submission)** - Generate test predictions\n",
    "\n",
    "## Model Types: Multiple Classifiers \n",
    "- BernoulliNB\n",
    "- LogisticRegression\n",
    "- DecisionTreeClassifier\n",
    "- RandomForestClassifier\n",
    "- GradientBoostingClassifier\n",
    "- ExtraTreesClassifier\n",
    "- AdaBoostClassifier\n",
    "- LGBMClassifier\n",
    "- XGBClassifier\n",
    "- CatBoostClassifier\n",
    "- BaggingClassifier\n",
    "- Perceptron\n",
    "- QuadraticDiscriminantAnalysis\n",
    "- GaussianNB\n",
    "- LinearDiscriminantAnalysis\n",
    "- ExtraTreeClassifier\n",
    "- SGDClassifier\n",
    "- DummyClassifier\n",
    "\n",
    "## Feature Sources\n",
    "- **XGBoost features**: Regular features (54) + Embeddings (PCA-compressed: 32 per family)\n",
    "- **Temporal**: days_since_publication, days_since_updated, num_years_after_publication\n",
    "- **Combined**: All features merged, duplicates removed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:25:49.260828Z",
     "iopub.status.busy": "2025-11-20T09:25:49.260559Z",
     "iopub.status.idle": "2025-11-20T09:25:50.210028Z",
     "shell.execute_reply": "2025-11-20T09:25:50.209643Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import gc\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "from typing import Dict, Optional, List, Tuple\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import signal\n",
    "import atexit\n",
    "from functools import wraps\n",
    "from datetime import datetime\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:25:50.211377Z",
     "iopub.status.busy": "2025-11-20T09:25:50.211272Z",
     "iopub.status.idle": "2025-11-20T09:25:50.235007Z",
     "shell.execute_reply": "2025-11-20T09:25:50.234608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MULTI-CLASSIFIER PIPELINE EXECUTION STARTED\n",
      "Start Time: 2025-11-20 04:25:50\n",
      "================================================================================\n",
      "\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# STARTUP & REPRODUCIBILITY\n",
    "# =========================\n",
    "\n",
    "TOTAL_START_TIME = time.time()\n",
    "START_TIME_STR = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"MULTI-CLASSIFIER PIPELINE EXECUTION STARTED\")\n",
    "print(f\"Start Time: {START_TIME_STR}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:25:50.250855Z",
     "iopub.status.busy": "2025-11-20T09:25:50.250726Z",
     "iopub.status.idle": "2025-11-20T09:25:50.253608Z",
     "shell.execute_reply": "2025-11-20T09:25:50.253276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /Users/santoshdesai/Documents/Desai_Projects/Kaggle2\n",
      "RESULTS_DIR: /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/results\n",
      "MODEL_SAVE_DIR: /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/models/saved_models\n",
      "SUBMISSION_DIR: /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/submission_files\n"
     ]
    }
   ],
   "source": [
    "# ==============\n",
    "# PATH MANAGEMENT\n",
    "# ==============\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Get project root by finding data/results directory\n",
    "current = Path(os.getcwd())\n",
    "PROJECT_ROOT = current\n",
    "\n",
    "# Search up to 10 levels to find data/results\n",
    "for _ in range(10):\n",
    "    if (PROJECT_ROOT / \"data\" / \"results\").exists():\n",
    "        break\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "else:\n",
    "    # Fallback: go up two levels from current (assuming we're in src/notebooks)\n",
    "    PROJECT_ROOT = current.parent.parent\n",
    "\n",
    "RESULTS_DIR = PROJECT_ROOT / \"data\" / \"results\"\n",
    "MODEL_SAVE_DIR = PROJECT_ROOT / \"models\" / \"saved_models\"\n",
    "SUBMISSION_DIR = PROJECT_ROOT / \"data\" / \"submission_files\"\n",
    "MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SUBMISSION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"RESULTS_DIR:\", RESULTS_DIR)\n",
    "print(\"MODEL_SAVE_DIR:\", MODEL_SAVE_DIR)\n",
    "print(\"SUBMISSION_DIR:\", SUBMISSION_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:25:50.254647Z",
     "iopub.status.busy": "2025-11-20T09:25:50.254571Z",
     "iopub.status.idle": "2025-11-20T09:25:52.150871Z",
     "shell.execute_reply": "2025-11-20T09:25:52.150443Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# ==========\n",
    "# ML LIBRARIES\n",
    "# ==========\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import (\n",
    "  train_test_split,\n",
    "  cross_val_score,\n",
    "  StratifiedKFold,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "  f1_score,\n",
    "  roc_auc_score,\n",
    "  classification_report,\n",
    "  precision_recall_curve,\n",
    "  roc_curve,\n",
    "  confusion_matrix,\n",
    ")\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "  RandomForestClassifier,\n",
    "  GradientBoostingClassifier,\n",
    "  ExtraTreesClassifier,\n",
    "  AdaBoostClassifier,\n",
    "  BaggingClassifier,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.linear_model import Perceptron, SGDClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# ==========\n",
    "# VISUALIZATION LIBRARIES\n",
    "# ==========\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Image\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "sns.set_palette(\"husl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:25:52.152053Z",
     "iopub.status.busy": "2025-11-20T09:25:52.151936Z",
     "iopub.status.idle": "2025-11-20T09:25:52.155313Z",
     "shell.execute_reply": "2025-11-20T09:25:52.155020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Using fallback memory utilities\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# MEMORY UTILITIES (FALLBACK DEFS)\n",
    "# ===============================\n",
    "try:\n",
    "  from model_training_utils import cleanup_memory, memory_usage, check_memory_safe\n",
    "\n",
    "  print(\"‚úÖ Memory utilities imported from shared module\")\n",
    "except ImportError:\n",
    "\n",
    "  def cleanup_memory():\n",
    "    \"\"\"Aggressive memory cleanup for both CPU and GPU.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "      torch.cuda.empty_cache()\n",
    "      torch.cuda.synchronize()\n",
    "      torch.cuda.ipc_collect()\n",
    "    gc.collect()\n",
    "\n",
    "  def memory_usage():\n",
    "    \"\"\"Display current memory usage statistics.\"\"\"\n",
    "    try:\n",
    "      import psutil\n",
    "\n",
    "      process = psutil.Process(os.getpid())\n",
    "      mem_gb = process.memory_info.rss / 1024**3\n",
    "      print(f\"üíæ Memory: {mem_gb:.2f} GB (RAM)\", end=\"\")\n",
    "      if torch.cuda.is_available():\n",
    "        gpu_mem = torch.cuda.memory_allocated() / 1024**3\n",
    "        gpu_reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\" | {gpu_mem:.2f}/{gpu_reserved:.2f} GB (GPU used/reserved)\")\n",
    "      else:\n",
    "        print()\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "  def check_memory_safe(ram_threshold_gb=0.85, gpu_threshold=0.80):\n",
    "    \"\"\"Check if memory usage is safe for operations.\"\"\"\n",
    "    try:\n",
    "      import psutil\n",
    "\n",
    "      process = psutil.Process(os.getpid())\n",
    "      ram_gb = process.memory_info().rss / 1024**3\n",
    "      total_ram = psutil.virtual_memory().total / 1024**3\n",
    "      ram_ratio = ram_gb / total_ram if total_ram > 0 else 0\n",
    "      gpu_ratio = 0\n",
    "      if torch.cuda.is_available():\n",
    "        gpu_used = torch.cuda.memory_allocated / 1024**3\n",
    "        gpu_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        gpu_ratio = gpu_used / gpu_total if gpu_total > 0 else 0\n",
    "      is_safe = ram_ratio < ram_threshold_gb and gpu_ratio < gpu_threshold\n",
    "      return is_safe, {\n",
    "        \"ram_gb\": ram_gb,\n",
    "        \"ram_ratio\": ram_ratio,\n",
    "        \"gpu_ratio\": gpu_ratio,\n",
    "      }\n",
    "    except:\n",
    "      return True, {}\n",
    "\n",
    "  print(\"‚ö†Ô∏è Using fallback memory utilities\")\n",
    "\n",
    "memory_usage()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Feature Combination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:25:52.156276Z",
     "iopub.status.busy": "2025-11-20T09:25:52.156220Z",
     "iopub.status.idle": "2025-11-20T09:25:52.635230Z",
     "shell.execute_reply": "2025-11-20T09:25:52.634779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 1: Data Loading (XGBoost Style)\n",
      "================================================================================\n",
      "Loading train base features from /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/results/X_train.parquet\n",
      "Loading val base features from /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/results/X_val.parquet\n",
      "Loading test base features from /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/results/X_test.parquet\n",
      "Loading train labels from /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/results/y_train.npy\n",
      "Loading val labels from /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/results/y_val.npy\n",
      "Loading train sent_transformer embeddings from /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/results/sent_transformer_X_train.parquet\n",
      "Loading val sent_transformer embeddings from /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/results/sent_transformer_X_val.parquet\n",
      "Loading test sent_transformer embeddings from /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/results/sent_transformer_X_test.parquet\n",
      "Loading train scibert embeddings from /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/results/scibert_X_train.parquet\n",
      "Loading val scibert embeddings from /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/results/scibert_X_val.parquet\n",
      "Loading test scibert embeddings from /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/results/scibert_X_test.parquet\n",
      "Loading train specter2 embeddings from /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/results/specter2_X_train.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading val specter2 embeddings from /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/results/specter2_X_val.parquet\n",
      "Loading test specter2 embeddings from /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/results/specter2_X_test.parquet\n",
      "\n",
      "üìä Combining base features with embeddings...\n",
      " ‚úÖ Merged sent_transformer embeddings\n",
      " ‚úÖ Merged scibert embeddings\n",
      " ‚úÖ Merged specter2 embeddings\n",
      "\n",
      "üìä Data Summary:\n",
      " Train samples: 9600, Features: 1977\n",
      " Val samples: 1200\n",
      " Test samples: 1200\n",
      " Train Positive: 648, Negative: 8952\n",
      " Val Positive: 81, Negative: 1119\n",
      "\n",
      "‚è±Ô∏è Data Loading Time: 0.31 seconds (0.01 minutes)\n"
     ]
    }
   ],
   "source": [
    "def load_base_features(split: str) -> pl.DataFrame:\n",
    "  \"\"\"Load base feature matrix from data/results/\"\"\"\n",
    "  path = RESULTS_DIR / f\"X_{split}.parquet\"\n",
    "  if not path.exists():\n",
    "    raise FileNotFoundError(f\"Could not find {split} base features at {path}\")\n",
    "  print(f\"Loading {split} base features from {path}\")\n",
    "  return pl.read_parquet(path)\n",
    "\n",
    "\n",
    "def load_embeddings(split: str, embedding_type: str) -> Optional[pl.DataFrame]:\n",
    "  \"\"\"Load embedding features from data/results/\"\"\"\n",
    "  path = RESULTS_DIR / f\"{embedding_type}_X_{split}.parquet\"\n",
    "  if not path.exists():\n",
    "    print(f\"‚ö†Ô∏è {embedding_type} embeddings not found for {split}\")\n",
    "    return None\n",
    "  print(f\"Loading {split} {embedding_type} embeddings from {path}\")\n",
    "  return pl.read_parquet(path)\n",
    "\n",
    "\n",
    "def load_labels(split: str) -> np.ndarray:\n",
    "  \"\"\"Load labels from data/results/\"\"\"\n",
    "  path = RESULTS_DIR / f\"y_{split}.npy\"\n",
    "  if not path.exists():\n",
    "    raise FileNotFoundError(f\"Could not find {split} labels at {path}\")\n",
    "  print(f\"Loading {split} labels from {path}\")\n",
    "  return np.load(path)\n",
    "\n",
    "\n",
    "def split_features_reg_and_all_emb(df: pl.DataFrame):\n",
    "  \"\"\"Split features into regular and embedding families (from XGBoost notebook).\"\"\"\n",
    "  cols = df.columns\n",
    "  dtypes = df.dtypes\n",
    "  label = df[\"label\"].to_numpy() if \"label\" in cols else None\n",
    "\n",
    "  reg_cols = []\n",
    "  EMBEDDING_FAMILY_PREFIXES = [\n",
    "    \"sent_transformer_\",\n",
    "    \"scibert_\",\n",
    "    \"specter_\",\n",
    "    \"specter2_\",\n",
    "    \"ner_\",\n",
    "  ]\n",
    "  emb_family_to_cols = {p: [] for p in EMBEDDING_FAMILY_PREFIXES}\n",
    "\n",
    "  NUMERIC_DTYPES = {\n",
    "    pl.Int8,\n",
    "    pl.Int16,\n",
    "    pl.Int32,\n",
    "    pl.Int64,\n",
    "    pl.UInt8,\n",
    "    pl.UInt16,\n",
    "    pl.UInt32,\n",
    "    pl.UInt64,\n",
    "    pl.Float32,\n",
    "    pl.Float64,\n",
    "  }\n",
    "\n",
    "  for c, dt in zip(cols, dtypes):\n",
    "    if c in (\"id\", \"label\"):\n",
    "      continue\n",
    "    matched = False\n",
    "    for p in EMBEDDING_FAMILY_PREFIXES:\n",
    "      if c.startswith(p):\n",
    "        emb_family_to_cols[p].append(c)\n",
    "        matched = True\n",
    "        break\n",
    "    if not matched and dt in NUMERIC_DTYPES:\n",
    "      reg_cols.append(c)\n",
    "\n",
    "  X_reg = df.select(reg_cols).to_numpy() if reg_cols else None\n",
    "  X_emb_families = {}\n",
    "  for p, clist in emb_family_to_cols.items():\n",
    "    if clist:\n",
    "      X_emb_families[p] = df.select(clist).to_numpy()\n",
    "\n",
    "  return X_reg, X_emb_families, label, reg_cols, emb_family_to_cols\n",
    "\n",
    "\n",
    "# Load data (XGBoost style: base features + embeddings)\n",
    "try:\n",
    "  print(\"\\n\" + \"=\" * 80)\n",
    "  print(\"PHASE 1: Data Loading (XGBoost Style)\")\n",
    "  print(\"=\" * 80)\n",
    "  phase_start = time.time()\n",
    "\n",
    "  # Load base features\n",
    "  X_train_base = load_base_features(\"train\")\n",
    "  X_val_base = load_base_features(\"val\")\n",
    "  X_test_base = load_base_features(\"test\")\n",
    "\n",
    "  # Load labels\n",
    "  y_train = load_labels(\"train\")\n",
    "  y_val = load_labels(\"val\")\n",
    "\n",
    "  # Load embeddings (if available)\n",
    "  embedding_types = [\"sent_transformer\", \"scibert\", \"specter2\"]\n",
    "  train_embeddings = {}\n",
    "  val_embeddings = {}\n",
    "  test_embeddings = {}\n",
    "\n",
    "  for emb_type in embedding_types:\n",
    "    train_emb = load_embeddings(\"train\", emb_type)\n",
    "    val_emb = load_embeddings(\"val\", emb_type)\n",
    "    test_emb = load_embeddings(\"test\", emb_type)\n",
    "    if train_emb is not None:\n",
    "      train_embeddings[emb_type] = train_emb\n",
    "      val_embeddings[emb_type] = val_emb\n",
    "      test_embeddings[emb_type] = test_emb\n",
    "\n",
    "  # Combine base features with embeddings (like XGBoost)\n",
    "  print(\"\\nüìä Combining base features with embeddings...\")\n",
    "\n",
    "  X_train_combined = X_train_base.clone()\n",
    "  X_val_combined = X_val_base.clone()\n",
    "  X_test_combined = X_test_base.clone()\n",
    "\n",
    "  # Merge embeddings by 'id' column\n",
    "  for emb_type, train_emb in train_embeddings.items():\n",
    "    if \"id\" in train_emb.columns:\n",
    "      X_train_combined = X_train_combined.join(train_emb, on=\"id\", how=\"left\")\n",
    "      X_val_combined = X_val_combined.join(\n",
    "        val_embeddings[emb_type], on=\"id\", how=\"left\"\n",
    "      )\n",
    "      X_test_combined = X_test_combined.join(\n",
    "        test_embeddings[emb_type], on=\"id\", how=\"left\"\n",
    "      )\n",
    "      print(f\" ‚úÖ Merged {emb_type} embeddings\")\n",
    "\n",
    "  # Extract work_id for later use\n",
    "  work_ids_train = None\n",
    "  work_ids_val = None\n",
    "  work_ids_test = None\n",
    "\n",
    "  if \"id\" in X_train_combined.columns:\n",
    "    work_ids_train = X_train_combined.select(\"id\").to_series().to_list()\n",
    "    work_ids_val = X_val_combined.select(\"id\").to_series().to_list()\n",
    "    work_ids_test = X_test_combined.select(\"id\").to_series().to_list()\n",
    "\n",
    "  # Store original dataframes for feature review\n",
    "  X_train_original = X_train_combined.clone()\n",
    "  X_val_original = X_val_combined.clone()\n",
    "  X_test_original = X_test_combined.clone()\n",
    "\n",
    "  phase_time = time.time() - phase_start\n",
    "  print(f\"\\nüìä Data Summary:\")\n",
    "  print(\n",
    "    f\" Train samples: {len(X_train_combined)}, Features: {len(X_train_combined.columns)}\"\n",
    "  )\n",
    "  print(f\" Val samples: {len(X_val_combined)}\")\n",
    "  print(f\" Test samples: {len(X_test_combined)}\")\n",
    "  print(f\" Train Positive: {y_train.sum()}, Negative: {(y_train==0).sum()}\")\n",
    "  print(f\" Val Positive: {y_val.sum()}, Negative: {(y_val==0).sum()}\")\n",
    "  print(\n",
    "    f\"\\n‚è±Ô∏è Data Loading Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\"\n",
    "  )\n",
    "\n",
    "  cleanup_memory()\n",
    "  memory_usage()\n",
    "except Exception as e:\n",
    "  print(f\"‚ùå Error loading data: {e}\")\n",
    "  import traceback\n",
    "\n",
    "  traceback.print_exc()\n",
    "  raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Temporal Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:25:52.636550Z",
     "iopub.status.busy": "2025-11-20T09:25:52.636463Z",
     "iopub.status.idle": "2025-11-20T09:25:52.880045Z",
     "shell.execute_reply": "2025-11-20T09:25:52.879661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 2: Feature Engineering (Temporal)\n",
      "================================================================================\n",
      "\n",
      "üîß Extracting temporal features...\n",
      " ‚úÖ Created 'num_years_after_publication'\n",
      " ‚úÖ Created 'new_language'\n",
      " ‚úÖ One-hot encoded 'new_language' (42 categories)\n",
      " ‚úÖ Feature engineering complete. Final shape: (9600, 2016)\n",
      "\n",
      "üîß Extracting temporal features...\n",
      " ‚úÖ Created 'num_years_after_publication'\n",
      " ‚úÖ Created 'new_language'\n",
      " ‚úÖ One-hot encoded 'new_language' (32 categories)\n",
      " ‚úÖ Feature engineering complete. Final shape: (1200, 2006)\n",
      "\n",
      "üîß Extracting temporal features...\n",
      " ‚úÖ Created 'num_years_after_publication'\n",
      " ‚úÖ Created 'new_language'\n",
      " ‚úÖ One-hot encoded 'new_language' (32 categories)\n",
      " ‚úÖ Feature engineering complete. Final shape: (1200, 2006)\n",
      "\n",
      "‚úÖ Feature engineering complete\n",
      " Final feature count: 2016\n",
      " Train shape: (9600, 2016)\n",
      " Val shape: (1200, 2016)\n",
      " Test shape: (1200, 2016)\n",
      "\n",
      "‚è±Ô∏è Feature Engineering Time: 0.11 seconds (0.00 minutes)\n"
     ]
    }
   ],
   "source": [
    "def safe_parse_json(value):\n",
    "  \"\"\"Safely parse JSON string or return empty dict/list.\"\"\"\n",
    "  if value is None:\n",
    "    return {}\n",
    "  if isinstance(value, (dict, list)):\n",
    "    return value\n",
    "  if isinstance(value, str):\n",
    "    try:\n",
    "      return ast.literal_eval(value) if value else {}\n",
    "    except:\n",
    "      return {}\n",
    "  return {}\n",
    "\n",
    "\n",
    "def extract_temporal_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "  \"\"\"Extract temporal features from date columns.\"\"\"\n",
    "  print(\"\\nüîß Extracting temporal features...\")\n",
    "\n",
    "  df_processed = df.clone()\n",
    "\n",
    "  # Drop abstract if present (temporal drops it)\n",
    "  if \"abstract\" in df_processed.columns:\n",
    "    df_processed = df_processed.drop(\"abstract\")\n",
    "    print(\" ‚úÖ Dropped 'abstract' column\")\n",
    "\n",
    "  # Drop id column (temporal extracts work_id separately)\n",
    "  if \"id\" in df_processed.columns:\n",
    "    df_processed = df_processed.drop(\"id\")\n",
    "\n",
    "  # Date features (matching temporal)\n",
    "  if \"publication_year\" in df_processed.columns:\n",
    "    current_year = datetime.now().year\n",
    "    df_processed = df_processed.with_columns(\n",
    "      (current_year - pl.col(\"publication_year\")).alias(\n",
    "        \"num_years_after_publication\"\n",
    "      )\n",
    "    )\n",
    "    df_processed = df_processed.drop(\"publication_year\")\n",
    "    print(\" ‚úÖ Created 'num_years_after_publication'\")\n",
    "\n",
    "  if \"updated_date\" in df_processed.columns:\n",
    "    try:\n",
    "      # Convert to datetime and calculate days\n",
    "      df_processed = df_processed.with_columns(\n",
    "        pl.col(\"updated_date\").str.strptime(\n",
    "          pl.Datetime, format=\"%Y-%m-%dT%H:%M:%S.%f\", strict=False\n",
    "        )\n",
    "      )\n",
    "      today = datetime.now()\n",
    "      df_processed = df_processed.with_columns(\n",
    "        ((today - pl.col(\"updated_date\")).dt.total_days()).alias(\n",
    "          \"days_since_updated\"\n",
    "        )\n",
    "      )\n",
    "      df_processed = df_processed.drop(\"updated_date\")\n",
    "      print(\" ‚úÖ Created 'days_since_updated'\")\n",
    "    except:\n",
    "      df_processed = df_processed.drop(\"updated_date\")\n",
    "\n",
    "  if \"publication_date\" in df_processed.columns:\n",
    "    try:\n",
    "      df_processed = df_processed.with_columns(\n",
    "        pl.col(\"publication_date\").str.strptime(\n",
    "          pl.Datetime, format=\"%Y-%m-%d\", strict=False\n",
    "        )\n",
    "      )\n",
    "      today = datetime.now()\n",
    "      df_processed = df_processed.with_columns(\n",
    "        ((today - pl.col(\"publication_date\")).dt.total_days()).alias(\n",
    "          \"days_since_publication\"\n",
    "        )\n",
    "      )\n",
    "      df_processed = df_processed.drop(\"publication_date\")\n",
    "      print(\" ‚úÖ Created 'days_since_publication'\")\n",
    "    except:\n",
    "      df_processed = df_processed.drop(\"publication_date\")\n",
    "\n",
    "  # Drop doi_url (temporal drops it)\n",
    "  if \"doi_url\" in df_processed.columns:\n",
    "    df_processed = df_processed.drop(\"doi_url\")\n",
    "\n",
    "  # Drop ids column (temporal drops it)\n",
    "  if \"ids\" in df_processed.columns:\n",
    "    df_processed = df_processed.drop(\"ids\")\n",
    "\n",
    "  # Open access normalization (matching temporal: extract is_oa, oa_status, any_repository_has_fulltext)\n",
    "  if \"open_access\" in df_processed.columns:\n",
    "    open_access_parsed = (\n",
    "      df_processed.select(\"open_access\")\n",
    "      .to_series()\n",
    "      .map_elements(safe_parse_json, return_dtype=pl.Object)\n",
    "    )\n",
    "\n",
    "    is_oa_values = []\n",
    "    oa_status_values = []\n",
    "    any_repository_has_fulltext_values = []\n",
    "\n",
    "    for oa in open_access_parsed:\n",
    "      if isinstance(oa, dict):\n",
    "        is_oa_values.append(1.0 if oa.get(\"is_oa\", False) else 0.0)\n",
    "        oa_status_values.append(oa.get(\"oa_status\", \"closed\"))\n",
    "        any_repository_has_fulltext_values.append(\n",
    "          1.0 if oa.get(\"any_repository_has_fulltext\", False) else 0.0\n",
    "        )\n",
    "      else:\n",
    "        is_oa_values.append(0.0)\n",
    "        oa_status_values.append(\"closed\")\n",
    "        any_repository_has_fulltext_values.append(0.0)\n",
    "\n",
    "    df_processed = df_processed.with_columns(\n",
    "      [\n",
    "        pl.Series(\"is_oa\", is_oa_values, dtype=pl.Float32),\n",
    "        pl.Series(\"oa_status\", oa_status_values, dtype=pl.Utf8),\n",
    "        pl.Series(\n",
    "          \"any_repository_has_fulltext\",\n",
    "          any_repository_has_fulltext_values,\n",
    "          dtype=pl.Float32,\n",
    "        ),\n",
    "      ]\n",
    "    )\n",
    "    df_processed = df_processed.drop(\"open_access\")\n",
    "    print(\n",
    "      \" ‚úÖ Extracted open_access features (is_oa, oa_status, any_repository_has_fulltext)\"\n",
    "    )\n",
    "\n",
    "  # Authorships count (matching temporal: num_authorships)\n",
    "  if \"authorships\" in df_processed.columns:\n",
    "    authorships_parsed = (\n",
    "      df_processed.select(\"authorships\")\n",
    "      .to_series()\n",
    "      .map_elements(safe_parse_json, return_dtype=pl.Object)\n",
    "    )\n",
    "\n",
    "    num_authorships = []\n",
    "    for auth in authorships_parsed:\n",
    "      if isinstance(auth, list):\n",
    "        # Count author positions (matching temporal logic)\n",
    "        count = sum(\n",
    "          1 for a in auth if isinstance(a, dict) and \"author_position\" in a\n",
    "        )\n",
    "        num_authorships.append(float(count))\n",
    "      else:\n",
    "        num_authorships.append(0.0)\n",
    "\n",
    "    df_processed = df_processed.with_columns(\n",
    "      pl.Series(\"num_authorships\", num_authorships, dtype=pl.Float32)\n",
    "    )\n",
    "    df_processed = df_processed.drop(\"authorships\")\n",
    "    print(\" ‚úÖ Created 'num_authorships'\")\n",
    "\n",
    "  # Drop locations (temporal drops it)\n",
    "  if \"locations\" in df_processed.columns:\n",
    "    df_processed = df_processed.drop(\"locations\")\n",
    "\n",
    "  # Primary location normalization (matching temporal)\n",
    "  if \"primary_location\" in df_processed.columns:\n",
    "    primary_location_parsed = (\n",
    "      df_processed.select(\"primary_location\")\n",
    "      .to_series()\n",
    "      .map_elements(safe_parse_json, return_dtype=pl.Object)\n",
    "    )\n",
    "\n",
    "    # Extract source fields if available\n",
    "    source_fields = {}\n",
    "    for ploc in primary_location_parsed:\n",
    "      if isinstance(ploc, dict) and \"source\" in ploc:\n",
    "        source = ploc[\"source\"]\n",
    "        if isinstance(source, dict):\n",
    "          for key, value in source.items():\n",
    "            if key not in source_fields:\n",
    "              source_fields[key] = []\n",
    "            source_fields[key].append(value if value is not None else \"\")\n",
    "          break\n",
    "\n",
    "    # Add source fields as columns (simplified - temporal does json_normalize)\n",
    "    # For now, we'll drop primary_location as the nested structure is complex\n",
    "    df_processed = df_processed.drop(\"primary_location\")\n",
    "    print(\" ‚úÖ Processed 'primary_location' (dropped nested structure)\")\n",
    "\n",
    "  # Related works count (matching temporal: num_related_words)\n",
    "  if \"related_works\" in df_processed.columns:\n",
    "    related_works_parsed = (\n",
    "      df_processed.select(\"related_works\")\n",
    "      .to_series()\n",
    "      .map_elements(safe_parse_json, return_dtype=pl.Object)\n",
    "    )\n",
    "\n",
    "    num_related_words = []\n",
    "    for rw in related_works_parsed:\n",
    "      if isinstance(rw, list):\n",
    "        num_related_words.append(float(len(rw)))\n",
    "      else:\n",
    "        num_related_words.append(0.0)\n",
    "\n",
    "    df_processed = df_processed.with_columns(\n",
    "      pl.Series(\"num_related_words\", num_related_words, dtype=pl.Float32)\n",
    "    )\n",
    "    df_processed = df_processed.drop(\"related_works\")\n",
    "    print(\" ‚úÖ Created 'num_related_words'\")\n",
    "\n",
    "  # Grants count (matching temporal: num_grants)\n",
    "  if \"grants\" in df_processed.columns:\n",
    "    grants_parsed = (\n",
    "      df_processed.select(\"grants\")\n",
    "      .to_series()\n",
    "      .map_elements(safe_parse_json, return_dtype=pl.Object)\n",
    "    )\n",
    "\n",
    "    num_grants = []\n",
    "    for g in grants_parsed:\n",
    "      if isinstance(g, list):\n",
    "        num_grants.append(float(len(g)))\n",
    "      else:\n",
    "        num_grants.append(0.0)\n",
    "\n",
    "    df_processed = df_processed.with_columns(\n",
    "      pl.Series(\"num_grants\", num_grants, dtype=pl.Float32)\n",
    "    )\n",
    "    df_processed = df_processed.drop(\"grants\")\n",
    "    print(\" ‚úÖ Created 'num_grants'\")\n",
    "\n",
    "  # Drop title, concepts (temporal drops them)\n",
    "  for col in [\"title\", \"concepts\"]:\n",
    "    if col in df_processed.columns:\n",
    "      df_processed = df_processed.drop(col)\n",
    "\n",
    "  # Language normalization (matching temporal: new_language)\n",
    "  if \"language\" in df_processed.columns:\n",
    "    df_processed = df_processed.with_columns(\n",
    "      pl.col(\"language\").fill_null(\"unknown\").alias(\"new_language\")\n",
    "    )\n",
    "    df_processed = df_processed.drop(\"language\")\n",
    "    print(\" ‚úÖ Created 'new_language'\")\n",
    "\n",
    "  # Type and type_crossref handling (matching temporal)\n",
    "  # Keep type_crossref if present, drop type\n",
    "  if \"type\" in df_processed.columns:\n",
    "    df_processed = df_processed.drop(\"type\")\n",
    "\n",
    "  # Categorical dummies for new_language, oa_status (matching temporal)\n",
    "  # Note: Polars doesn't have get_dummies, so we'll do it manually\n",
    "  if \"new_language\" in df_processed.columns:\n",
    "    lang_values = df_processed.select(\"new_language\").to_series().unique().to_list()\n",
    "    for lang in lang_values:\n",
    "      if lang is not None:\n",
    "        col_name = f\"new_language_{lang}\"\n",
    "        df_processed = df_processed.with_columns(\n",
    "          (pl.col(\"new_language\") == lang).cast(pl.Float32).alias(col_name)\n",
    "        )\n",
    "    df_processed = df_processed.drop(\"new_language\")\n",
    "    print(f\" ‚úÖ One-hot encoded 'new_language' ({len(lang_values)} categories)\")\n",
    "\n",
    "  if \"oa_status\" in df_processed.columns:\n",
    "    oa_status_values = (\n",
    "      df_processed.select(\"oa_status\").to_series().unique().to_list()\n",
    "    )\n",
    "    for status in oa_status_values:\n",
    "      if status is not None:\n",
    "        col_name = f\"oa_status_{status}\"\n",
    "        df_processed = df_processed.with_columns(\n",
    "          (pl.col(\"oa_status\") == status).cast(pl.Float32).alias(col_name)\n",
    "        )\n",
    "    df_processed = df_processed.drop(\"oa_status\")\n",
    "    print(f\" ‚úÖ One-hot encoded 'oa_status' ({len(oa_status_values)} categories)\")\n",
    "\n",
    "  # Fill nulls and ensure float types\n",
    "  df_processed = df_processed.fill_null(0.0)\n",
    "\n",
    "  # Convert all numeric columns to float32\n",
    "  for col in df_processed.columns:\n",
    "    if df_processed[col].dtype in [\n",
    "      pl.Int8,\n",
    "      pl.Int16,\n",
    "      pl.Int32,\n",
    "      pl.Int64,\n",
    "      pl.UInt8,\n",
    "      pl.UInt16,\n",
    "      pl.UInt32,\n",
    "      pl.UInt64,\n",
    "    ]:\n",
    "      df_processed = df_processed.with_columns(pl.col(col).cast(pl.Float32))\n",
    "\n",
    "  print(f\" ‚úÖ Feature engineering complete. Final shape: {df_processed.shape}\")\n",
    "  return df_processed\n",
    "\n",
    "\n",
    "# Apply feature engineering\n",
    "try:\n",
    "  print(\"\\n\" + \"=\" * 80)\n",
    "  print(\"PHASE 2: Feature Engineering (Temporal)\")\n",
    "  print(\"=\" * 80)\n",
    "  phase_start = time.time()\n",
    "\n",
    "  X_train_engineered = extract_temporal_features(X_train_combined)\n",
    "  X_val_engineered = extract_temporal_features(X_val_combined)\n",
    "  X_test_engineered = extract_temporal_features(X_test_combined)\n",
    "\n",
    "  # Ensure all dataframes have the same columns (align test to train)\n",
    "  train_cols = X_train_engineered.columns\n",
    "  val_cols = X_val_engineered.columns\n",
    "  test_cols = X_test_engineered.columns\n",
    "\n",
    "  # Add missing columns to val and test (fill with 0)\n",
    "  missing_val_cols = [c for c in train_cols if c not in val_cols]\n",
    "  missing_test_cols = [c for c in train_cols if c not in test_cols]\n",
    "\n",
    "  for col in missing_val_cols:\n",
    "    X_val_engineered = X_val_engineered.with_columns(pl.lit(0.0).alias(col))\n",
    "  for col in missing_test_cols:\n",
    "    X_test_engineered = X_test_engineered.with_columns(pl.lit(0.0).alias(col))\n",
    "\n",
    "  # Remove extra columns from val and test\n",
    "  extra_val_cols = [c for c in val_cols if c not in train_cols]\n",
    "  extra_test_cols = [c for c in test_cols if c not in train_cols]\n",
    "\n",
    "  if extra_val_cols:\n",
    "    X_val_engineered = X_val_engineered.drop(extra_val_cols)\n",
    "  if extra_test_cols:\n",
    "    X_test_engineered = X_test_engineered.drop(extra_test_cols)\n",
    "\n",
    "  # Reorder columns to match train\n",
    "  X_val_engineered = X_val_engineered.select(train_cols)\n",
    "  X_test_engineered = X_test_engineered.select(train_cols)\n",
    "\n",
    "  # Store column names for later use\n",
    "  feature_column_names = train_cols\n",
    "\n",
    "  # Convert to numpy arrays (keep as DataFrame for now to identify numeric columns)\n",
    "  # Keep as DataFrame for feature combination\n",
    "  # X_train_np = X_train_engineered.to_numpy().astype(np.float32)\n",
    "  # X_val_np = X_val_engineered.to_numpy().astype(np.float32)\n",
    "  # X_test_np = X_test_engineered.to_numpy().astype(np.float32)\n",
    "\n",
    "  phase_time = time.time() - phase_start\n",
    "  print(f\"\\n‚úÖ Feature engineering complete\")\n",
    "  print(f\" Final feature count: {len(train_cols)}\")\n",
    "  print(f\" Train shape: {X_train_engineered.shape}\")\n",
    "  print(f\" Val shape: {X_val_engineered.shape}\")\n",
    "  print(f\" Test shape: {X_test_engineered.shape}\")\n",
    "  print(\n",
    "    f\"\\n‚è±Ô∏è Feature Engineering Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\"\n",
    "  )\n",
    "\n",
    "  del X_train_combined, X_val_combined, X_test_combined\n",
    "  cleanup_memory()\n",
    "  memory_usage()\n",
    "except Exception as e:\n",
    "  print(f\"‚ùå Error in feature engineering: {e}\")\n",
    "  import traceback\n",
    "\n",
    "  traceback.print_exc()\n",
    "  raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Combination (XGBoost Style: Regular + Embeddings with PCA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:25:52.881326Z",
     "iopub.status.busy": "2025-11-20T09:25:52.881252Z",
     "iopub.status.idle": "2025-11-20T09:25:53.825829Z",
     "shell.execute_reply": "2025-11-20T09:25:53.825238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 3: Feature Combination (XGBoost Style)\n",
      "================================================================================\n",
      "\n",
      "üìä Feature Split:\n",
      " Regular features: 96\n",
      " Embedding sent_transformer_: 384 dims\n",
      " Embedding scibert_: 768 dims\n",
      " Embedding specter2_: 768 dims\n",
      "\n",
      "üìä Applying IncrementalPCA to embedding families...\n",
      " sent_transformer_: 384 dims ‚Üí 32 components\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " scibert_: 768 dims ‚Üí 32 components\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " specter2_: 768 dims ‚Üí 32 components\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Feature combination complete\n",
      " Combined train: (9600, 192)\n",
      " Combined val: (1200, 192)\n",
      " Combined test: (1200, 192)\n",
      "\n",
      "‚è±Ô∏è Feature Combination Time: 0.80 seconds (0.01 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Split features into regular and embeddings (XGBoost style)\n",
    "# Then apply PCA to embeddings and combine with regular features\n",
    "try:\n",
    "  print(\"\\n\" + \"=\" * 80)\n",
    "  print(\"PHASE 3: Feature Combination (XGBoost Style)\")\n",
    "  print(\"=\" * 80)\n",
    "  phase_start = time.time()\n",
    "\n",
    "  # Split features (XGBoost style)\n",
    "  X_reg_train, X_emb_train_fams, _, reg_cols, emb_family_to_cols = (\n",
    "    split_features_reg_and_all_emb(X_train_engineered)\n",
    "  )\n",
    "  X_reg_val, X_emb_val_fams, _, _, _ = split_features_reg_and_all_emb(\n",
    "    X_val_engineered\n",
    "  )\n",
    "  X_reg_test, X_emb_test_fams, _, _, _ = split_features_reg_and_all_emb(\n",
    "    X_test_engineered\n",
    "  )\n",
    "\n",
    "  print(f\"\\nüìä Feature Split:\")\n",
    "  print(f\" Regular features: {len(reg_cols) if reg_cols else 0}\")\n",
    "  for fam, arr in X_emb_train_fams.items():\n",
    "    print(f\" Embedding {fam}: {arr.shape[1]} dims\")\n",
    "\n",
    "  # PCA configuration (matching XGBoost)\n",
    "  PCA_COMPONENTS_PER_FAMILY = {\n",
    "    \"sent_transformer_\": 32,\n",
    "    \"scibert_\": 32,\n",
    "    \"specter_\": 32,\n",
    "    \"specter2_\": 32,\n",
    "    \"ner_\": 16,\n",
    "  }\n",
    "\n",
    "  # Apply PCA to embeddings\n",
    "  print(\"\\nüìä Applying IncrementalPCA to embedding families...\")\n",
    "  X_emb_train_pca_list = []\n",
    "  X_emb_val_pca_list = []\n",
    "  X_emb_test_pca_list = []\n",
    "  pca_models = {}\n",
    "\n",
    "  for fam, X_emb_train in X_emb_train_fams.items():\n",
    "    n_components = PCA_COMPONENTS_PER_FAMILY.get(fam, 32)\n",
    "    print(f\" {fam}: {X_emb_train.shape[1]} dims ‚Üí {n_components} components\")\n",
    "\n",
    "    # Fit PCA on train\n",
    "    ipca = IncrementalPCA(\n",
    "      n_components=min(n_components, X_emb_train.shape[1]), batch_size=2000\n",
    "    )\n",
    "\n",
    "    # Fit on subset if too large\n",
    "    max_pca_rows = int(X_emb_train.shape[0] * 0.3)\n",
    "    if X_emb_train.shape[0] > max_pca_rows:\n",
    "      idx = np.random.choice(\n",
    "        X_emb_train.shape[0], size=max_pca_rows, replace=False\n",
    "      )\n",
    "      ipca.fit(X_emb_train[idx])\n",
    "      del idx\n",
    "    else:\n",
    "      ipca.fit(X_emb_train)\n",
    "\n",
    "    pca_models[fam] = ipca\n",
    "\n",
    "    # Transform train, val, test\n",
    "    X_emb_train_pca = ipca.transform(X_emb_train)\n",
    "    X_emb_val_pca = ipca.transform(X_emb_val_fams[fam])\n",
    "    X_emb_test_pca = ipca.transform(X_emb_test_fams[fam])\n",
    "\n",
    "    X_emb_train_pca_list.append(X_emb_train_pca)\n",
    "    X_emb_val_pca_list.append(X_emb_val_pca)\n",
    "    X_emb_test_pca_list.append(X_emb_test_pca)\n",
    "\n",
    "    cleanup_memory()\n",
    "\n",
    "  # Combine embeddings\n",
    "  X_emb_train_combined = (\n",
    "    np.hstack(X_emb_train_pca_list) if X_emb_train_pca_list else None\n",
    "  )\n",
    "  X_emb_val_combined = np.hstack(X_emb_val_pca_list) if X_emb_val_pca_list else None\n",
    "  X_emb_test_combined = (\n",
    "    np.hstack(X_emb_test_pca_list) if X_emb_test_pca_list else None\n",
    "  )\n",
    "\n",
    "  # Combine regular + embeddings\n",
    "  if X_reg_train is not None and X_emb_train_combined is not None:\n",
    "    X_train_combined_np = np.hstack([X_reg_train, X_emb_train_combined])\n",
    "    X_val_combined_np = np.hstack([X_reg_val, X_emb_val_combined])\n",
    "    X_test_combined_np = np.hstack([X_reg_test, X_emb_test_combined])\n",
    "  elif X_reg_train is not None:\n",
    "    X_train_combined_np = X_reg_train\n",
    "    X_val_combined_np = X_reg_val\n",
    "    X_test_combined_np = X_reg_test\n",
    "  elif X_emb_train_combined is not None:\n",
    "    X_train_combined_np = X_emb_train_combined\n",
    "    X_val_combined_np = X_emb_val_combined\n",
    "    X_test_combined_np = X_emb_test_combined\n",
    "  else:\n",
    "    raise ValueError(\"No features available!\")\n",
    "\n",
    "  phase_time = time.time() - phase_start\n",
    "  print(f\"\\n‚úÖ Feature combination complete\")\n",
    "  print(f\" Combined train: {X_train_combined_np.shape}\")\n",
    "  print(f\" Combined val: {X_val_combined_np.shape}\")\n",
    "  print(f\" Combined test: {X_test_combined_np.shape}\")\n",
    "  print(\n",
    "    f\"\\n‚è±Ô∏è Feature Combination Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\"\n",
    "  )\n",
    "\n",
    "  # Store feature names for temporal\n",
    "  combined_feature_names = []\n",
    "  if reg_cols:\n",
    "    combined_feature_names.extend(reg_cols)\n",
    "  for fam in X_emb_train_fams.keys():\n",
    "    n_comp = PCA_COMPONENTS_PER_FAMILY.get(fam, 32)\n",
    "    combined_feature_names.extend([f\"{fam}pca_{i}\" for i in range(n_comp)])\n",
    "\n",
    "  del X_reg_train, X_reg_val, X_reg_test\n",
    "  del X_emb_train_fams, X_emb_val_fams, X_emb_test_fams\n",
    "  del X_emb_train_pca_list, X_emb_val_pca_list, X_emb_test_pca_list\n",
    "  cleanup_memory()\n",
    "  memory_usage()\n",
    "except Exception as e:\n",
    "  print(f\"‚ùå Error in feature combination: {e}\")\n",
    "  import traceback\n",
    "\n",
    "  traceback.print_exc()\n",
    "  raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Duplicate Feature Elimination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:25:53.827107Z",
     "iopub.status.busy": "2025-11-20T09:25:53.827026Z",
     "iopub.status.idle": "2025-11-20T09:25:54.871132Z",
     "shell.execute_reply": "2025-11-20T09:25:54.870766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 4: Duplicate Feature Elimination\n",
      "================================================================================\n",
      "\n",
      "üîç Identifying duplicate features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found duplicate group: columns [2, 3, 5, 6, 7, 8, 12, 14, 18, 20, 34, 36, 37, 39, 40, 49, 50, 51, 52] (keeping column 2, removing 18 duplicates)\n",
      "\n",
      "üìä Removing 18 duplicate features\n",
      "\n",
      "‚úÖ Duplicate elimination complete\n",
      " Final feature count: 174 (removed 18 duplicates)\n",
      " Train shape: (9600, 174)\n",
      " Val shape: (1200, 174)\n",
      " Test shape: (1200, 174)\n",
      "\n",
      "‚è±Ô∏è Duplicate Elimination Time: 0.92 seconds (0.02 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Identify and remove absolute duplicate features\n",
    "try:\n",
    "  print(\"\\n\" + \"=\" * 80)\n",
    "  print(\"PHASE 4: Duplicate Feature Elimination\")\n",
    "  print(\"=\" * 80)\n",
    "  phase_start = time.time()\n",
    "\n",
    "  # Find duplicate columns (columns with identical values)\n",
    "  print(\"\\nüîç Identifying duplicate features...\")\n",
    "  duplicate_groups = []\n",
    "  checked_cols = set()\n",
    "\n",
    "  for i in range(X_train_combined_np.shape[1]):\n",
    "    if i in checked_cols:\n",
    "      continue\n",
    "\n",
    "    col_i_data = X_train_combined_np[:, i]\n",
    "\n",
    "    duplicates = [i]\n",
    "    for j in range(i + 1, X_train_combined_np.shape[1]):\n",
    "      if j in checked_cols:\n",
    "        continue\n",
    "\n",
    "      col_j_data = X_train_combined_np[:, j]\n",
    "\n",
    "      # Check if columns are identical (allowing for small floating point differences)\n",
    "      if np.allclose(col_i_data, col_j_data, rtol=1e-5, atol=1e-8):\n",
    "        duplicates.append(j)\n",
    "        checked_cols.add(j)\n",
    "\n",
    "    if len(duplicates) > 1:\n",
    "      duplicate_groups.append(duplicates)\n",
    "      checked_cols.add(i)\n",
    "\n",
    "  # Keep first column from each duplicate group, remove others\n",
    "  cols_to_keep = set(range(X_train_combined_np.shape[1]))\n",
    "  cols_to_remove = []\n",
    "\n",
    "  for group in duplicate_groups:\n",
    "    # Keep first, remove rest\n",
    "    cols_to_remove.extend(group[1:])\n",
    "    print(\n",
    "      f\" Found duplicate group: columns {group} (keeping column {group[0]}, removing {len(group)-1} duplicates)\"\n",
    "    )\n",
    "\n",
    "  cols_to_keep = sorted(list(cols_to_keep - set(cols_to_remove)))\n",
    "\n",
    "  if cols_to_remove:\n",
    "    print(f\"\\nüìä Removing {len(cols_to_remove)} duplicate features\")\n",
    "    X_train_final = X_train_combined_np[:, cols_to_keep]\n",
    "    X_val_final = X_val_combined_np[:, cols_to_keep]\n",
    "    X_test_final = X_test_combined_np[:, cols_to_keep]\n",
    "  else:\n",
    "    print(\"\\n‚úÖ No duplicate features found\")\n",
    "    X_train_final = X_train_combined_np\n",
    "    X_val_final = X_val_combined_np\n",
    "    X_test_final = X_test_combined_np\n",
    "\n",
    "  phase_time = time.time() - phase_start\n",
    "  print(f\"\\n‚úÖ Duplicate elimination complete\")\n",
    "  print(\n",
    "    f\" Final feature count: {X_train_final.shape[1]} (removed {len(cols_to_remove)} duplicates)\"\n",
    "  )\n",
    "  print(f\" Train shape: {X_train_final.shape}\")\n",
    "  print(f\" Val shape: {X_val_final.shape}\")\n",
    "  print(f\" Test shape: {X_test_final.shape}\")\n",
    "  print(\n",
    "    f\"\\n‚è±Ô∏è Duplicate Elimination Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\"\n",
    "  )\n",
    "\n",
    "  cleanup_memory()\n",
    "except Exception as e:\n",
    "  print(f\"‚ùå Error in duplicate elimination: {e}\")\n",
    "  import traceback\n",
    "\n",
    "  traceback.print_exc()\n",
    "  # Fallback: use original data\n",
    "  X_train_final = X_train_combined_np\n",
    "  X_val_final = X_val_combined_np\n",
    "  X_test_final = X_test_combined_np\n",
    "  cols_to_remove = []\n",
    "  print(\"‚ö†Ô∏è Continuing with original features (no deduplication)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Review: Original vs Engineered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:25:54.872345Z",
     "iopub.status.busy": "2025-11-20T09:25:54.872281Z",
     "iopub.status.idle": "2025-11-20T09:25:54.875471Z",
     "shell.execute_reply": "2025-11-20T09:25:54.875160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 5: Feature Review (Original vs Engineered)\n",
      "================================================================================\n",
      "\n",
      "üìä Original Features (from data_exploration_organized.ipynb):\n",
      " Columns: 1977\n",
      " Sample columns: ['abstract_length', 'abstract_word_count', 'avg_author_citations', 'avg_author_h_index', 'avg_concept_score', 'avg_topic_score', 'first_author_citations', 'first_author_h_index', 'first_author_papers', 'has_abstract']\n",
      "\n",
      "üìä Engineered Features (after temporal + PCA + deduplication):\n",
      " Total features: 174\n",
      " Regular features: 96\n",
      " Temporal features added: num_years_after_publication, days_since_updated, days_since_publication\n",
      " PCA-compressed embeddings: 144 components\n",
      "\n",
      "üìã Feature Breakdown:\n",
      " - Original regular features: 96\n",
      " - Temporal features (temporal): 3\n",
      " - PCA-compressed embeddings: 75\n",
      " - Duplicates removed: 18\n",
      "\n",
      "‚úÖ Feature review complete\n"
     ]
    }
   ],
   "source": [
    "# Display feature comparison for review\n",
    "try:\n",
    "  print(\"\\n\" + \"=\" * 80)\n",
    "  print(\"PHASE 5: Feature Review (Original vs Engineered)\")\n",
    "  print(\"=\" * 80)\n",
    "\n",
    "  print(\"\\nüìä Original Features (from data_exploration_organized.ipynb):\")\n",
    "  print(f\" Columns: {len(X_train_original.columns)}\")\n",
    "  print(f\" Sample columns: {list(X_train_original.columns[:10])}\")\n",
    "\n",
    "  print(\"\\nüìä Engineered Features (after temporal + PCA + deduplication):\")\n",
    "  print(f\" Total features: {X_train_final.shape[1]}\")\n",
    "  print(f\" Regular features: {len(reg_cols) if reg_cols else 0}\")\n",
    "  print(\n",
    "    f\" Temporal features added: num_years_after_publication, days_since_updated, days_since_publication\"\n",
    "  )\n",
    "  print(\n",
    "    f\" PCA-compressed embeddings: {sum(PCA_COMPONENTS_PER_FAMILY.values())} components\"\n",
    "  )\n",
    "\n",
    "  # Show feature breakdown\n",
    "  print(\"\\nüìã Feature Breakdown:\")\n",
    "  print(f\" - Original regular features: {len(reg_cols) if reg_cols else 0}\")\n",
    "  print(f\" - Temporal features (temporal): 3\")\n",
    "  print(\n",
    "    f\" - PCA-compressed embeddings: {X_train_final.shape[1] - (len(reg_cols) if reg_cols else 0) - 3}\"\n",
    "  )\n",
    "  print(\n",
    "    f\" - Duplicates removed: {len(cols_to_remove) if 'cols_to_remove' in locals() and cols_to_remove else 0}\"\n",
    "  )\n",
    "\n",
    "  # Store for later temporal\n",
    "  feature_review = {\n",
    "    \"original_features\": len(X_train_original.columns),\n",
    "    \"final_features\": X_train_final.shape[1],\n",
    "    \"regular_features\": len(reg_cols) if reg_cols else 0,\n",
    "    \"temporal_features\": 3,\n",
    "    \"embedding_features\": X_train_final.shape[1]\n",
    "    - (len(reg_cols) if reg_cols else 0)\n",
    "    - 3,\n",
    "    \"duplicates_removed\": (\n",
    "      len(cols_to_remove) if \"cols_to_remove\" in locals() else 0\n",
    "    ),\n",
    "  }\n",
    "\n",
    "  print(\"\\n‚úÖ Feature review complete\")\n",
    "\n",
    "except Exception as e:\n",
    "  print(f\"‚ö†Ô∏è Error in feature review: {e}\")\n",
    "  import traceback\n",
    "\n",
    "  traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:25:54.876715Z",
     "iopub.status.busy": "2025-11-20T09:25:54.876643Z",
     "iopub.status.idle": "2025-11-20T09:25:55.038929Z",
     "shell.execute_reply": "2025-11-20T09:25:55.038459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 6: Selective Feature Scaling (Temporal)\n",
      "================================================================================\n",
      "\n",
      "üìä Column Analysis:\n",
      " Numeric columns to scale: 118\n",
      " Binary/one-hot columns (preserved): 56\n",
      " ‚úÖ Scaled 118 numeric columns\n",
      " ‚úÖ Preserved 56 binary/one-hot columns\n",
      "\n",
      "‚úÖ Selective feature scaling complete\n",
      " Train shape: (9600, 174)\n",
      " Val shape: (1200, 174)\n",
      " Test shape: (1200, 174)\n",
      "\n",
      "‚è±Ô∏è Feature Scaling Time: 0.03 seconds (0.00 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Identify numeric columns for selective scaling (matching temporal)\n",
    "# Only scale continuous numeric features, preserve binary/one-hot features\n",
    "try:\n",
    "  print(\"\\n\" + \"=\" * 80)\n",
    "  print(\"PHASE 6: Selective Feature Scaling (Temporal)\")\n",
    "  print(\"=\" * 80)\n",
    "  phase_start = time.time()\n",
    "\n",
    "  # Identify numeric columns that should be scaled\n",
    "  # Binary/one-hot features should NOT be scaled\n",
    "  numeric_indices = []\n",
    "  binary_indices = []\n",
    "\n",
    "  # Check each feature column\n",
    "  for i in range(X_train_final.shape[1]):\n",
    "    col_data = X_train_final[:, i]\n",
    "    unique_vals = np.unique(col_data)\n",
    "\n",
    "    # If column only has 0 and 1 (or 0.0 and 1.0), it's binary/one-hot\n",
    "    if len(unique_vals) <= 2 and set(unique_vals).issubset({0, 1, 0.0, 1.0}):\n",
    "      binary_indices.append(i)\n",
    "    else:\n",
    "      # It's a numeric column that should be scaled\n",
    "      numeric_indices.append(i)\n",
    "\n",
    "  print(f\"\\nüìä Column Analysis:\")\n",
    "  print(f\" Numeric columns to scale: {len(numeric_indices)}\")\n",
    "  print(f\" Binary/one-hot columns (preserved): {len(binary_indices)}\")\n",
    "\n",
    "  # Scale only numeric columns (matching temporal approach)\n",
    "  scaler = StandardScaler()\n",
    "\n",
    "  if numeric_indices:\n",
    "    # Scale numeric columns only\n",
    "    X_train_scaled = X_train_final.copy()\n",
    "    X_val_scaled = X_val_final.copy()\n",
    "    X_test_scaled = X_test_final.copy()\n",
    "\n",
    "    X_train_scaled[:, numeric_indices] = scaler.fit_transform(\n",
    "      X_train_final[:, numeric_indices]\n",
    "    )\n",
    "    X_val_scaled[:, numeric_indices] = scaler.transform(\n",
    "      X_val_final[:, numeric_indices]\n",
    "    )\n",
    "    X_test_scaled[:, numeric_indices] = scaler.transform(\n",
    "      X_test_final[:, numeric_indices]\n",
    "    )\n",
    "\n",
    "    print(f\" ‚úÖ Scaled {len(numeric_indices)} numeric columns\")\n",
    "    print(f\" ‚úÖ Preserved {len(binary_indices)} binary/one-hot columns\")\n",
    "  else:\n",
    "    # Fallback: scale all if no numeric columns identified\n",
    "    print(\" ‚ö†Ô∏è No numeric columns identified, scaling all features\")\n",
    "    X_train_scaled = scaler.fit_transform(X_train_final)\n",
    "    X_val_scaled = scaler.transform(X_val_final)\n",
    "    X_test_scaled = scaler.transform(X_test_final)\n",
    "\n",
    "  # Update variable names\n",
    "  X_train = X_train_scaled\n",
    "  X_val = X_val_scaled\n",
    "  X_test = X_test_scaled\n",
    "\n",
    "  phase_time = time.time() - phase_start\n",
    "  print(f\"\\n‚úÖ Selective feature scaling complete\")\n",
    "  print(f\" Train shape: {X_train.shape}\")\n",
    "  print(f\" Val shape: {X_val.shape}\")\n",
    "  print(f\" Test shape: {X_test.shape}\")\n",
    "  print(\n",
    "    f\"\\n‚è±Ô∏è Feature Scaling Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\"\n",
    "  )\n",
    "\n",
    "  del X_train_final, X_val_final, X_test_final\n",
    "  cleanup_memory()\n",
    "  memory_usage()\n",
    "except Exception as e:\n",
    "  print(f\"‚ùå Error in feature scaling: {e}\")\n",
    "  import traceback\n",
    "\n",
    "  traceback.print_exc()\n",
    "  raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Class Imbalance Handling: RandomUnderSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:25:55.040017Z",
     "iopub.status.busy": "2025-11-20T09:25:55.039945Z",
     "iopub.status.idle": "2025-11-20T09:25:55.157573Z",
     "shell.execute_reply": "2025-11-20T09:25:55.157234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 7: Class Imbalance Handling (RandomUnderSampler)\n",
      "================================================================================\n",
      "\n",
      "üìä Before resampling:\n",
      " Train samples: 9600\n",
      " Positive: 648, Negative: 8952\n",
      " Imbalance ratio: 13.81:1\n",
      "\n",
      "‚úÖ RandomUnderSampler complete\n",
      " After resampling:\n",
      " Train samples: 1296\n",
      " Positive: 648, Negative: 648\n",
      " Balance ratio: 1.00:1\n",
      "\n",
      "‚è±Ô∏è Resampling Time: 0.00 seconds (0.00 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Apply RandomUnderSampler (matching temporal)\n",
    "try:\n",
    "  print(\"\\n\" + \"=\" * 80)\n",
    "  print(\"PHASE 7: Class Imbalance Handling (RandomUnderSampler)\")\n",
    "  print(\"=\" * 80)\n",
    "  phase_start = time.time()\n",
    "\n",
    "  print(f\"\\nüìä Before resampling:\")\n",
    "  print(f\" Train samples: {len(X_train)}\")\n",
    "  print(f\" Positive: {y_train.sum()}, Negative: {(y_train==0).sum()}\")\n",
    "  print(f\" Imbalance ratio: {(y_train==0).sum() / max(y_train.sum(), 1):.2f}:1\")\n",
    "\n",
    "  # Apply RandomUnderSampler (matching temporal)\n",
    "  rus = RandomUnderSampler(random_state=SEED)\n",
    "  X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "  phase_time = time.time() - phase_start\n",
    "  print(f\"\\n‚úÖ RandomUnderSampler complete\")\n",
    "  print(f\" After resampling:\")\n",
    "  print(f\" Train samples: {len(X_train_resampled)}\")\n",
    "  print(\n",
    "    f\" Positive: {y_train_resampled.sum()}, Negative: {(y_train_resampled==0).sum()}\"\n",
    "  )\n",
    "  print(\n",
    "    f\" Balance ratio: {(y_train_resampled==0).sum() / max(y_train_resampled.sum(), 1):.2f}:1\"\n",
    "  )\n",
    "  print(\n",
    "    f\"\\n‚è±Ô∏è Resampling Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\"\n",
    "  )\n",
    "\n",
    "  X_train = X_train_resampled\n",
    "  y_train = y_train_resampled\n",
    "\n",
    "  del X_train_resampled, y_train_resampled\n",
    "  cleanup_memory()\n",
    "  memory_usage()\n",
    "except Exception as e:\n",
    "  print(f\"‚ùå Error in resampling: {e}\")\n",
    "  import traceback\n",
    "\n",
    "  traceback.print_exc()\n",
    "  print(\"‚ö†Ô∏è Continuing with original training data...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:25:55.159141Z",
     "iopub.status.busy": "2025-11-20T09:25:55.159053Z",
     "iopub.status.idle": "2025-11-20T09:26:10.203887Z",
     "shell.execute_reply": "2025-11-20T09:26:10.203490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 8: Model Training Pipeline (Temporal)\n",
      "================================================================================\n",
      "\n",
      "üìä Training BernoulliNB...\n",
      " ‚úÖ BernoulliNB - F1: 0.2851, AUC: 0.8649, Time: 0.01s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Training LogisticRegression...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úÖ LogisticRegression - F1: 0.3469, AUC: 0.8832, Time: 1.56s\n",
      "\n",
      "üìä Training DecisionTree...\n",
      " ‚úÖ DecisionTree - F1: 0.2667, AUC: 0.6866, Time: 0.08s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Training RandomForest...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úÖ RandomForest - F1: 0.3411, AUC: 0.8992, Time: 0.37s\n",
      "\n",
      "üìä Training GradientBoosting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úÖ GradientBoosting - F1: 0.3494, AUC: 0.8883, Time: 3.91s\n",
      "\n",
      "üìä Training ExtraTrees...\n",
      " ‚úÖ ExtraTrees - F1: 0.3571, AUC: 0.9055, Time: 0.10s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Training AdaBoost...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úÖ AdaBoost - F1: 0.3282, AUC: 0.8855, Time: 0.53s\n",
      "\n",
      "üìä Training LGBMClassifier...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úÖ LGBMClassifier - F1: 0.3535, AUC: 0.9015, Time: 0.60s\n",
      "\n",
      "üìä Training XGBClassifier...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úÖ XGBClassifier - F1: 0.3515, AUC: 0.9020, Time: 0.20s\n",
      "\n",
      "üìä Training CatBoostClassifier...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úÖ CatBoostClassifier - F1: 0.3706, AUC: 0.9106, Time: 2.90s\n",
      "\n",
      "üìä Training BaggingClassifier...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úÖ BaggingClassifier - F1: 0.3293, AUC: 0.8753, Time: 2.16s\n",
      "\n",
      "üìä Training Perceptron...\n",
      " ‚úÖ Perceptron - F1: 0.3065, AUC: 0.7547, Time: 0.01s\n",
      "\n",
      "üìä Training QuadraticDiscriminantAnalysis...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úÖ QuadraticDiscriminantAnalysis - F1: 0.1770, AUC: 0.6642, Time: 0.01s\n",
      "\n",
      "üìä Training GaussianNB...\n",
      " ‚úÖ GaussianNB - F1: 0.1660, AUC: 0.6381, Time: 0.01s\n",
      "\n",
      "üìä Training LinearDiscriminantAnalysis...\n",
      " ‚úÖ LinearDiscriminantAnalysis - F1: 0.3350, AUC: 0.8771, Time: 0.01s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Training ExtraTreeClassifier...\n",
      " ‚úÖ ExtraTreeClassifier - F1: 0.2922, AUC: 0.8241, Time: 0.00s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Training SGDClassifier...\n",
      " ‚úÖ SGDClassifier - F1: 0.3065, AUC: 0.7473, Time: 0.02s\n",
      "\n",
      "üìä Training DummyClassifier...\n",
      " ‚úÖ DummyClassifier - F1: 0.0000, AUC: 0.5000, Time: 0.00s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Model training complete\n",
      " Trained 18 models\n",
      "\n",
      "‚è±Ô∏è Total Training Time: 14.92 seconds (0.25 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Define models to train (matching temporal: multiple classifiers)\n",
    "# Including all models that are commonly used in temporal notebooks\n",
    "models_to_train = {\n",
    "  \"BernoulliNB\": BernoulliNB(),\n",
    "  \"LogisticRegression\": LogisticRegression(\n",
    "    random_state=SEED, max_iter=1000, n_jobs=2\n",
    "  ),\n",
    "  \"DecisionTree\": DecisionTreeClassifier(random_state=SEED, max_depth=10),\n",
    "  \"RandomForest\": RandomForestClassifier(\n",
    "    n_estimators=100, random_state=SEED, n_jobs=2, max_depth=10\n",
    "  ),\n",
    "  \"GradientBoosting\": GradientBoostingClassifier(\n",
    "    n_estimators=100, random_state=SEED, max_depth=5\n",
    "  ),\n",
    "  \"ExtraTrees\": ExtraTreesClassifier(\n",
    "    n_estimators=100, random_state=SEED, n_jobs=2, max_depth=10\n",
    "  ),\n",
    "  \"AdaBoost\": AdaBoostClassifier(n_estimators=50, random_state=SEED),\n",
    "  \"LGBMClassifier\": LGBMClassifier(random_state=SEED, verbose=-1),\n",
    "  \"XGBClassifier\": XGBClassifier(random_state=SEED, eval_metric='logloss', use_label_encoder=False),\n",
    "  \"CatBoostClassifier\": CatBoostClassifier(random_state=SEED, verbose=False),\n",
    "  \"BaggingClassifier\": BaggingClassifier(random_state=SEED, n_jobs=2),\n",
    "  \"Perceptron\": Perceptron(random_state=SEED),\n",
    "  \"QuadraticDiscriminantAnalysis\": QuadraticDiscriminantAnalysis(),\n",
    "  \"GaussianNB\": GaussianNB(),\n",
    "  \"LinearDiscriminantAnalysis\": LinearDiscriminantAnalysis(),\n",
    "  \"ExtraTreeClassifier\": ExtraTreeClassifier(random_state=SEED, max_depth=10),\n",
    "  \"SGDClassifier\": SGDClassifier(random_state=SEED, n_jobs=2),\n",
    "  \"DummyClassifier\": DummyClassifier(strategy='most_frequent'),\n",
    "}\n",
    "\n",
    "# Train all models\n",
    "trained_models = {}\n",
    "model_scores = {}\n",
    "\n",
    "try:\n",
    "  print(\"\\n\" + \"=\" * 80)\n",
    "  print(\"PHASE 8: Model Training Pipeline (Temporal)\")\n",
    "  print(\"=\" * 80)\n",
    "  phase_start = time.time()\n",
    "\n",
    "  for model_name, model in models_to_train.items():\n",
    "    print(f\"\\nüìä Training {model_name}...\")\n",
    "    model_start = time.time()\n",
    "\n",
    "    try:\n",
    "      # Train model\n",
    "      model.fit(X_train, y_train)\n",
    "\n",
    "      # Evaluate on validation set\n",
    "      y_val_pred = model.predict(X_val)\n",
    "      y_val_proba = (\n",
    "        model.predict_proba(X_val)[:, 1]\n",
    "        if hasattr(model, \"predict_proba\")\n",
    "        else y_val_pred\n",
    "      )\n",
    "\n",
    "      # Calculate metrics\n",
    "      f1 = f1_score(y_val, y_val_pred, zero_division=0)\n",
    "      try:\n",
    "        auc = roc_auc_score(y_val, y_val_proba)\n",
    "      except:\n",
    "        auc = 0.0\n",
    "\n",
    "      trained_models[model_name] = model\n",
    "      model_scores[model_name] = {\n",
    "        \"f1_score\": f1,\n",
    "        \"roc_auc\": auc,\n",
    "        \"train_time\": time.time() - model_start,\n",
    "      }\n",
    "\n",
    "      print(\n",
    "        f\" ‚úÖ {model_name} - F1: {f1:.4f}, AUC: {auc:.4f}, Time: {model_scores[model_name]['train_time']:.2f}s\"\n",
    "      )\n",
    "\n",
    "    except Exception as e:\n",
    "      print(f\" ‚ùå {model_name} failed: {e}\")\n",
    "      continue\n",
    "\n",
    "    cleanup_memory()\n",
    "\n",
    "  phase_time = time.time() - phase_start\n",
    "  print(f\"\\n‚úÖ Model training complete\")\n",
    "  print(f\" Trained {len(trained_models)} models\")\n",
    "  print(\n",
    "    f\"\\n‚è±Ô∏è Total Training Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\"\n",
    "  )\n",
    "\n",
    "  cleanup_memory()\n",
    "  memory_usage()\n",
    "except Exception as e:\n",
    "  print(f\"‚ùå Error in model training: {e}\")\n",
    "  import traceback\n",
    "\n",
    "  traceback.print_exc()\n",
    "  raise\n",
    "\n",
    "  \n",
    "  # Find best model based on F1 score\n",
    "  if model_scores:\n",
    "    best_model_name = max(model_scores, key=lambda k: model_scores[k].get('f1_score', 0))\n",
    "    best_model = trained_models[best_model_name]\n",
    "    best_f1_score = model_scores[best_model_name].get('f1_score', 0)\n",
    "    print(f\"\\nüèÜ Best Model: {best_model_name} (F1: {best_f1_score:.4f})\")\n",
    "  else:\n",
    "    # Fallback: use first model\n",
    "    best_model_name = list(trained_models.keys())[0] if trained_models else \"Unknown\"\n",
    "    best_model = list(trained_models.values())[0] if trained_models else None\n",
    "    print(f\"‚ö†Ô∏è No scores available, using first model: {best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:26:10.205481Z",
     "iopub.status.busy": "2025-11-20T09:26:10.205399Z",
     "iopub.status.idle": "2025-11-20T09:26:10.208509Z",
     "shell.execute_reply": "2025-11-20T09:26:10.207830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /Users/santoshdesai/Documents/Desai_Projects/Kaggle2\n",
      "RESULTS_DIR: /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/results\n",
      "MODEL_SAVE_DIR: /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/models/saved_models\n",
      "SUBMISSION_DIR: /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/submission_files\n"
     ]
    }
   ],
   "source": [
    "# ==============\n",
    "# PATH MANAGEMENT\n",
    "# ==============\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Get project root by finding data/results directory\n",
    "current = Path(os.getcwd())\n",
    "PROJECT_ROOT = current\n",
    "\n",
    "# Search up to 10 levels to find data/results\n",
    "for _ in range(10):\n",
    "    if (PROJECT_ROOT / \"data\" / \"results\").exists():\n",
    "        break\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "else:\n",
    "    # Fallback: go up two levels from current (assuming we're in src/notebooks)\n",
    "    PROJECT_ROOT = current.parent.parent\n",
    "\n",
    "RESULTS_DIR = PROJECT_ROOT / \"data\" / \"results\"\n",
    "MODEL_SAVE_DIR = PROJECT_ROOT / \"models\" / \"saved_models\"\n",
    "SUBMISSION_DIR = PROJECT_ROOT / \"data\" / \"submission_files\"\n",
    "MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SUBMISSION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"RESULTS_DIR:\", RESULTS_DIR)\n",
    "print(\"MODEL_SAVE_DIR:\", MODEL_SAVE_DIR)\n",
    "print(\"SUBMISSION_DIR:\", SUBMISSION_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Threshold Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:26:10.209599Z",
     "iopub.status.busy": "2025-11-20T09:26:10.209517Z",
     "iopub.status.idle": "2025-11-20T09:26:10.437353Z",
     "shell.execute_reply": "2025-11-20T09:26:10.436956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 6: Threshold Tuning\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Threshold tuning complete\n",
      " Best threshold: 0.8434\n",
      " Best F1 score: 0.4737\n",
      "\n",
      "üìä Final Validation Performance:\n",
      " F1 Score: 0.4737\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.95      1119\n",
      "           1       0.41      0.56      0.47        81\n",
      "\n",
      "    accuracy                           0.92      1200\n",
      "   macro avg       0.69      0.75      0.71      1200\n",
      "weighted avg       0.93      0.92      0.92      1200\n",
      "\n",
      "\n",
      "‚è±Ô∏è Threshold Tuning Time: 0.08 seconds (0.00 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Find optimal threshold for best model\n",
    "try:\n",
    "  print(\"\\n\" + \"=\" * 80)\n",
    "  print(\"PHASE 6: Threshold Tuning\")\n",
    "  print(\"=\" * 80)\n",
    "  phase_start = time.time()\n",
    "\n",
    "  # Get predictions from best model\n",
    "  if 'best_model' not in locals() or best_model is None:\n",
    "    if 'trained_models' in locals() and trained_models:\n",
    "      if 'model_scores' in locals() and model_scores:\n",
    "        best_model_name = max(model_scores, key=lambda k: model_scores[k].get('f1_score', 0))\n",
    "        best_model = trained_models[best_model_name]\n",
    "      else:\n",
    "        best_model_name = list(trained_models.keys())[0]\n",
    "        best_model = trained_models[best_model_name]\n",
    "    else:\n",
    "      raise ValueError(\"No models available for threshold tuning!\")\n",
    "  \n",
    "  y_val_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "  # Find optimal threshold using precision-recall curve\n",
    "  precision, recall, pr_thresholds = precision_recall_curve(y_val, y_val_proba)\n",
    "  f1_scores_pr = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "  best_pr_idx = np.argmax(f1_scores_pr)\n",
    "  best_pr_threshold = (\n",
    "    pr_thresholds[best_pr_idx] if best_pr_idx < len(pr_thresholds) else 0.5\n",
    "  )\n",
    "  best_pr_f1 = f1_scores_pr[best_pr_idx]\n",
    "\n",
    "  # Fine-grained search\n",
    "  thresholds = np.concatenate(\n",
    "    [\n",
    "      np.linspace(0.01, 0.05, 20),\n",
    "      np.linspace(0.05, 0.15, 50),\n",
    "      np.linspace(0.15, 0.3, 30),\n",
    "      np.linspace(0.3, 0.9, 20),\n",
    "    ]\n",
    "  )\n",
    "\n",
    "  best_threshold = best_pr_threshold\n",
    "  best_f1 = best_pr_f1\n",
    "\n",
    "  for thr in thresholds:\n",
    "    y_pred = (y_val_proba >= thr).astype(int)\n",
    "    f1 = f1_score(y_val, y_pred, pos_label=1, zero_division=0)\n",
    "    if f1 > best_f1:\n",
    "      best_f1 = f1\n",
    "      best_threshold = thr\n",
    "\n",
    "  print(f\"\\n‚úÖ Threshold tuning complete\")\n",
    "  print(f\" Best threshold: {best_threshold:.4f}\")\n",
    "  print(f\" Best F1 score: {best_f1:.4f}\")\n",
    "\n",
    "  # Evaluate with optimal threshold\n",
    "  y_val_pred_optimal = (y_val_proba >= best_threshold).astype(int)\n",
    "  final_f1 = f1_score(y_val, y_val_pred_optimal, zero_division=0)\n",
    "\n",
    "  print(f\"\\nüìä Final Validation Performance:\")\n",
    "  print(f\" F1 Score: {final_f1:.4f}\")\n",
    "  print(classification_report(y_val, y_val_pred_optimal, zero_division=0))\n",
    "\n",
    "  phase_time = time.time() - phase_start\n",
    "  print(\n",
    "    f\"\\n‚è±Ô∏è Threshold Tuning Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\"\n",
    "  )\n",
    "\n",
    "  cleanup_memory()\n",
    "except Exception as e:\n",
    "  print(f\"‚ùå Error in threshold tuning: {e}\")\n",
    "  import traceback\n",
    "\n",
    "  traceback.print_exc()\n",
    "  best_threshold = 0.5\n",
    "  print(f\"‚ö†Ô∏è Using default threshold: {best_threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:26:10.438674Z",
     "iopub.status.busy": "2025-11-20T09:26:10.438598Z",
     "iopub.status.idle": "2025-11-20T09:26:10.441981Z",
     "shell.execute_reply": "2025-11-20T09:26:10.441228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üéâ PIPELINE EXECUTION COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "‚è±Ô∏è Total Execution Time: 20.23 seconds (0.34 minutes)\n",
      "\n",
      "üìä Summary:\n",
      " Best Model: CatBoostClassifier\n",
      " Best F1 Score: 0.4737\n",
      " Optimal Threshold: 0.8434\n",
      " Models Trained: 18\n",
      "\n",
      "üìã Feature Summary:\n",
      " Original features: 1977\n",
      " Final features: 174\n",
      " Regular features: 96\n",
      " Temporal features: 3\n",
      " Embedding features (PCA): 75\n",
      " Duplicates removed: 18\n",
      "\n",
      "üíæ Outputs:\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Final summary\n",
    "total_time = time.time() - TOTAL_START_TIME\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéâ PIPELINE EXECUTION COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(\n",
    "  f\"\\n‚è±Ô∏è Total Execution Time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\"\n",
    ")\n",
    "print(f\"\\nüìä Summary:\")\n",
    "# Safely get variables with defaults\n",
    "best_model_name_val = locals().get(\"best_model_name\", \"N/A\")\n",
    "best_f1_val = locals().get(\"best_f1\", 0.0)\n",
    "best_threshold_val = locals().get(\"best_threshold\", 0.5)\n",
    "trained_models_val = locals().get(\"trained_models\", {})\n",
    "print(f\" Best Model: {best_model_name_val}\")\n",
    "print(f\" Best F1 Score: {best_f1_val:.4f}\")\n",
    "print(f\" Optimal Threshold: {best_threshold_val:.4f}\")\n",
    "print(f\" Models Trained: {len(trained_models_val)}\")\n",
    "print(f\"\\nüìã Feature Summary:\")\n",
    "if \"feature_review\" in locals():\n",
    "  print(f\" Original features: {feature_review.get('original_features', 'N/A')}\")\n",
    "  print(f\" Final features: {feature_review.get('final_features', 'N/A')}\")\n",
    "  print(f\" Regular features: {feature_review.get('regular_features', 'N/A')}\")\n",
    "  print(f\" Temporal features: {feature_review.get('temporal_features', 'N/A')}\")\n",
    "  print(f\" Embedding features (PCA): {feature_review.get('embedding_features', 'N/A')}\")\n",
    "  print(f\" Duplicates removed: {feature_review.get('duplicates_removed', 'N/A')}\")\n",
    "print(f\"\\nüíæ Outputs:\")\n",
    "if \"model_save_path\" in locals():\n",
    "  print(f\" Model: {model_save_path}\")\n",
    "if \"submission_path\" in locals():\n",
    "  print(f\" Submission: {submission_path}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:26:10.443214Z",
     "iopub.status.busy": "2025-11-20T09:26:10.443117Z",
     "iopub.status.idle": "2025-11-20T09:26:10.448292Z",
     "shell.execute_reply": "2025-11-20T09:26:10.447713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 7: Save Model\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Model saved to: /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/models/saved_models/model_catboostclassifier_leakage_test.pkl\n",
      " Model: CatBoostClassifier\n",
      " Threshold: 0.8434\n",
      " F1 Score: 0.4737\n"
     ]
    }
   ],
   "source": [
    "# Save best model and scaler\n",
    "try:\n",
    "  print(\"\\n\" + \"=\" * 80)\n",
    "  print(\"PHASE 7: Save Model\")\n",
    "  print(\"=\" * 80)\n",
    "\n",
    "  # Ensure best_model_name is set\n",
    "  if 'best_model_name' not in locals() or best_model_name is None:\n",
    "    if 'trained_models' in locals() and trained_models:\n",
    "      if 'model_scores' in locals() and model_scores:\n",
    "        best_model_name = max(model_scores, key=lambda k: model_scores[k].get('f1_score', 0))\n",
    "      else:\n",
    "        best_model_name = list(trained_models.keys())[0]\n",
    "    else:\n",
    "      best_model_name = \"unknown\"\n",
    "  \n",
    "  if 'best_model' not in locals() or best_model is None:\n",
    "    if 'trained_models' in locals() and trained_models:\n",
    "      best_model = trained_models.get(best_model_name, list(trained_models.values())[0])\n",
    "    else:\n",
    "      raise ValueError(\"No model available to save!\")\n",
    "  \n",
    "  model_save_path = (\n",
    "    MODEL_SAVE_DIR / f\"model_{best_model_name.lower()}_leakage_test.pkl\"\n",
    "  )\n",
    "\n",
    "  model_data = {\n",
    "    \"model\": best_model,\n",
    "    \"scaler\": scaler,\n",
    "    \"model_name\": best_model_name,\n",
    "    \"best_threshold\": best_threshold,\n",
    "    \"best_f1\": best_f1,\n",
    "    \"feature_count\": X_train.shape[1],\n",
    "    \"train_samples\": len(X_train),\n",
    "    \"val_samples\": len(X_val),\n",
    "    \"timestamp\": START_TIME_STR,\n",
    "  }\n",
    "\n",
    "  with open(model_save_path, \"wb\") as f:\n",
    "    pickle.dump(model_data, f)\n",
    "\n",
    "  print(f\"\\n‚úÖ Model saved to: {model_save_path}\")\n",
    "  print(f\" Model: {best_model_name}\")\n",
    "  print(f\" Threshold: {best_threshold:.4f}\")\n",
    "  print(f\" F1 Score: {best_f1:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "  print(f\"‚ùå Error saving model: {e}\")\n",
    "  import traceback\n",
    "\n",
    "  traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:26:10.449356Z",
     "iopub.status.busy": "2025-11-20T09:26:10.449288Z",
     "iopub.status.idle": "2025-11-20T09:26:10.484371Z",
     "shell.execute_reply": "2025-11-20T09:26:10.483965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 8: Generate Submission\n",
      "================================================================================\n",
      "Generating test predictions...\n",
      " Predictions produced: 1200\n",
      " Positive predictions: 93\n",
      " Negative predictions: 1107\n",
      "\n",
      "‚úÖ Submission generated\n",
      " File: /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/submission_files/submission_catboostclassifier_leakage_test.csv\n",
      " Samples: 1200\n",
      "\n",
      "‚è±Ô∏è Submission Generation Time: 0.03 seconds (0.00 minutes)\n",
      "\n",
      "üìã Submission Sample (first 10 rows):\n",
      "shape: (10, 2)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ work_id     ‚îÜ label ‚îÇ\n",
      "‚îÇ ---         ‚îÜ ---   ‚îÇ\n",
      "‚îÇ str         ‚îÜ i64   ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ W3183595028 ‚îÜ 1     ‚îÇ\n",
      "‚îÇ W4281846540 ‚îÜ 0     ‚îÇ\n",
      "‚îÇ W3041108599 ‚îÜ 0     ‚îÇ\n",
      "‚îÇ W3177215330 ‚îÜ 1     ‚îÇ\n",
      "‚îÇ W4398934184 ‚îÜ 0     ‚îÇ\n",
      "‚îÇ W3043702115 ‚îÜ 0     ‚îÇ\n",
      "‚îÇ W3105126562 ‚îÜ 1     ‚îÇ\n",
      "‚îÇ W4212866133 ‚îÜ 0     ‚îÇ\n",
      "‚îÇ W3129363439 ‚îÜ 0     ‚îÇ\n",
      "‚îÇ W4256661485 ‚îÜ 0     ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
     ]
    }
   ],
   "source": [
    "# Generate test predictions and submission file\n",
    "try:\n",
    "  print(\"\\n\" + \"=\" * 80)\n",
    "  print(\"PHASE 8: Generate Submission\")\n",
    "  print(\"=\" * 80)\n",
    "  phase_start = time.time()\n",
    "\n",
    "  # Get test predictions\n",
    "  print(\"Generating test predictions...\")\n",
    "  # Get best model from trained_models if available\n",
    "  best_model = locals().get(\"best_model\", None)\n",
    "  if best_model is None and \"trained_models\" in locals() and trained_models:\n",
    "    # Use first model as fallback\n",
    "      best_model = list(trained_models.values())[0]\n",
    "      print(\"‚ö†Ô∏è Using first trained model as best_model\")\n",
    "  if best_model is None:\n",
    "    raise ValueError(\"No model available for prediction!\")\n",
    "  y_test_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "  best_threshold_val = locals().get(\"best_threshold\", 0.5)\n",
    "  y_test_pred = (y_test_proba >= best_threshold_val).astype(int)\n",
    "\n",
    "  print(f\" Predictions produced: {len(y_test_pred)}\")\n",
    "  print(f\" Positive predictions: {y_test_pred.sum()}\")\n",
    "  print(f\" Negative predictions: {(y_test_pred == 0).sum()}\")\n",
    "\n",
    "  # Extract work_id from test IDs\n",
    "  def extract_work_id(id_value: str) -> str:\n",
    "    \"\"\"Extract work_id from OpenAlex ID format.\"\"\"\n",
    "    if isinstance(id_value, str):\n",
    "      if id_value.startswith(\"https://openalex.org/\"):\n",
    "        return id_value.replace(\"https://openalex.org/\", \"\")\n",
    "      elif id_value.startswith(\"W\"):\n",
    "        return id_value\n",
    "    return str(id_value)\n",
    "\n",
    "  # Get work_ids for test set\n",
    "  if work_ids_test is not None:\n",
    "    test_work_ids = [extract_work_id(wid) for wid in work_ids_test]\n",
    "  else:\n",
    "    # Fallback: generate sequential IDs\n",
    "    test_work_ids = [f\"W{i:010d}\" for i in range(len(y_test_pred))]\n",
    "    print(\" ‚ö†Ô∏è Using generated work_ids (original IDs not found)\")\n",
    "\n",
    "  # Create submission DataFrame\n",
    "  submission_df = pl.DataFrame({\"work_id\": test_work_ids, \"label\": y_test_pred})\n",
    "\n",
    "  # Save submission\n",
    "  best_model_name_val = locals().get(\"best_model_name\", \"unknown\")\n",
    "  submission_path = (\n",
    "    SUBMISSION_DIR / f\"submission_{best_model_name_val.lower()}_leakage_test.csv\"\n",
    "  )\n",
    "  submission_df.write_csv(submission_path)\n",
    "\n",
    "  phase_time = time.time() - phase_start\n",
    "  print(f\"\\n‚úÖ Submission generated\")\n",
    "  print(f\" File: {submission_path}\")\n",
    "  print(f\" Samples: {len(submission_df)}\")\n",
    "  print(\n",
    "    f\"\\n‚è±Ô∏è Submission Generation Time: {phase_time:.2f} seconds ({phase_time/60:.2f} minutes)\"\n",
    "  )\n",
    "\n",
    "  # Display sample\n",
    "  print(\"\\nüìã Submission Sample (first 10 rows):\")\n",
    "  print(submission_df.head(10))\n",
    "\n",
    "except Exception as e:\n",
    "  print(f\"‚ùå Error generating submission: {e}\")\n",
    "  import traceback\n",
    "\n",
    "  traceback.print_exc()\n",
    "  raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T09:26:10.485328Z",
     "iopub.status.busy": "2025-11-20T09:26:10.485257Z",
     "iopub.status.idle": "2025-11-20T09:26:10.487995Z",
     "shell.execute_reply": "2025-11-20T09:26:10.487737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üéâ PIPELINE EXECUTION COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "‚è±Ô∏è Total Execution Time: 20.27 seconds (0.34 minutes)\n",
      "\n",
      "üìä Summary:\n",
      " Best Model: CatBoostClassifier\n",
      " Best F1 Score: 0.4737\n",
      " Optimal Threshold: 0.8434\n",
      " Models Trained: 18\n",
      "\n",
      "üìã Feature Summary:\n",
      " Original features: 1977\n",
      " Final features: 174\n",
      " Regular features: 96\n",
      " Temporal features: 3\n",
      " Embedding features (PCA): 75\n",
      " Duplicates removed: 18\n",
      "\n",
      "üíæ Outputs:\n",
      " Model: /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/models/saved_models/model_catboostclassifier_leakage_test.pkl\n",
      " Submission: /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/submission_files/submission_catboostclassifier_leakage_test.csv\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Final summary\n",
    "total_time = time.time() - TOTAL_START_TIME\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéâ PIPELINE EXECUTION COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(\n",
    "  f\"\\n‚è±Ô∏è Total Execution Time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\"\n",
    ")\n",
    "print(f\"\\nüìä Summary:\")\n",
    "# Safely get variables with defaults\n",
    "best_model_name_val = locals().get(\"best_model_name\", \"N/A\")\n",
    "best_f1_val = locals().get(\"best_f1\", 0.0)\n",
    "best_threshold_val = locals().get(\"best_threshold\", 0.5)\n",
    "trained_models_val = locals().get(\"trained_models\", {})\n",
    "print(f\" Best Model: {best_model_name_val}\")\n",
    "print(f\" Best F1 Score: {best_f1_val:.4f}\")\n",
    "print(f\" Optimal Threshold: {best_threshold_val:.4f}\")\n",
    "print(f\" Models Trained: {len(trained_models_val)}\")\n",
    "print(f\"\\nüìã Feature Summary:\")\n",
    "if \"feature_review\" in locals():\n",
    "  print(f\" Original features: {feature_review.get('original_features', 'N/A')}\")\n",
    "  print(f\" Final features: {feature_review.get('final_features', 'N/A')}\")\n",
    "  print(f\" Regular features: {feature_review.get('regular_features', 'N/A')}\")\n",
    "  print(f\" Temporal features: {feature_review.get('temporal_features', 'N/A')}\")\n",
    "  print(f\" Embedding features (PCA): {feature_review.get('embedding_features', 'N/A')}\")\n",
    "  print(f\" Duplicates removed: {feature_review.get('duplicates_removed', 'N/A')}\")\n",
    "print(f\"\\nüíæ Outputs:\")\n",
    "if \"model_save_path\" in locals():\n",
    "  print(f\" Model: {model_save_path}\")\n",
    "if \"submission_path\" in locals():\n",
    "  print(f\" Submission: {submission_path}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
