{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c84e1595",
   "metadata": {},
   "source": [
    "# Data Exploration: Next Steps (Polars-only, Scalable)\n",
    "\n",
    "This notebook assumes that you have already run **`data_exploration_organized.ipynb`**.\n",
    "\n",
    "From that notebook, the following outputs are expected in `data/results`:\n",
    "\n",
    "- Base tabular features: `X_train.parquet`, `X_val.parquet`, `X_test.parquet`\n",
    "- Labels (if supervised): `y_train.npy`, `y_val.npy`\n",
    "- NLP embedding features, per model and split, for example:\n",
    "  - `sent_transformer_X_train.parquet`, `sent_transformer_X_val.parquet`, `sent_transformer_X_test.parquet`\n",
    "  - `scibert_X_train.parquet`, `scibert_X_val.parquet`, `scibert_X_test.parquet`\n",
    "  - (optionally) `specter2_X_train.parquet`, `specter2_X_val.parquet`, `specter2_X_test.parquet`\n",
    "\n",
    "Each embedding parquet contains an `id` column plus one column per embedding dimension\n",
    "(e.g. `sent_transformer_dim_0`, `sent_transformer_dim_1`, ...).\n",
    "\n",
    "This notebook will:\n",
    "\n",
    "1. Load the **base feature matrices** and all available **embedding parquets**\n",
    "2. Merge embeddings into the base feature matrices by `id`\n",
    "3. Inspect and impute missing values in a train-centric way\n",
    "4. Replace `is_oa` with derived feature `is_not_oa` (1 - is_oa) for better signal\n",
    "5. Run a set of statistical analyses (Pearson, Spearman, Chi-square, Cram√©r's V, ANOVA, Tukey's HSD)\n",
    "6. Keep **all non-embedding features** (no feature reduction) to preserve signals that may be sparse in sample but informative in full dataset\n",
    "7. Save model-ready train / validation / test parquet files for downstream ML\n",
    "\n",
    "Implementation is **Polars-only** for dataframes (no pandas). NumPy, SciPy and statsmodels are used only for\n",
    "vectorised numerical routines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c919c96c",
   "metadata": {},
   "source": [
    "## 1. Imports & directory configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb87d44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T19:48:07.167446Z",
     "iopub.status.busy": "2025-11-17T19:48:07.167204Z",
     "iopub.status.idle": "2025-11-17T19:48:08.247600Z",
     "shell.execute_reply": "2025-11-17T19:48:08.247291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /Users/santoshdesai/Documents/Desai_Projects/Kaggle2\n",
      "DATA_DIR:     /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/processed\n",
      "RESULTS_DIR:  /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/results\n",
      "MODEL_DIR:    /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/model_ready\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Optional: statsmodels for Tukey's HSD\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "    HAS_STATSMODELS = True\n",
    "except ImportError:\n",
    "    HAS_STATSMODELS = False\n",
    "    print(\"‚ö†Ô∏è statsmodels not found. Tukey's HSD will be skipped unless you install statsmodels.\")\n",
    "\n",
    "_current_dir = Path(os.getcwd()).parent.parent\n",
    "if (_current_dir / \"data\").exists():\n",
    "    PROJECT_ROOT = _current_dir\n",
    "else:\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "DATA_DIR    = PROJECT_ROOT / \"data/processed\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"data/results\"\n",
    "MODEL_DIR   = PROJECT_ROOT / \"data/model_ready\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")\n",
    "print(f\"DATA_DIR:     {DATA_DIR}\")\n",
    "print(f\"RESULTS_DIR:  {RESULTS_DIR}\")\n",
    "print(f\"MODEL_DIR:    {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cc70a5",
   "metadata": {},
   "source": [
    "## 2. Load base feature matrices & labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd3d4c1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T19:48:08.248972Z",
     "iopub.status.busy": "2025-11-17T19:48:08.248856Z",
     "iopub.status.idle": "2025-11-17T19:48:08.295886Z",
     "shell.execute_reply": "2025-11-17T19:48:08.295539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base feature file locations:\n",
      "   /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/results/X_train.parquet\n",
      "   /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/results/X_val.parquet\n",
      "   /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/results/X_test.parquet\n",
      "\n",
      "Base shapes (before merging embeddings):\n",
      "  X_train_base: (480, 57)\n",
      "  X_val_base:   (60, 57)\n",
      "  X_test_base:  (60, 57)\n",
      "\n",
      "Loaded labels:\n",
      "  y_train: (480,)\n",
      "  y_val:   (60,)\n"
     ]
    }
   ],
   "source": [
    "X_train_path = RESULTS_DIR / \"X_train.parquet\"\n",
    "X_val_path   = RESULTS_DIR / \"X_val.parquet\"\n",
    "X_test_path  = RESULTS_DIR / \"X_test.parquet\"\n",
    "\n",
    "print(\"Base feature file locations:\")\n",
    "print(\"  \", X_train_path)\n",
    "print(\"  \", X_val_path)\n",
    "print(\"  \", X_test_path)\n",
    "\n",
    "if not X_train_path.exists() or not X_val_path.exists() or not X_test_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"One or more X_*.parquet files are missing in data/results. \"\n",
    "        \"Please run data_exploration_organized.ipynb first.\"\n",
    "    )\n",
    "\n",
    "X_train_base = pl.read_parquet(X_train_path)\n",
    "X_val_base   = pl.read_parquet(X_val_path)\n",
    "X_test_base  = pl.read_parquet(X_test_path)\n",
    "\n",
    "print(\"\\nBase shapes (before merging embeddings):\")\n",
    "print(\"  X_train_base:\", X_train_base.shape)\n",
    "print(\"  X_val_base:  \", X_val_base.shape)\n",
    "print(\"  X_test_base: \", X_test_base.shape)\n",
    "\n",
    "y_train_path = RESULTS_DIR / \"y_train.npy\"\n",
    "y_val_path   = RESULTS_DIR / \"y_val.npy\"\n",
    "\n",
    "y_train = np.load(y_train_path) if y_train_path.exists() else None\n",
    "y_val   = np.load(y_val_path)   if y_val_path.exists()   else None\n",
    "\n",
    "if y_train is not None:\n",
    "    print(\"\\nLoaded labels:\")\n",
    "    print(\"  y_train:\", y_train.shape)\n",
    "    if y_val is not None:\n",
    "        print(\"  y_val:  \", y_val.shape)\n",
    "else:\n",
    "    print(\"\\nNo y_train.npy found. Target-based analyses will be limited.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3553d1",
   "metadata": {},
   "source": [
    "## 3. Discover and merge embedding parquets by `id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa10d23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T19:48:08.296944Z",
     "iopub.status.busy": "2025-11-17T19:48:08.296876Z",
     "iopub.status.idle": "2025-11-17T19:48:08.387975Z",
     "shell.execute_reply": "2025-11-17T19:48:08.387603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered embedding models (based on *_X_train.parquet):\n",
      "['scibert', 'sent_transformer', 'specter2']\n",
      "\n",
      "üîó Merging embeddings for model: scibert\n",
      "  train: 57 -> 825 columns after merging scibert\n",
      "  val:   57 -> 825 columns after merging scibert\n",
      "  test:  57 -> 825 columns after merging scibert\n",
      "\n",
      "üîó Merging embeddings for model: sent_transformer\n",
      "  train: 825 -> 1209 columns after merging sent_transformer\n",
      "  val:   825 -> 1209 columns after merging sent_transformer\n",
      "  test:  825 -> 1209 columns after merging sent_transformer\n",
      "\n",
      "üîó Merging embeddings for model: specter2\n",
      "  train: 1209 -> 1977 columns after merging specter2\n",
      "  val:   1209 -> 1977 columns after merging specter2\n",
      "  test:  1209 -> 1977 columns after merging specter2\n",
      "\n",
      "Shapes AFTER embedding merge:\n",
      "  X_train: (480, 1977)\n",
      "  X_val:   (60, 1977)\n",
      "  X_test:  (60, 1977)\n"
     ]
    }
   ],
   "source": [
    "def discover_embedding_models(results_dir: Path) -> List[str]:\n",
    "    models: List[str] = []\n",
    "    for p in results_dir.glob(\"*_X_train.parquet\"):\n",
    "        name = p.name.replace(\"_X_train.parquet\", \"\")\n",
    "        if name in {\"X\", \"base\", \"features\"}:\n",
    "            continue\n",
    "        models.append(name)\n",
    "    return sorted(set(models))\n",
    "\n",
    "embedding_models = discover_embedding_models(RESULTS_DIR)\n",
    "print(\"Discovered embedding models (based on *_X_train.parquet):\")\n",
    "print(embedding_models if embedding_models else \"(none)\")\n",
    "\n",
    "def load_embedding_split(model_name: str, split: str) -> pl.DataFrame:\n",
    "    path = RESULTS_DIR / f\"{model_name}_X_{split}.parquet\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Expected embedding file not found: {path}\")\n",
    "    df = pl.read_parquet(path)\n",
    "    if \"id\" not in df.columns:\n",
    "        raise ValueError(f\"Embedding file {path} does not contain an 'id' column.\")\n",
    "    cols = [\"id\"] + [c for c in df.columns if c != \"id\"]\n",
    "    return df.select(cols)\n",
    "\n",
    "def merge_embeddings(\n",
    "    X_train_base: pl.DataFrame,\n",
    "    X_val_base: pl.DataFrame,\n",
    "    X_test_base: pl.DataFrame,\n",
    "    models: List[str],\n",
    ") -> tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:\n",
    "    X_train = X_train_base\n",
    "    X_val   = X_val_base\n",
    "    X_test  = X_test_base\n",
    "\n",
    "    if \"id\" not in X_train.columns:\n",
    "        raise ValueError(\"Base feature matrices must contain an 'id' column for merging embeddings.\")\n",
    "\n",
    "    for model_name in models:\n",
    "        # Check if all required files exist for this model\n",
    "        required_files = [\n",
    "            RESULTS_DIR / f\"{model_name}_X_train.parquet\",\n",
    "            RESULTS_DIR / f\"{model_name}_X_val.parquet\",\n",
    "            RESULTS_DIR / f\"{model_name}_X_test.parquet\"\n",
    "        ]\n",
    "        missing_files = [f for f in required_files if not f.exists()]\n",
    "        \n",
    "        if missing_files:\n",
    "            print(f\"\\n‚ö†Ô∏è  Skipping {model_name} embeddings - missing files:\")\n",
    "            for f in missing_files:\n",
    "                print(f\"     {f.name}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nüîó Merging embeddings for model: {model_name}\")\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            try:\n",
    "                emb_df = load_embedding_split(model_name, split)\n",
    "                if split == \"train\":\n",
    "                    before_cols = len(X_train.columns)\n",
    "                    X_train = X_train.join(emb_df, on=\"id\", how=\"left\")\n",
    "                    after_cols = len(X_train.columns)\n",
    "                    print(f\"  train: {before_cols} -> {after_cols} columns after merging {model_name}\")\n",
    "                elif split == \"val\":\n",
    "                    before_cols = len(X_val.columns)\n",
    "                    X_val = X_val.join(emb_df, on=\"id\", how=\"left\")\n",
    "                    after_cols = len(X_val.columns)\n",
    "                    print(f\"  val:   {before_cols} -> {after_cols} columns after merging {model_name}\")\n",
    "                else:\n",
    "                    before_cols = len(X_test.columns)\n",
    "                    X_test = X_test.join(emb_df, on=\"id\", how=\"left\")\n",
    "                    after_cols = len(X_test.columns)\n",
    "                    print(f\"  test:  {before_cols} -> {after_cols} columns after merging {model_name}\")\n",
    "            except (FileNotFoundError, ValueError) as e:\n",
    "                print(f\"  ‚ö†Ô∏è  Skipping {split} split for {model_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "if embedding_models:\n",
    "    X_train, X_val, X_test = merge_embeddings(X_train_base, X_val_base, X_test_base, embedding_models)\n",
    "else:\n",
    "    print(\"\\nNo embedding parquets discovered; using base features only.\")\n",
    "    X_train, X_val, X_test = X_train_base, X_val_base, X_test_base\n",
    "\n",
    "print(\"\\nShapes AFTER embedding merge:\")\n",
    "print(\"  X_train:\", X_train.shape)\n",
    "print(\"  X_val:  \", X_val.shape)\n",
    "print(\"  X_test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9303b9",
   "metadata": {},
   "source": [
    "## 4. Dataset overview & missingness diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b62b2b57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T19:48:08.389039Z",
     "iopub.status.busy": "2025-11-17T19:48:08.388962Z",
     "iopub.status.idle": "2025-11-17T19:48:08.429583Z",
     "shell.execute_reply": "2025-11-17T19:48:08.429291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Overview: X_train (with embeddings)\n",
      "================================================================================\n",
      "Shape: (480, 1977)\n",
      "\n",
      "Dtypes:\n",
      "[Int64, Int64, Float64, Float64, Float64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, String, Int64, Int64, Int64, String, Int64, Float64, Int64, Float64, Float64, Float64, Int64, Float64, Float64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, String, Int64, Int64, Int64, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32]\n",
      "\n",
      "Head:\n",
      "shape: (5, 1_977)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ abstract_ ‚îÜ abstract_ ‚îÜ avg_autho ‚îÜ avg_autho ‚îÜ ‚Ä¶ ‚îÜ specter2_ ‚îÜ specter2_ ‚îÜ specter2_ ‚îÜ specter2 ‚îÇ\n",
      "‚îÇ length    ‚îÜ word_coun ‚îÜ r_citatio ‚îÜ r_h_index ‚îÜ   ‚îÜ dim_96    ‚îÜ dim_97    ‚îÜ dim_98    ‚îÜ _dim_99  ‚îÇ\n",
      "‚îÇ ---       ‚îÜ t         ‚îÜ ns        ‚îÜ ---       ‚îÜ   ‚îÜ ---       ‚îÜ ---       ‚îÜ ---       ‚îÜ ---      ‚îÇ\n",
      "‚îÇ i64       ‚îÜ ---       ‚îÜ ---       ‚îÜ f64       ‚îÜ   ‚îÜ f32       ‚îÜ f32       ‚îÜ f32       ‚îÜ f32      ‚îÇ\n",
      "‚îÇ           ‚îÜ i64       ‚îÜ f64       ‚îÜ           ‚îÜ   ‚îÜ           ‚îÜ           ‚îÜ           ‚îÜ          ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ 1149      ‚îÜ 206       ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ 0.06904   ‚îÜ 0.22483   ‚îÜ -0.131973 ‚îÜ 0.621468 ‚îÇ\n",
      "‚îÇ 1190      ‚îÜ 166       ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -0.684604 ‚îÜ -0.052904 ‚îÜ -0.526493 ‚îÜ 0.429081 ‚îÇ\n",
      "‚îÇ 1946      ‚îÜ 268       ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -0.313982 ‚îÜ 0.503523  ‚îÜ 0.365675  ‚îÜ 0.293453 ‚îÇ\n",
      "‚îÇ 1105      ‚îÜ 157       ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -0.847443 ‚îÜ -0.104007 ‚îÜ -0.676101 ‚îÜ 0.103466 ‚îÇ\n",
      "‚îÇ 648       ‚îÜ 94        ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -0.503381 ‚îÜ -0.028732 ‚îÜ 0.359454  ‚îÜ 0.122353 ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "================================================================================\n",
      "Overview: X_val (with embeddings)\n",
      "================================================================================\n",
      "Shape: (60, 1977)\n",
      "\n",
      "Dtypes:\n",
      "[Int64, Int64, Float64, Float64, Float64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, String, Int64, Int64, Int64, String, Int64, Float64, Int64, Float64, Float64, Float64, Int64, Float64, Float64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, String, Int64, Int64, Int64, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32]\n",
      "\n",
      "Head:\n",
      "shape: (5, 1_977)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ abstract_ ‚îÜ abstract_ ‚îÜ avg_autho ‚îÜ avg_autho ‚îÜ ‚Ä¶ ‚îÜ specter2_ ‚îÜ specter2_ ‚îÜ specter2_ ‚îÜ specter2 ‚îÇ\n",
      "‚îÇ length    ‚îÜ word_coun ‚îÜ r_citatio ‚îÜ r_h_index ‚îÜ   ‚îÜ dim_96    ‚îÜ dim_97    ‚îÜ dim_98    ‚îÜ _dim_99  ‚îÇ\n",
      "‚îÇ ---       ‚îÜ t         ‚îÜ ns        ‚îÜ ---       ‚îÜ   ‚îÜ ---       ‚îÜ ---       ‚îÜ ---       ‚îÜ ---      ‚îÇ\n",
      "‚îÇ i64       ‚îÜ ---       ‚îÜ ---       ‚îÜ f64       ‚îÜ   ‚îÜ f32       ‚îÜ f32       ‚îÜ f32       ‚îÜ f32      ‚îÇ\n",
      "‚îÇ           ‚îÜ i64       ‚îÜ f64       ‚îÜ           ‚îÜ   ‚îÜ           ‚îÜ           ‚îÜ           ‚îÜ          ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ 1545      ‚îÜ 230       ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -0.972821 ‚îÜ -0.330806 ‚îÜ -0.020504 ‚îÜ -0.58438 ‚îÇ\n",
      "‚îÇ           ‚îÜ           ‚îÜ           ‚îÜ           ‚îÜ   ‚îÜ           ‚îÜ           ‚îÜ           ‚îÜ 9        ‚îÇ\n",
      "‚îÇ 0         ‚îÜ 0         ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -0.379097 ‚îÜ 0.146527  ‚îÜ -0.106909 ‚îÜ 0.531452 ‚îÇ\n",
      "‚îÇ 0         ‚îÜ 0         ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -0.316178 ‚îÜ -0.550159 ‚îÜ 0.325088  ‚îÜ 0.773191 ‚îÇ\n",
      "‚îÇ 595       ‚îÜ 84        ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -0.950184 ‚îÜ -0.050371 ‚îÜ 0.370916  ‚îÜ 0.389277 ‚îÇ\n",
      "‚îÇ 549       ‚îÜ 66        ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -0.742922 ‚îÜ -0.443554 ‚îÜ 0.009811  ‚îÜ 0.497528 ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "================================================================================\n",
      "Overview: X_test (with embeddings)\n",
      "================================================================================\n",
      "Shape: (60, 1977)\n",
      "\n",
      "Dtypes:\n",
      "[Int64, Int64, Float64, Float64, Float64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, String, Int64, Int64, Int64, String, Int64, Float64, Int64, Float64, Float64, Float64, Int64, Float64, Float64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, Int64, String, Int64, Int64, Int64, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32, Float32]\n",
      "\n",
      "Head:\n",
      "shape: (5, 1_977)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ abstract_ ‚îÜ abstract_ ‚îÜ avg_autho ‚îÜ avg_autho ‚îÜ ‚Ä¶ ‚îÜ specter2_ ‚îÜ specter2_ ‚îÜ specter2_ ‚îÜ specter2 ‚îÇ\n",
      "‚îÇ length    ‚îÜ word_coun ‚îÜ r_citatio ‚îÜ r_h_index ‚îÜ   ‚îÜ dim_96    ‚îÜ dim_97    ‚îÜ dim_98    ‚îÜ _dim_99  ‚îÇ\n",
      "‚îÇ ---       ‚îÜ t         ‚îÜ ns        ‚îÜ ---       ‚îÜ   ‚îÜ ---       ‚îÜ ---       ‚îÜ ---       ‚îÜ ---      ‚îÇ\n",
      "‚îÇ i64       ‚îÜ ---       ‚îÜ ---       ‚îÜ f64       ‚îÜ   ‚îÜ f32       ‚îÜ f32       ‚îÜ f32       ‚îÜ f32      ‚îÇ\n",
      "‚îÇ           ‚îÜ i64       ‚îÜ f64       ‚îÜ           ‚îÜ   ‚îÜ           ‚îÜ           ‚îÜ           ‚îÜ          ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ 0         ‚îÜ 0         ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -1.022253 ‚îÜ 0.041082  ‚îÜ -0.004506 ‚îÜ 0.202222 ‚îÇ\n",
      "‚îÇ 0         ‚îÜ 0         ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -0.988544 ‚îÜ 0.208138  ‚îÜ 0.062278  ‚îÜ 0.471004 ‚îÇ\n",
      "‚îÇ 0         ‚îÜ 0         ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -0.5535   ‚îÜ 1.109443  ‚îÜ 0.020317  ‚îÜ 0.323297 ‚îÇ\n",
      "‚îÇ 812       ‚îÜ 108       ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -0.195705 ‚îÜ 0.306467  ‚îÜ 0.222098  ‚îÜ 0.615315 ‚îÇ\n",
      "‚îÇ 713       ‚îÜ 108       ‚îÜ 0.0       ‚îÜ 0.0       ‚îÜ ‚Ä¶ ‚îÜ -1.318603 ‚îÜ 0.063921  ‚îÜ -0.791118 ‚îÜ 0.406547 ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "Columns with most missing values (train perspective):\n",
      "shape: (30, 7)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ column            ‚îÜ train_missing ‚îÜ val_missing ‚îÜ test_missing ‚îÜ train_pct ‚îÜ val_pct  ‚îÜ test_pct ‚îÇ\n",
      "‚îÇ ---               ‚îÜ ---           ‚îÜ ---         ‚îÜ ---          ‚îÜ ---       ‚îÜ ---      ‚îÜ ---      ‚îÇ\n",
      "‚îÇ str               ‚îÜ i64           ‚îÜ i64         ‚îÜ i64          ‚îÜ f64       ‚îÜ f64      ‚îÜ f64      ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ oa_status_diamond ‚îÜ 468           ‚îÜ 58          ‚îÜ 59           ‚îÜ 0.975     ‚îÜ 0.966667 ‚îÜ 0.983333 ‚îÇ\n",
      "‚îÇ oa_status_bronze  ‚îÜ 449           ‚îÜ 57          ‚îÜ 55           ‚îÜ 0.935417  ‚îÜ 0.95     ‚îÜ 0.916667 ‚îÇ\n",
      "‚îÇ oa_status_green   ‚îÜ 447           ‚îÜ 58          ‚îÜ 56           ‚îÜ 0.93125   ‚îÜ 0.966667 ‚îÜ 0.933333 ‚îÇ\n",
      "‚îÇ oa_status_hybrid  ‚îÜ 447           ‚îÜ 54          ‚îÜ 55           ‚îÜ 0.93125   ‚îÜ 0.9      ‚îÜ 0.916667 ‚îÇ\n",
      "‚îÇ oa_status_gold    ‚îÜ 394           ‚îÜ 49          ‚îÜ 48           ‚îÜ 0.820833  ‚îÜ 0.816667 ‚îÜ 0.8      ‚îÇ\n",
      "‚îÇ ‚Ä¶                 ‚îÜ ‚Ä¶             ‚îÜ ‚Ä¶           ‚îÜ ‚Ä¶            ‚îÜ ‚Ä¶         ‚îÜ ‚Ä¶        ‚îÜ ‚Ä¶        ‚îÇ\n",
      "‚îÇ language          ‚îÜ 0             ‚îÜ 0           ‚îÜ 0            ‚îÜ 0.0       ‚îÜ 0.0      ‚îÜ 0.0      ‚îÇ\n",
      "‚îÇ max_author_h_inde ‚îÜ 0             ‚îÜ 0           ‚îÜ 0            ‚îÜ 0.0       ‚îÜ 0.0      ‚îÜ 0.0      ‚îÇ\n",
      "‚îÇ x                 ‚îÜ               ‚îÜ             ‚îÜ              ‚îÜ           ‚îÜ          ‚îÜ          ‚îÇ\n",
      "‚îÇ max_concept_score ‚îÜ 0             ‚îÜ 0           ‚îÜ 0            ‚îÜ 0.0       ‚îÜ 0.0      ‚îÜ 0.0      ‚îÇ\n",
      "‚îÇ max_topic_score   ‚îÜ 0             ‚îÜ 0           ‚îÜ 0            ‚îÜ 0.0       ‚îÜ 0.0      ‚îÜ 0.0      ‚îÇ\n",
      "‚îÇ nlp_avg_sentence_ ‚îÜ 0             ‚îÜ 0           ‚îÜ 0            ‚îÜ 0.0       ‚îÜ 0.0      ‚îÜ 0.0      ‚îÇ\n",
      "‚îÇ length            ‚îÜ               ‚îÜ             ‚îÜ              ‚îÜ           ‚îÜ          ‚îÜ          ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "üíæ Missingness report saved to: /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/model_ready/missingness_report.csv\n"
     ]
    }
   ],
   "source": [
    "def describe_dataframe(df: pl.DataFrame, name: str) -> None:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Overview: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"Shape:\", df.shape)\n",
    "    print(\"\\nDtypes:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"\\nHead:\")\n",
    "    print(df.head())\n",
    "\n",
    "describe_dataframe(X_train, \"X_train (with embeddings)\")\n",
    "describe_dataframe(X_val,   \"X_val (with embeddings)\")\n",
    "describe_dataframe(X_test,  \"X_test (with embeddings)\")\n",
    "\n",
    "train_nulls = X_train.select(pl.all().null_count()).to_dicts()[0]\n",
    "val_nulls   = X_val.select(pl.all().null_count()).to_dicts()[0]\n",
    "test_nulls  = X_test.select(pl.all().null_count()).to_dicts()[0]\n",
    "\n",
    "rows_train = X_train.height\n",
    "rows_val   = X_val.height\n",
    "rows_test  = X_test.height\n",
    "\n",
    "records = []\n",
    "for col in X_train.columns:\n",
    "    records.append({\n",
    "        \"column\": col,\n",
    "        \"train_missing\": train_nulls.get(col, 0),\n",
    "        \"val_missing\":   val_nulls.get(col, 0),\n",
    "        \"test_missing\":  test_nulls.get(col, 0),\n",
    "        \"train_pct\": train_nulls.get(col, 0) / rows_train if rows_train > 0 else 0.0,\n",
    "        \"val_pct\":   val_nulls.get(col, 0) / rows_val   if rows_val > 0 else 0.0,\n",
    "        \"test_pct\":  test_nulls.get(col, 0) / rows_test  if rows_test > 0 else 0.0,\n",
    "    })\n",
    "\n",
    "missing_report = pl.DataFrame(records).sort(\"train_missing\", descending=True)\n",
    "print(\"\\nColumns with most missing values (train perspective):\")\n",
    "print(missing_report.head(30))\n",
    "\n",
    "missing_report_path = MODEL_DIR / \"missingness_report.csv\"\n",
    "missing_report.write_csv(missing_report_path)\n",
    "print(f\"\\nüíæ Missingness report saved to: {missing_report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1914f67b",
   "metadata": {},
   "source": [
    "## 5. Train-centric missing value imputation (including embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e94021e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T19:48:08.430715Z",
     "iopub.status.busy": "2025-11-17T19:48:08.430648Z",
     "iopub.status.idle": "2025-11-17T19:48:09.656565Z",
     "shell.execute_reply": "2025-11-17T19:48:09.656249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Features by type (after dropping all-missing columns)\n",
      "Numeric features    : 1974\n",
      "Categorical features: 2\n",
      "Embedding features  : 1920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample numeric impute values:\n",
      "  abstract_length: 189.5\n",
      "  abstract_word_count: 25.5\n",
      "  avg_author_citations: 0.0\n",
      "  avg_author_h_index: 0.0\n",
      "  avg_concept_score: 0.35070151138888883\n",
      "  avg_topic_score: 0.0\n",
      "  first_author_citations: 0.0\n",
      "  first_author_h_index: 0.0\n",
      "  first_author_papers: 0.0\n",
      "  has_abstract: 1.0\n",
      "\n",
      "Sample categorical impute values:\n",
      "  language: en\n",
      "  type: article\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post-imputation missingness (train):\n",
      "shape: (1_977, 1)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ column_0 ‚îÇ\n",
      "‚îÇ ---      ‚îÇ\n",
      "‚îÇ u32      ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ 0        ‚îÇ\n",
      "‚îÇ 0        ‚îÇ\n",
      "‚îÇ 0        ‚îÇ\n",
      "‚îÇ 0        ‚îÇ\n",
      "‚îÇ 0        ‚îÇ\n",
      "‚îÇ ‚Ä¶        ‚îÇ\n",
      "‚îÇ 0        ‚îÇ\n",
      "‚îÇ 0        ‚îÇ\n",
      "‚îÇ 0        ‚îÇ\n",
      "‚îÇ 0        ‚îÇ\n",
      "‚îÇ 0        ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "üíæ Imputation spec saved to: /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/model_ready/imputation_spec.json\n",
      "\n",
      "üîÑ Replacing is_oa with is_not_oa (derived feature)...\n",
      "‚úÖ Replaced is_oa with is_not_oa in all datasets\n"
     ]
    }
   ],
   "source": [
    "id_cols = [c for c in X_train.columns if c.lower() == \"id\"]\n",
    "feature_cols = [c for c in X_train.columns if c not in id_cols]\n",
    "\n",
    "train_nulls = X_train.select(pl.all().null_count()).to_dicts()[0]\n",
    "all_missing_cols = [c for c in feature_cols if train_nulls.get(c, 0) == X_train.height]\n",
    "\n",
    "if all_missing_cols:\n",
    "    print(\"Dropping columns that are entirely missing in train:\")\n",
    "    print(all_missing_cols)\n",
    "    X_train = X_train.drop(all_missing_cols)\n",
    "    X_val   = X_val.drop([c for c in all_missing_cols if c in X_val.columns])\n",
    "    X_test  = X_test.drop([c for c in all_missing_cols if c in X_test.columns])\n",
    "    feature_cols = [c for c in feature_cols if c not in all_missing_cols]\n",
    "\n",
    "embedding_cols: List[str] = []\n",
    "for m in embedding_models:\n",
    "    prefix = f\"{m}_\"\n",
    "    embedding_cols.extend([c for c in feature_cols if c.startswith(prefix)])\n",
    "embedding_cols = sorted(set(embedding_cols))\n",
    "\n",
    "numeric_cols: List[str] = []\n",
    "categorical_cols: List[str] = []\n",
    "\n",
    "NUMERIC_DTYPES = {\n",
    "    pl.Int8, pl.Int16, pl.Int32, pl.Int64,\n",
    "    pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,\n",
    "    pl.Float32, pl.Float64\n",
    "}\n",
    "\n",
    "for c, dt in zip(X_train.columns, X_train.dtypes):\n",
    "    if c in feature_cols:\n",
    "        if dt in NUMERIC_DTYPES:\n",
    "            numeric_cols.append(c)\n",
    "        else:\n",
    "            categorical_cols.append(c)\n",
    "\n",
    "print(\"\\n# Features by type (after dropping all-missing columns)\")\n",
    "print(\"Numeric features    :\", len(numeric_cols))\n",
    "print(\"Categorical features:\", len(categorical_cols))\n",
    "print(\"Embedding features  :\", len(embedding_cols))\n",
    "\n",
    "numeric_exprs = [pl.col(c).median().alias(c) for c in numeric_cols]\n",
    "numeric_medians_row = X_train.select(numeric_exprs).to_dicts()[0] if numeric_exprs else {}\n",
    "numeric_impute_values: Dict[str, float] = {c: float(v) for c, v in numeric_medians_row.items()}\n",
    "\n",
    "categorical_impute_values: Dict[str, Any] = {}\n",
    "for col in categorical_cols:\n",
    "    mode_df = X_train.select(pl.col(col).mode())\n",
    "    if mode_df.height > 0:\n",
    "        mode_val = mode_df.select(pl.col(col))[0, 0]\n",
    "        categorical_impute_values[col] = mode_val\n",
    "    else:\n",
    "        categorical_impute_values[col] = \"__missing__\"\n",
    "\n",
    "print(\"\\nSample numeric impute values:\")\n",
    "for k in list(numeric_impute_values.keys())[:10]:\n",
    "    print(f\"  {k}: {numeric_impute_values[k]}\")\n",
    "\n",
    "print(\"\\nSample categorical impute values:\")\n",
    "for k in list(categorical_impute_values.keys())[:10]:\n",
    "    print(f\"  {k}: {categorical_impute_values[k]}\")\n",
    "\n",
    "def apply_imputation(\n",
    "    df: pl.DataFrame,\n",
    "    numeric_values: Dict[str, float],\n",
    "    categorical_values: Dict[str, Any],\n",
    "    id_cols: List[str],\n",
    ") -> pl.DataFrame:\n",
    "    out = df\n",
    "\n",
    "    for col, val in numeric_values.items():\n",
    "        if col in out.columns:\n",
    "            out = out.with_columns(pl.col(col).cast(pl.Float64).fill_null(val))\n",
    "\n",
    "    for col, val in categorical_values.items():\n",
    "        if col in out.columns:\n",
    "            out = out.with_columns(pl.col(col).cast(pl.Utf8).fill_null(str(val)))\n",
    "\n",
    "    remaining_nulls = out.select(pl.all().null_count()).to_dicts()[0]\n",
    "    remaining_cols = [c for c, cnt in remaining_nulls.items() if cnt > 0]\n",
    "    if remaining_cols:\n",
    "        print(\"\\nColumns still with missing values after primary imputation:\", remaining_cols)\n",
    "        for c in remaining_cols:\n",
    "            if c in numeric_values:\n",
    "                out = out.with_columns(pl.col(c).fill_null(0.0))\n",
    "            else:\n",
    "                out = out.with_columns(pl.col(c).cast(pl.Utf8).fill_null(\"__missing__\"))\n",
    "\n",
    "    ordered_cols = id_cols + [c for c in out.columns if c not in id_cols]\n",
    "    return out.select(ordered_cols)\n",
    "\n",
    "X_train_imputed = apply_imputation(X_train, numeric_impute_values, categorical_impute_values, id_cols)\n",
    "X_val_imputed   = apply_imputation(X_val,   numeric_impute_values, categorical_impute_values, id_cols)\n",
    "X_test_imputed  = apply_imputation(X_test,  numeric_impute_values, categorical_impute_values, id_cols)\n",
    "\n",
    "print(\"\\nPost-imputation missingness (train):\")\n",
    "print(X_train_imputed.select(pl.all().null_count()).transpose())\n",
    "\n",
    "import json\n",
    "impute_spec = {\n",
    "    \"numeric_impute_values\": numeric_impute_values,\n",
    "    \"categorical_impute_values\": {k: str(v) for k, v in categorical_impute_values.items()},\n",
    "    \"dropped_all_missing_cols\": all_missing_cols,\n",
    "    \"id_cols\": id_cols,\n",
    "    \"embedding_models\": embedding_models,\n",
    "    \"embedding_cols\": embedding_cols,\n",
    "}\n",
    "impute_spec_path = MODEL_DIR / \"imputation_spec.json\"\n",
    "with impute_spec_path.open(\"w\") as f:\n",
    "    json.dump(impute_spec, f, indent=2)\n",
    "print(f\"\\nüíæ Imputation spec saved to: {impute_spec_path}\")\n",
    "\n",
    "# Replace is_oa with is_not_oa (derived feature: 1 - is_oa)\n",
    "# This provides better signal for class imbalance scenarios\n",
    "if \"is_oa\" in X_train_imputed.columns:\n",
    "    print(\"\\nüîÑ Replacing is_oa with is_not_oa (derived feature)...\")\n",
    "    X_train_imputed = X_train_imputed.with_columns(\n",
    "        (1 - pl.col(\"is_oa\")).alias(\"is_not_oa\")\n",
    "    ).drop(\"is_oa\")\n",
    "    X_val_imputed = X_val_imputed.with_columns(\n",
    "        (1 - pl.col(\"is_oa\")).alias(\"is_not_oa\")\n",
    "    ).drop(\"is_oa\")\n",
    "    X_test_imputed = X_test_imputed.with_columns(\n",
    "        (1 - pl.col(\"is_oa\")).alias(\"is_not_oa\")\n",
    "    ).drop(\"is_oa\")\n",
    "    print(\"‚úÖ Replaced is_oa with is_not_oa in all datasets\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  is_oa column not found, skipping replacement\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8225930f",
   "metadata": {},
   "source": [
    "## 6. Statistical analyses (correlations, chi-square, Cram√©r's V, ANOVA, Tukey's HSD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126273de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T19:48:09.658668Z",
     "iopub.status.busy": "2025-11-17T19:48:09.658546Z",
     "iopub.status.idle": "2025-11-17T19:48:12.101767Z",
     "shell.execute_reply": "2025-11-17T19:48:12.101346Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label diagnostics:\n",
      "  dtype        : int64\n",
      "  # unique vals: 2\n",
      "  treated as categorical: True\n",
      "\n",
      "=== 6.1 Pearson & Spearman correlations ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top correlations (by |Pearson r|):\n",
      "shape: (30, 5)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ feature          ‚îÜ pearson_r ‚îÜ pearson_p  ‚îÜ spearman_r ‚îÜ spearman_p ‚îÇ\n",
      "‚îÇ ---              ‚îÜ ---       ‚îÜ ---        ‚îÜ ---        ‚îÜ ---        ‚îÇ\n",
      "‚îÇ str              ‚îÜ f64       ‚îÜ f64        ‚îÜ f64        ‚îÜ f64        ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ has_grants       ‚îÜ 0.317602  ‚îÜ 1.0371e-12 ‚îÜ 0.317602   ‚îÜ 1.0371e-12 ‚îÇ\n",
      "‚îÇ num_institutions ‚îÜ 0.272109  ‚îÜ 1.3544e-9  ‚îÜ 0.284405   ‚îÜ 2.2060e-10 ‚îÇ\n",
      "‚îÇ num_locations    ‚îÜ 0.263193  ‚îÜ 4.7766e-9  ‚îÜ 0.236306   ‚îÜ 1.6227e-7  ‚îÇ\n",
      "‚îÇ has_pmid         ‚îÜ 0.26116   ‚îÜ 6.3256e-9  ‚îÜ 0.26116    ‚îÜ 6.3256e-9  ‚îÇ\n",
      "‚îÇ specter2_dim_541 ‚îÜ 0.252012  ‚îÜ 2.1746e-8  ‚îÜ 0.222229   ‚îÜ 8.7558e-7  ‚îÇ\n",
      "‚îÇ ‚Ä¶                ‚îÜ ‚Ä¶         ‚îÜ ‚Ä¶          ‚îÜ ‚Ä¶          ‚îÜ ‚Ä¶          ‚îÇ\n",
      "‚îÇ specter2_dim_86  ‚îÜ 0.174547  ‚îÜ 0.000121   ‚îÜ 0.173222   ‚îÜ 0.000137   ‚îÇ\n",
      "‚îÇ specter2_dim_148 ‚îÜ 0.173367  ‚îÜ 0.000135   ‚îÜ 0.153123   ‚îÜ 0.000763   ‚îÇ\n",
      "‚îÇ specter2_dim_354 ‚îÜ 0.172439  ‚îÜ 0.000147   ‚îÜ 0.161476   ‚îÜ 0.000383   ‚îÇ\n",
      "‚îÇ specter2_dim_722 ‚îÜ 0.17185   ‚îÜ 0.000155   ‚îÜ 0.162259   ‚îÜ 0.000358   ‚îÇ\n",
      "‚îÇ specter2_dim_670 ‚îÜ 0.169735  ‚îÜ 0.000187   ‚îÜ 0.189666   ‚îÜ 0.000029   ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "üíæ Correlation summary saved to: /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/model_ready/feature_label_correlations.csv\n",
      "\n",
      "=== 6.2 Chi-square & Cram√©r's V (categorical features vs label) ===\n",
      "\n",
      "Top categorical features by Cram√©r's V:\n",
      "shape: (3, 6)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ feature  ‚îÜ chi2      ‚îÜ p_value   ‚îÜ dof ‚îÜ cramers_v ‚îÜ n   ‚îÇ\n",
      "‚îÇ ---      ‚îÜ ---       ‚îÜ ---       ‚îÜ --- ‚îÜ ---       ‚îÜ --- ‚îÇ\n",
      "‚îÇ str      ‚îÜ f64       ‚îÜ f64       ‚îÜ i64 ‚îÜ f64       ‚îÜ i64 ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ id       ‚îÜ 480.0     ‚îÜ 0.478538  ‚îÜ 479 ‚îÜ 1.0       ‚îÜ 480 ‚îÇ\n",
      "‚îÇ type     ‚îÜ 57.504468 ‚îÜ 6.7372e-7 ‚îÜ 15  ‚îÜ 0.346123  ‚îÜ 480 ‚îÇ\n",
      "‚îÇ language ‚îÜ 7.816326  ‚îÜ 0.993015  ‚îÜ 20  ‚îÜ 0.127609  ‚îÜ 480 ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "üíæ Chi-square / Cram√©r's V summary saved to: /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/model_ready/categorical_chi2_cramersv.csv\n",
      "\n",
      "=== 6.3 One-way ANOVA (numeric features across label groups) ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top features by ANOVA p-value:\n",
      "shape: (30, 3)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ feature          ‚îÜ F         ‚îÜ p_value    ‚îÇ\n",
      "‚îÇ ---              ‚îÜ ---       ‚îÜ ---        ‚îÇ\n",
      "‚îÇ str              ‚îÜ f64       ‚îÜ f64        ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ has_grants       ‚îÜ 53.625721 ‚îÜ 1.0371e-12 ‚îÇ\n",
      "‚îÇ num_institutions ‚îÜ 38.222907 ‚îÜ 1.3544e-9  ‚îÇ\n",
      "‚îÇ num_locations    ‚îÜ 35.575682 ‚îÜ 4.7766e-9  ‚îÇ\n",
      "‚îÇ has_pmid         ‚îÜ 34.988242 ‚îÜ 6.3256e-9  ‚îÇ\n",
      "‚îÇ specter2_dim_541 ‚îÜ 32.41657  ‚îÜ 2.1746e-8  ‚îÇ\n",
      "‚îÇ ‚Ä¶                ‚îÜ ‚Ä¶         ‚îÜ ‚Ä¶          ‚îÇ\n",
      "‚îÇ specter2_dim_86  ‚îÜ 15.020667 ‚îÜ 0.000121   ‚îÇ\n",
      "‚îÇ specter2_dim_148 ‚îÜ 14.812006 ‚îÜ 0.000135   ‚îÇ\n",
      "‚îÇ specter2_dim_354 ‚îÜ 14.649097 ‚îÜ 0.000147   ‚îÇ\n",
      "‚îÇ specter2_dim_722 ‚îÜ 14.546057 ‚îÜ 0.000155   ‚îÇ\n",
      "‚îÇ specter2_dim_670 ‚îÜ 14.179688 ‚îÜ 0.000187   ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "üíæ ANOVA summary saved to: /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/model_ready/anova_numeric_features_vs_label.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of Tukey HSD results:\n",
      "shape: (20, 8)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ group1 ‚îÜ group2 ‚îÜ meandiff ‚îÜ p-adj  ‚îÜ lower   ‚îÜ upper   ‚îÜ reject ‚îÜ feature          ‚îÇ\n",
      "‚îÇ ---    ‚îÜ ---    ‚îÜ ---      ‚îÜ ---    ‚îÜ ---     ‚îÜ ---     ‚îÜ ---    ‚îÜ ---              ‚îÇ\n",
      "‚îÇ i64    ‚îÜ i64    ‚îÜ f64      ‚îÜ f64    ‚îÜ f64     ‚îÜ f64     ‚îÜ f64    ‚îÜ str              ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ 0      ‚îÜ 1      ‚îÜ 0.3284   ‚îÜ 0.0    ‚îÜ 0.2403  ‚îÜ 0.4166  ‚îÜ 1.0    ‚îÜ has_grants       ‚îÇ\n",
      "‚îÇ 0      ‚îÜ 1      ‚îÜ 1.5796   ‚îÜ 0.0    ‚îÜ 1.0776  ‚îÜ 2.0816  ‚îÜ 1.0    ‚îÜ num_institutions ‚îÇ\n",
      "‚îÇ 0      ‚îÜ 1      ‚îÜ 0.8761   ‚îÜ 0.0    ‚îÜ 0.5875  ‚îÜ 1.1648  ‚îÜ 1.0    ‚îÜ num_locations    ‚îÇ\n",
      "‚îÇ 0      ‚îÜ 1      ‚îÜ 0.2566   ‚îÜ 0.0    ‚îÜ 0.1713  ‚îÜ 0.3418  ‚îÜ 1.0    ‚îÜ has_pmid         ‚îÇ\n",
      "‚îÇ 0      ‚îÜ 1      ‚îÜ 0.3846   ‚îÜ 0.0    ‚îÜ 0.2518  ‚îÜ 0.5173  ‚îÜ 1.0    ‚îÜ specter2_dim_541 ‚îÇ\n",
      "‚îÇ ‚Ä¶      ‚îÜ ‚Ä¶      ‚îÜ ‚Ä¶        ‚îÜ ‚Ä¶      ‚îÜ ‚Ä¶       ‚îÜ ‚Ä¶       ‚îÜ ‚Ä¶      ‚îÜ ‚Ä¶                ‚îÇ\n",
      "‚îÇ 0      ‚îÜ 1      ‚îÜ 0.2555   ‚îÜ 0.0    ‚îÜ 0.1329  ‚îÜ 0.3782  ‚îÜ 1.0    ‚îÜ specter2_dim_762 ‚îÇ\n",
      "‚îÇ 0      ‚îÜ 1      ‚îÜ 0.2261   ‚îÜ 0.0001 ‚îÜ 0.1161  ‚îÜ 0.3361  ‚îÜ 1.0    ‚îÜ specter2_dim_308 ‚îÇ\n",
      "‚îÇ 0      ‚îÜ 1      ‚îÜ -0.2472  ‚îÜ 0.0001 ‚îÜ -0.3682 ‚îÜ -0.1261 ‚îÜ 1.0    ‚îÜ specter2_dim_192 ‚îÇ\n",
      "‚îÇ 0      ‚îÜ 1      ‚îÜ 0.2459   ‚îÜ 0.0001 ‚îÜ 0.1249  ‚îÜ 0.3669  ‚îÜ 1.0    ‚îÜ specter2_dim_257 ‚îÇ\n",
      "‚îÇ 0      ‚îÜ 1      ‚îÜ 0.2117   ‚îÜ 0.0001 ‚îÜ 0.1067  ‚îÜ 0.3167  ‚îÜ 1.0    ‚îÜ specter2_dim_690 ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "üíæ Tukey HSD results saved to: /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/model_ready/tukey_hsd_top_features.csv\n"
     ]
    }
   ],
   "source": [
    "if y_train is None:\n",
    "    print(\"No y_train.npy found. Skipping label-based statistical analyses.\")\n",
    "\n",
    "else:\n",
    "    label_series = y_train\n",
    "    uniq = np.unique(label_series)\n",
    "    n_unique = len(uniq)\n",
    "    is_label_categorical = (n_unique <= 20)\n",
    "\n",
    "    print(\"\\nLabel diagnostics:\")\n",
    "    print(\"  dtype        :\", label_series.dtype)\n",
    "    print(\"  # unique vals:\", n_unique)\n",
    "    print(\"  treated as categorical:\", is_label_categorical)\n",
    "\n",
    "    print(\"\\n=== 6.1 Pearson & Spearman correlations ===\")\n",
    "    numeric_feature_cols = [\n",
    "        c for c, dt in zip(X_train_imputed.columns, X_train_imputed.dtypes)\n",
    "        if dt in {\n",
    "            pl.Int8, pl.Int16, pl.Int32, pl.Int64,\n",
    "            pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,\n",
    "            pl.Float32, pl.Float64\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    if np.issubdtype(label_series.dtype, np.number):\n",
    "        label_numeric = label_series.astype(float)\n",
    "    else:\n",
    "        _, label_numeric = np.unique(label_series, return_inverse=True)\n",
    "        label_numeric = label_numeric.astype(float)\n",
    "        print(\"Label converted to numeric codes for correlation analysis.\")\n",
    "\n",
    "    corr_records: List[Dict[str, Any]] = []\n",
    "    for col in numeric_feature_cols:\n",
    "        x = X_train_imputed.select(pl.col(col)).to_series().to_numpy().astype(float)\n",
    "        mask = ~np.isnan(x) & ~np.isnan(label_numeric)\n",
    "        if mask.sum() < 10:\n",
    "            continue\n",
    "\n",
    "        x_vals = x[mask]\n",
    "        y_vals = label_numeric[mask]\n",
    "\n",
    "        # Skip if either array is constant (all values the same)\n",
    "        if len(np.unique(x_vals)) <= 1 or len(np.unique(y_vals)) <= 1:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            pearson_r, pearson_p = stats.pearsonr(x_vals, y_vals)\n",
    "            spearman_r, spearman_p = stats.spearmanr(x_vals, y_vals)\n",
    "\n",
    "            # Handle NaN results\n",
    "            if np.isnan(pearson_r) or np.isnan(spearman_r):\n",
    "                continue\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        corr_records.append({\n",
    "            \"feature\": col,\n",
    "            \"pearson_r\": float(pearson_r),\n",
    "            \"pearson_p\": float(pearson_p),\n",
    "            \"spearman_r\": float(spearman_r),\n",
    "            \"spearman_p\": float(spearman_p),\n",
    "        })\n",
    "\n",
    "    if corr_records:\n",
    "        corr_df = pl.DataFrame(corr_records)\n",
    "        corr_df = corr_df.with_columns(\n",
    "            pl.col(\"pearson_r\").abs().alias(\"abs_pearson\")\n",
    "        ).sort(\"abs_pearson\", descending=True).drop(\"abs_pearson\")\n",
    "        print(\"\\nTop correlations (by |Pearson r|):\")\n",
    "        print(corr_df.head(30))\n",
    "        corr_path = MODEL_DIR / \"feature_label_correlations.csv\"\n",
    "        corr_df.write_csv(corr_path)\n",
    "        print(f\"üíæ Correlation summary saved to: {corr_path}\")\n",
    "    else:\n",
    "        print(\"No valid numeric feature-label correlations could be computed.\")\n",
    "\n",
    "    # Categorical Label Analyses\n",
    "    if is_label_categorical:\n",
    "        print(\"\\n=== 6.2 Chi-square & Cram√©r's V (categorical features vs label) ===\")\n",
    "        cat_cols = [\n",
    "            c for c, dt in zip(X_train_imputed.columns, X_train_imputed.dtypes)\n",
    "            if dt in {pl.Utf8, pl.Categorical}\n",
    "        ]\n",
    "        chi_records: List[Dict[str, Any]] = []\n",
    "        uniq_labels, label_codes = np.unique(label_series, return_inverse=True)\n",
    "\n",
    "        for col in cat_cols:\n",
    "            x_vals = X_train_imputed.select(pl.col(col)).to_series().to_list()\n",
    "            x_vals = np.array(x_vals, dtype=object)\n",
    "            uniq_x, x_codes = np.unique(x_vals, return_inverse=True)\n",
    "            if len(uniq_x) < 2 or len(uniq_labels) < 2:\n",
    "                continue\n",
    "\n",
    "            table = np.zeros((len(uniq_x), len(uniq_labels)), dtype=int)\n",
    "            for i in range(len(x_codes)):\n",
    "                table[x_codes[i], label_codes[i]] += 1\n",
    "\n",
    "            try:\n",
    "                chi2, p, dof, expected = stats.chi2_contingency(table)\n",
    "                n = table.sum()\n",
    "                r, k = table.shape\n",
    "                if min(r, k) > 1:\n",
    "                    cramers_v = np.sqrt(chi2 / (n * (min(r, k) - 1)))\n",
    "                else:\n",
    "                    cramers_v = np.nan\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            chi_records.append({\n",
    "                \"feature\": col,\n",
    "                \"chi2\": float(chi2),\n",
    "                \"p_value\": float(p),\n",
    "                \"dof\": int(dof),\n",
    "                \"cramers_v\": float(cramers_v),\n",
    "                \"n\": int(n),\n",
    "            })\n",
    "\n",
    "        if chi_records:\n",
    "            chi_df = pl.DataFrame(chi_records).sort(\"cramers_v\", descending=True)\n",
    "            print(\"\\nTop categorical features by Cram√©r's V:\")\n",
    "            print(chi_df.head(30))\n",
    "            chi_path = MODEL_DIR / \"categorical_chi2_cramersv.csv\"\n",
    "            chi_df.write_csv(chi_path)\n",
    "            print(f\"üíæ Chi-square / Cram√©r's V summary saved to: {chi_path}\")\n",
    "        else:\n",
    "            print(\"No suitable categorical features for chi-square / Cram√©r's V.\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nSkipping chi-square / Cram√©r's V: label not treated as categorical.\")\n",
    "\n",
    "    # ANOVA for Numeric Features\n",
    "    if is_label_categorical:\n",
    "        print(\"\\n=== 6.3 One-way ANOVA (numeric features across label groups) ===\")\n",
    "        anova_records: List[Dict[str, Any]] = []\n",
    "        uniq_labels = np.unique(label_series)\n",
    "        # Suppress ConstantInputWarning from scipy.stats.f_oneway\n",
    "        import warnings\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "            warnings.filterwarnings('ignore', message='(?i).*constant.*')\n",
    "            warnings.filterwarnings('ignore', message='(?i).*F statistic.*')\n",
    "            for col in numeric_feature_cols:\n",
    "                x = X_train_imputed.select(pl.col(col)).to_series().to_numpy().astype(float)\n",
    "                groups = []\n",
    "                for level in uniq_labels:\n",
    "                    mask = (label_series == level) & ~np.isnan(x)\n",
    "                    vals = x[mask]\n",
    "                    # Filter out constant groups (variance = 0) to avoid warnings\n",
    "                    if vals.size > 1 and np.var(vals) > 1e-10:\n",
    "                        groups.append(vals)\n",
    "                if len(groups) <= 1:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    F, p = stats.f_oneway(*groups)\n",
    "                    # Check for valid results (not inf or nan)\n",
    "                    if np.isfinite(F) and np.isfinite(p):\n",
    "                        anova_records.append({\"feature\": col, \"F\": float(F), \"p_value\": float(p)})\n",
    "                except (ValueError, RuntimeWarning):\n",
    "                    continue\n",
    "\n",
    "        if anova_records:\n",
    "            anova_df = pl.DataFrame(anova_records).sort(\"p_value\")\n",
    "            print(\"\\nTop features by ANOVA p-value:\")\n",
    "            print(anova_df.head(30))\n",
    "            anova_path = MODEL_DIR / \"anova_numeric_features_vs_label.csv\"\n",
    "            anova_df.write_csv(anova_path)\n",
    "            print(f\"üíæ ANOVA summary saved to: {anova_path}\")\n",
    "        else:\n",
    "            print(\"No valid ANOVA comparisons could be computed.\")\n",
    "    else:\n",
    "        print(\"\\nSkipping ANOVA: label not treated as categorical.\")\n",
    "\n",
    "    # Tukey's HSD Post-hoc\n",
    "    if is_label_categorical and HAS_STATSMODELS:\n",
    "        try:\n",
    "            # statsmodels is pandas-based; we only use it to compute Tukey on a few columns\n",
    "            import pandas as pd  # local import, not used for core data handling\n",
    "            anova_pdf = anova_df.to_pandas()\n",
    "            N_TOP_TUKEY = 20\n",
    "            top_features = anova_pdf.nsmallest(N_TOP_TUKEY, \"p_value\")[\"feature\"].tolist()\n",
    "            tukey_records: List[Dict[str, Any]] = []\n",
    "\n",
    "            for feat in top_features:\n",
    "                vals = X_train_imputed.select(pl.col(feat)).to_series().to_numpy().astype(float)\n",
    "                mask = ~np.isnan(vals)\n",
    "                if mask.sum() < 10:\n",
    "                    continue\n",
    "                tukey = pairwise_tukeyhsd(\n",
    "                    endog=vals[mask],\n",
    "                    groups=label_series[mask],\n",
    "                    alpha=0.05,\n",
    "                )\n",
    "                summary = tukey.summary()\n",
    "                header = summary.data[0]\n",
    "                for row in summary.data[1:]:\n",
    "                    rec = {h: v for h, v in zip(header, row)}\n",
    "                    rec[\"feature\"] = feat\n",
    "                    tukey_records.append(rec)\n",
    "\n",
    "            if tukey_records:\n",
    "                tukey_df = pl.DataFrame(tukey_records)\n",
    "                print(\"\\nSample of Tukey HSD results:\")\n",
    "                print(tukey_df.head(30))\n",
    "                tukey_path = MODEL_DIR / \"tukey_hsd_top_features.csv\"\n",
    "                tukey_df.write_csv(tukey_path)\n",
    "                print(f\"üíæ Tukey HSD results saved to: {tukey_path}\")\n",
    "            else:\n",
    "                print(\"Tukey's HSD did not produce any results (insufficient data after filtering).\")\n",
    "        except Exception as e:\n",
    "            print(f\"Tukey's HSD step encountered an error and will be skipped: {e}\")\n",
    "    elif is_label_categorical and not HAS_STATSMODELS:\n",
    "        print(\"\\nTukey's HSD skipped: statsmodels is not installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e24376d",
   "metadata": {},
   "source": [
    "## 7. Prepare model-ready feature sets (keep all non-embedding features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1c27f65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T19:48:12.103637Z",
     "iopub.status.busy": "2025-11-17T19:48:12.103543Z",
     "iopub.status.idle": "2025-11-17T19:48:12.244386Z",
     "shell.execute_reply": "2025-11-17T19:48:12.243939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature summary:\n",
      "  Embedding features (kept): 1920\n",
      "  Non-embedding numeric features (kept): 54\n",
      "  Categorical features (kept): 2\n",
      "  ID columns: 1\n",
      "\n",
      "  Total non-embedding features kept: 56\n",
      "\n",
      "Total columns in model-ready feature set (excluding label): 1977\n",
      "\n",
      "Shapes of model-ready feature matrices:\n",
      "  X_train_model_ready: (480, 1977)\n",
      "  X_val_model_ready:   (60, 1977)\n",
      "  X_test_model_ready:  (60, 1977)\n"
     ]
    }
   ],
   "source": [
    "# Keep ALL non-embedding features (no reduction)\n",
    "# This preserves signals that may be sparse in the sample but informative in the full dataset\n",
    "# Scalable approach: works with any dataset size using Polars operations\n",
    "\n",
    "embedding_cols_current: List[str] = []\n",
    "for m in embedding_models:\n",
    "    prefix = f\"{m}_\"\n",
    "    embedding_cols_current.extend([c for c in X_train_imputed.columns if c.startswith(prefix)])\n",
    "embedding_cols_current = sorted(set(embedding_cols_current))\n",
    "\n",
    "# Get all non-embedding features (numeric + categorical)\n",
    "non_emb_numeric_cols = [\n",
    "    c for c, dt in zip(X_train_imputed.columns, X_train_imputed.dtypes)\n",
    "    if dt in {\n",
    "        pl.Int8, pl.Int16, pl.Int32, pl.Int64,\n",
    "        pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,\n",
    "        pl.Float32, pl.Float64\n",
    "    }\n",
    "    and c not in embedding_cols_current \n",
    "    and c not in id_cols \n",
    "    and c != \"label\"\n",
    "]\n",
    "\n",
    "categorical_cols_current = [\n",
    "    c for c, dt in zip(X_train_imputed.columns, X_train_imputed.dtypes)\n",
    "    if dt in {pl.Utf8, pl.Categorical}\n",
    "    and c not in id_cols\n",
    "]\n",
    "\n",
    "print(\"\\nFeature summary:\")\n",
    "print(f\"  Embedding features (kept): {len(embedding_cols_current)}\")\n",
    "print(f\"  Non-embedding numeric features (kept): {len(non_emb_numeric_cols)}\")\n",
    "print(f\"  Categorical features (kept): {len(categorical_cols_current)}\")\n",
    "print(f\"  ID columns: {len(id_cols)}\")\n",
    "print(f\"\\n  Total non-embedding features kept: {len(non_emb_numeric_cols) + len(categorical_cols_current)}\")\n",
    "\n",
    "# Prepare model-ready feature sets (all features)\n",
    "model_ready_feature_cols = (\n",
    "    id_cols\n",
    "    + embedding_cols_current\n",
    "    + non_emb_numeric_cols\n",
    "    + categorical_cols_current\n",
    ")\n",
    "\n",
    "# Ensure column order and existence\n",
    "seen: set[str] = set()\n",
    "ordered_model_ready_cols: List[str] = []\n",
    "for c in model_ready_feature_cols:\n",
    "    if c not in seen and c in X_train_imputed.columns:\n",
    "        ordered_model_ready_cols.append(c)\n",
    "        seen.add(c)\n",
    "\n",
    "print(f\"\\nTotal columns in model-ready feature set (excluding label): {len(ordered_model_ready_cols)}\")\n",
    "\n",
    "# Create model-ready datasets (all features, scalable with Polars)\n",
    "X_train_model_ready = X_train_imputed.select(ordered_model_ready_cols)\n",
    "X_val_model_ready   = X_val_imputed.select(ordered_model_ready_cols)\n",
    "X_test_model_ready  = X_test_imputed.select(ordered_model_ready_cols)\n",
    "\n",
    "print(\"\\nShapes of model-ready feature matrices:\")\n",
    "print(f\"  X_train_model_ready: {X_train_model_ready.shape}\")\n",
    "print(f\"  X_val_model_ready:   {X_val_model_ready.shape}\")\n",
    "print(f\"  X_test_model_ready:  {X_test_model_ready.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8087c4a1",
   "metadata": {},
   "source": [
    "## 8. Save model-ready train / validation / test parquet files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0ba3b3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T19:48:12.245496Z",
     "iopub.status.busy": "2025-11-17T19:48:12.245418Z",
     "iopub.status.idle": "2025-11-17T19:48:12.362534Z",
     "shell.execute_reply": "2025-11-17T19:48:12.362214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model-ready shapes:\n",
      "  train_model_ready: (480, 1978)\n",
      "  val_model_ready:   (60, 1978)\n",
      "  test_model_ready:  (60, 1977)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saved model-ready parquet files:\n",
      "  /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/model_ready/train_model_ready.parquet\n",
      "  /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/model_ready/val_model_ready.parquet\n",
      "  /Users/santoshdesai/Documents/Desai_Projects/Kaggle2/data/model_ready/test_model_ready.parquet\n"
     ]
    }
   ],
   "source": [
    "# Prepare final model-ready datasets with labels (scalable with Polars)\n",
    "train_model_ready = X_train_model_ready\n",
    "val_model_ready   = X_val_model_ready\n",
    "test_model_ready  = X_test_model_ready\n",
    "\n",
    "if y_train is not None:\n",
    "    train_model_ready = train_model_ready.with_columns(pl.Series(\"label\", y_train))\n",
    "if y_val is not None and y_val.shape[0] == val_model_ready.height:\n",
    "    val_model_ready = val_model_ready.with_columns(pl.Series(\"label\", y_val))\n",
    "\n",
    "print(\"\\nModel-ready shapes:\")\n",
    "print(f\"  train_model_ready: {train_model_ready.shape}\")\n",
    "print(f\"  val_model_ready:   {val_model_ready.shape}\")\n",
    "print(f\"  test_model_ready:  {test_model_ready.shape}\")\n",
    "\n",
    "# Save model-ready parquet files (scalable - Polars handles large datasets efficiently)\n",
    "train_path = MODEL_DIR / \"train_model_ready.parquet\"\n",
    "val_path   = MODEL_DIR / \"val_model_ready.parquet\"\n",
    "test_path  = MODEL_DIR / \"test_model_ready.parquet\"\n",
    "\n",
    "train_model_ready.write_parquet(train_path)\n",
    "val_model_ready.write_parquet(val_path)\n",
    "test_model_ready.write_parquet(test_path)\n",
    "\n",
    "print(\"\\nüíæ Saved model-ready parquet files:\")\n",
    "print(f\"  {train_path}\")\n",
    "print(f\"  {val_path}\")\n",
    "print(f\"  {test_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d249b9b",
   "metadata": {},
   "source": [
    "## 9. Recap\n",
    "\n",
    "This notebook:\n",
    "\n",
    "- Loads the base features from `X_train.parquet`, `X_val.parquet`, `X_test.parquet`\n",
    "- Discovers and merges all available embedding parquets via `id`\n",
    "- Performs train-centric imputation with polars\n",
    "- Replaces `is_oa` with `is_not_oa` (derived feature: 1 - is_oa) for better signal\n",
    "- Computes Pearson / Spearman, chi-square / Cram√©r's V, ANOVA, optional Tukey HSD\n",
    "- Keeps **all non-embedding features** (no reduction) to preserve signals that may be sparse in sample but informative in full dataset\n",
    "- Writes model-ready splits under `data/model_ready`\n",
    "\n",
    "All dataframe logic is handled with **polars** (scalable for large datasets); pandas appears only in a small local scope if you enable Tukey HSD via statsmodels.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
