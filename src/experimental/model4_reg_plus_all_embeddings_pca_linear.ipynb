{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f13f54",
   "metadata": {},
   "source": [
    "# Model 4: Regular features + all embedding families (each PCA-compressed) \u2013 Linear classifier\n",
    "\n",
    "This notebook trains a linear PyTorch classifier on regular features plus **all available embedding families**, each\n",
    "compressed separately via IncrementalPCA. This gives a \"full information but simple model\" that respects the\n",
    "constraint of using simpler models as feature complexity grows.\n",
    "\n",
    "**Features:**\n",
    "- \u2705 Uses **all embedding families** (sent_transformer, scibert, specter2, etc.)\n",
    "- \u2705 **Per-family PCA** to preserve information while reducing dimensionality\n",
    "- \u2705 5-fold Cross-Validation\n",
    "- \u2705 Hyperparameter Tuning\n",
    "- \u2705 Threshold Fine-tuning\n",
    "- \u2705 Model Weight Saving\n",
    "- \u2705 Submission.csv Generation\n",
    "- \u2705 OOM Safe with aggressive memory management\n",
    "- \u2705 SMOTETomek for class imbalance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ff5719",
   "metadata": {},
   "source": [
    "# \ud83d\udcd1 Model 4 - Code Navigation Index\n",
    "\n",
    "## Quick Navigation\n",
    "- **[Setup](#1-setup)** - Imports, paths, device configuration, robustness utilities\n",
    "- **[Data Loading](#2-data-loading--feature-extraction)** - Load and split features\n",
    "- **[PCA Preprocessing](#3-feature-preprocessing-pca)** - Embedding compression (if applicable)\n",
    "- **[SMOTETomek](#4-class-imbalance-handling-smotetomek)** - Class imbalance resampling\n",
    "- **[Feature Scaling](#5-feature-scaling)** - StandardScaler normalization\n",
    "- **[Cross-Validation](#6-cross-validation--hyperparameter-tuning)** - Hyperparameter optimization\n",
    "- **[Threshold Tuning](#7-threshold-tuning--final-evaluation)** - Optimal threshold finding\n",
    "- **[Model Saving](#8-save-model)** - Save model weights and metadata\n",
    "- **[Submission](#9-generate-submission)** - Generate test predictions\n",
    "\n",
    "## Model Type: Linear Classifier (regular + all embeddings, PCA)\n",
    "\n",
    "## Key Features\n",
    "\u2705 GPU-friendly with CPU fallback  \n",
    "\u2705 Aggressive garbage collection  \n",
    "\u2705 OOM resistant with chunked processing  \n",
    "\u2705 Kernel panic resistant (signal handlers, checkpoints)  \n",
    "\u2705 Polars-only (no pandas)  \n",
    "\u2705 GPU-friendly PCA (IncrementalTorchPCA option)  \n",
    "\u2705 SMOTETomek for class imbalance  \n",
    "\u2705 Feature scaling & embedding normalization  \n",
    "\u2705 Hyperparameter tuning (RandomizedSearchCV/GridSearchCV)  \n",
    "\u2705 Fine-grained threshold optimization (120+ thresholds)  \n",
    "\u2705 Model weights saved  \n",
    "\u2705 Chunked/batched data processing  \n",
    "\n",
    "## Memory Management\n",
    "- `cleanup_memory()`: Aggressive GC + GPU cache clearing\n",
    "- `check_memory_safe()`: Pre-operation memory checks\n",
    "- `chunked_operation()`: Process large data in chunks\n",
    "- `safe_operation()`: Retry decorator with OOM handling\n",
    "- Signal handlers: SIGINT/SIGTERM for graceful shutdown\n",
    "- Checkpoints: Resume from failures\n",
    "\n",
    "## Device Handling\n",
    "- Automatic GPU detection with CPU fallback\n",
    "- `device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')`\n",
    "- All tensors moved to device explicitly\n",
    "- GPU cache cleared aggressively after operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34920cb8",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7621ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import gc\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import signal\n",
    "import atexit\n",
    "from functools import wraps\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Device (GPU if available, else CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# Paths\n",
    "current = Path(os.getcwd())\n",
    "PROJECT_ROOT = current\n",
    "for _ in range(5):\n",
    "    if (PROJECT_ROOT / 'data').exists():\n",
    "        break\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "else:\n",
    "    PROJECT_ROOT = current.parent.parent\n",
    "\n",
    "MODEL_READY_DIR = PROJECT_ROOT / 'data' / 'model_ready'\n",
    "utils_path = PROJECT_ROOT / 'src' / 'utils'\n",
    "print('PROJECT_ROOT:', PROJECT_ROOT)\n",
    "print('MODEL_READY_DIR:', MODEL_READY_DIR)\n",
    "\n",
    "# Import PCA utilities\n",
    "USE_TORCH_PCA = False  # Set to True to use PyTorch PCA (requires more memory)\n",
    "if utils_path.exists():\n",
    "    sys.path.insert(0, str(utils_path))\n",
    "\n",
    "if USE_TORCH_PCA:\n",
    "    try:\n",
    "        from pca_utils import IncrementalTorchPCA\n",
    "        IncrementalPCA = IncrementalTorchPCA  # Alias for compatibility\n",
    "        IS_TORCH_PCA = True\n",
    "        print(\"\u2705 Using PyTorch PCA (GPU-friendly)\")\n",
    "    except ImportError:\n",
    "        from sklearn.decomposition import IncrementalPCA\n",
    "        IS_TORCH_PCA = False\n",
    "        print(\"\u26a0\ufe0f Using sklearn IncrementalPCA (CPU only)\")\n",
    "else:\n",
    "    from sklearn.decomposition import IncrementalPCA\n",
    "    IS_TORCH_PCA = False\n",
    "    print(\"\u2705 Using sklearn IncrementalPCA (memory-efficient)\")\n",
    "\n",
    "# Import memory utilities from shared module\n",
    "if utils_path.exists():\n",
    "    sys.path.insert(0, str(utils_path))\n",
    "\n",
    "try:\n",
    "    from model_training_utils import cleanup_memory, memory_usage\n",
    "    print(\"\u2705 Memory utilities imported from shared module\")\n",
    "except ImportError:\n",
    "    def cleanup_memory():\n",
    "        \"\"\"Aggressive memory cleanup for both CPU and GPU.\"\"\"\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "            torch.cuda.ipc_collect()\n",
    "        gc.collect()\n",
    "    \n",
    "    def memory_usage():\n",
    "        \"\"\"Display current memory usage statistics.\"\"\"\n",
    "        try:\n",
    "            import psutil\n",
    "            process = psutil.Process(os.getpid())\n",
    "            mem_info = process.memory_info()\n",
    "            print(f\"\ud83d\udcbe Memory: {mem_info.rss / 1024**3:.2f} GB (RAM)\", end=\"\")\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_mem = torch.cuda.memory_allocated() / 1024**3\n",
    "                gpu_reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "                print(f\" | {gpu_mem:.2f}/{gpu_reserved:.2f} GB (GPU used/reserved)\")\n",
    "            else:\n",
    "                print()\n",
    "        except ImportError:\n",
    "            print(\"\ud83d\udcbe Memory tracking requires psutil: pip install psutil\")\n",
    "    \n",
    "    print(\"\u26a0\ufe0f Using fallback memory utilities\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3784af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ENHANCED ROBUSTNESS UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "# Global checkpoint state\n",
    "_checkpoint_state = {\n",
    "    'pca_complete': False,\n",
    "    'scaling_complete': False,\n",
    "    'cv_complete': False,\n",
    "    'final_model_trained': False,\n",
    "    'last_saved_checkpoint': None\n",
    "}\n",
    "\n",
    "def save_checkpoint(state_name: str, data: dict, checkpoint_dir: Path = None):\n",
    "    \"\"\"Save checkpoint to resume from failures.\"\"\"\n",
    "    if checkpoint_dir is None:\n",
    "        checkpoint_dir = PROJECT_ROOT / 'data' / 'checkpoints'\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    checkpoint_path = checkpoint_dir / f'model4_checkpoint_{state_name}.pkl'\n",
    "    try:\n",
    "        with open(checkpoint_path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "        _checkpoint_state['last_saved_checkpoint'] = checkpoint_path\n",
    "        print(f\"\u2705 Checkpoint saved: {checkpoint_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f Failed to save checkpoint: {e}\")\n",
    "\n",
    "def load_checkpoint(state_name: str, checkpoint_dir: Path = None):\n",
    "    \"\"\"Load checkpoint to resume from failures.\"\"\"\n",
    "    if checkpoint_dir is None:\n",
    "        checkpoint_dir = PROJECT_ROOT / 'data' / 'checkpoints'\n",
    "    checkpoint_path = checkpoint_dir / f'model4_checkpoint_{state_name}.pkl'\n",
    "    if checkpoint_path.exists():\n",
    "        try:\n",
    "            with open(checkpoint_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            print(f\"\u2705 Checkpoint loaded: {checkpoint_path}\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"\u26a0\ufe0f Failed to load checkpoint: {e}\")\n",
    "    return None\n",
    "\n",
    "def check_memory_safe(ram_threshold_gb=0.85, gpu_threshold=0.80):\n",
    "    \"\"\"Check if memory usage is safe before operations.\"\"\"\n",
    "    try:\n",
    "        import psutil\n",
    "        process = psutil.Process(os.getpid())\n",
    "        ram_gb = process.memory_info().rss / 1024**3\n",
    "        total_ram = psutil.virtual_memory().total / 1024**3\n",
    "        ram_ratio = ram_gb / total_ram if total_ram > 0 else 0\n",
    "        gpu_ratio = 0\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_used = torch.cuda.memory_allocated() / 1024**3\n",
    "            gpu_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "            gpu_ratio = gpu_used / gpu_total if gpu_total > 0 else 0\n",
    "        is_safe = ram_ratio < ram_threshold_gb and gpu_ratio < gpu_threshold\n",
    "        return is_safe, {'ram_gb': ram_gb, 'ram_ratio': ram_ratio, 'gpu_ratio': gpu_ratio}\n",
    "    except:\n",
    "        return True, {}\n",
    "\n",
    "def chunked_operation(data, operation_func, chunk_size: int = 10000, progress_every: int = 10, operation_name: str = \"operation\"):\n",
    "    \"\"\"Execute operation on data in chunks with progress tracking.\"\"\"\n",
    "    total_chunks = (len(data) + chunk_size - 1) // chunk_size\n",
    "    results = []\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        chunk_num = i // chunk_size + 1\n",
    "        chunk = data[i:i+chunk_size]\n",
    "        try:\n",
    "            is_safe, mem_info = check_memory_safe(ram_threshold_gb=0.85, gpu_threshold=0.80)\n",
    "            if not is_safe:\n",
    "                cleanup_memory()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                time.sleep(0.5)\n",
    "            chunk_result = operation_func(chunk)\n",
    "            results.append(chunk_result)\n",
    "            if chunk_num % progress_every == 0 or chunk_num == total_chunks:\n",
    "                print(f\"  Progress: {chunk_num}/{total_chunks} chunks ({chunk_num*100//total_chunks}%)\")\n",
    "            del chunk\n",
    "            if chunk_num % 5 == 0:\n",
    "                cleanup_memory()\n",
    "        except (MemoryError, RuntimeError) as e:\n",
    "            error_msg = str(e).lower()\n",
    "            if 'out of memory' in error_msg or 'oom' in error_msg:\n",
    "                cleanup_memory()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                smaller_chunk_size = max(1000, chunk_size // 2)\n",
    "                if smaller_chunk_size < chunk_size:\n",
    "                    return chunked_operation(data[i:], operation_func, chunk_size=smaller_chunk_size, progress_every=progress_every, operation_name=operation_name)\n",
    "                else:\n",
    "                    raise\n",
    "            else:\n",
    "                raise\n",
    "    return results\n",
    "\n",
    "def emergency_cleanup():\n",
    "    \"\"\"Emergency cleanup on exit.\"\"\"\n",
    "    try:\n",
    "        cleanup_memory()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        print(\"\u2705 Emergency cleanup completed\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "atexit.register(emergency_cleanup)\n",
    "\n",
    "# Signal handler for graceful shutdown\n",
    "def signal_handler(signum, frame):\n",
    "    \"\"\"Handle signals for graceful shutdown.\"\"\"\n",
    "    print(f\"\u26a0\ufe0f Received signal {signum}, saving checkpoint...\")\n",
    "    save_checkpoint('emergency', {'status': 'signal_received', 'signal': signum})\n",
    "    emergency_cleanup()\n",
    "    raise KeyboardInterrupt\n",
    "\n",
    "try:\n",
    "    signal.signal(signal.SIGINT, signal_handler)\n",
    "    signal.signal(signal.SIGTERM, signal_handler)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"\u2705 Enhanced robustness utilities loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61767fa4",
   "metadata": {},
   "source": [
    "## 2. Dataset & utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8722555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquet_split(split: str) -> pl.DataFrame:\n",
    "    \"\"\"Load a model_ready parquet split with error handling.\"\"\"\n",
    "    try:\n",
    "        path = MODEL_READY_DIR / f'{split}_model_ready.parquet'\n",
    "        if not path.exists():\n",
    "            alt = MODEL_READY_DIR / f'{split}_model_ready_reduced.parquet'\n",
    "            if alt.exists():\n",
    "                path = alt\n",
    "            else:\n",
    "                raise FileNotFoundError(f'Could not find {split} data')\n",
    "        print(f'Loading {split} from {path}')\n",
    "        return pl.read_parquet(path)\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error loading {split}: {e}\")\n",
    "        raise\n",
    "\n",
    "def split_features_reg_and_all_emb(df: pl.DataFrame):\n",
    "    \"\"\"Split features into regular and all embedding families.\"\"\"\n",
    "    cols = df.columns\n",
    "    dtypes = df.dtypes\n",
    "    label = df['label'].to_numpy() if 'label' in cols else None\n",
    "    \n",
    "    reg_cols = []\n",
    "    EMBEDDING_FAMILY_PREFIXES = ['sent_transformer_', 'scibert_', 'specter_', 'specter2_', 'ner_']\n",
    "    emb_family_to_cols = {p: [] for p in EMBEDDING_FAMILY_PREFIXES}\n",
    "    \n",
    "    NUMERIC_DTYPES = {\n",
    "        pl.Int8, pl.Int16, pl.Int32, pl.Int64,\n",
    "        pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,\n",
    "        pl.Float32, pl.Float64\n",
    "    }\n",
    "    \n",
    "    for c, dt in zip(cols, dtypes):\n",
    "        if c in ('id', 'label'):\n",
    "            continue\n",
    "        matched = False\n",
    "        for p in EMBEDDING_FAMILY_PREFIXES:\n",
    "            if c.startswith(p):\n",
    "                emb_family_to_cols[p].append(c)\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched and dt in NUMERIC_DTYPES:\n",
    "            reg_cols.append(c)\n",
    "    \n",
    "    X_reg = df.select(reg_cols).to_numpy() if reg_cols else None\n",
    "    X_emb_families = {}\n",
    "    for p, clist in emb_family_to_cols.items():\n",
    "        if clist:\n",
    "            X_emb_families[p] = df.select(clist).to_numpy()\n",
    "    \n",
    "    return X_reg, X_emb_families, label, reg_cols, emb_family_to_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6ad60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ROBUST FEATURE SELECTION: Use reasoned handpicked features\n",
    "# ============================================================================\n",
    "\n",
    "USE_FEATURE_SELECTION = False  # Disabled: JSON contains all features anyway\n",
    "\n",
    "if USE_FEATURE_SELECTION:\n",
    "    try:\n",
    "        curated_path = MODEL_READY_DIR / 'handpicked_features_reasoned.json'\n",
    "        if curated_path.exists():\n",
    "            with open(curated_path) as f:\n",
    "                curated_data = json.load(f)\n",
    "            handpicked_features = curated_data['handpicked_features']\n",
    "            print(f'\\n\ud83d\udcca Feature Selection: Using {len(handpicked_features)} reasoned handpicked features')\n",
    "        else:\n",
    "            print(f'  \u26a0\ufe0f Reasoned features file not found, using all features')\n",
    "            handpicked_features = None\n",
    "    except Exception as e:\n",
    "        print(f'  \u26a0\ufe0f Error in feature selection: {e}')\n",
    "        handpicked_features = None\n",
    "else:\n",
    "    handpicked_features = None\n",
    "    print('\\n\ud83d\udcca Feature Selection: DISABLED (using all features)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535fcfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = load_parquet_split('train')\n",
    "val_df = load_parquet_split('val')\n",
    "\n",
    "X_reg_train, X_emb_train_fams, y_train, reg_cols, emb_family_to_cols = split_features_reg_and_all_emb(train_df)\n",
    "X_reg_val, X_emb_val_fams, y_val, _, _ = split_features_reg_and_all_emb(val_df)\n",
    "\n",
    "# Apply feature selection to regular features if enabled\n",
    "if handpicked_features is not None:\n",
    "    available_features = set(reg_cols)\n",
    "    selected_features = [f for f in handpicked_features if f in available_features]\n",
    "    if len(selected_features) < len(reg_cols):\n",
    "        feature_idx_map = {f: i for i, f in enumerate(reg_cols)}\n",
    "        selected_indices = [feature_idx_map[f] for f in selected_features]\n",
    "        X_reg_train = X_reg_train[:, selected_indices]\n",
    "        X_reg_val = X_reg_val[:, selected_indices]\n",
    "        reg_cols = selected_features\n",
    "        print(f'  \u2705 Feature selection applied! Regular features: {len(reg_cols)}')\n",
    "\n",
    "print(f'\\n\ud83d\udcca Data Summary:')\n",
    "print(f'  Regular features: {len(reg_cols)}')\n",
    "for fam, arr in X_emb_train_fams.items():\n",
    "    print(f'  Embedding {fam}: {arr.shape[1]} dims')\n",
    "\n",
    "del train_df, val_df\n",
    "cleanup_memory()\n",
    "memory_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a9d195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PCA COMPRESSION PER EMBEDDING FAMILY\n",
    "# ============================================================================\n",
    "\n",
    "# Number of PCA components per family\n",
    "PCA_COMPONENTS_PER_FAMILY = {\n",
    "    'sent_transformer_': 64,\n",
    "    'scibert_': 64,\n",
    "    'specter_': 64,\n",
    "    'specter2_': 64,\n",
    "    'ner_': 32,\n",
    "}\n",
    "\n",
    "print('\\n\ud83d\udcca Applying PCA compression per embedding family...')\n",
    "\n",
    "X_emb_train_pca_list = []\n",
    "X_emb_val_pca_list = []\n",
    "pca_transformers = {}\n",
    "\n",
    "for fam_prefix, X_emb_train_fam in X_emb_train_fams.items():\n",
    "    n_components = PCA_COMPONENTS_PER_FAMILY.get(fam_prefix, 64)\n",
    "    print(f'\\n  Processing {fam_prefix}:')\n",
    "    print(f'    Original dim: {X_emb_train_fam.shape[1]}')\n",
    "    print(f'    Target PCA components: {n_components}')\n",
    "    \n",
    "    # Fit PCA on training data (use subset if large)\n",
    "    max_pca_rows = min(150_000, X_emb_train_fam.shape[0])\n",
    "    if X_emb_train_fam.shape[0] > max_pca_rows:\n",
    "        idx = np.random.choice(X_emb_train_fam.shape[0], size=max_pca_rows, replace=False)\n",
    "        pca_fit_data = X_emb_train_fam[idx]\n",
    "    else:\n",
    "        pca_fit_data = X_emb_train_fam\n",
    "    \n",
    "    # Create PCA transformer\n",
    "    if IS_TORCH_PCA:\n",
    "        ipca = IncrementalPCA(n_components=n_components, batch_size=5000, device=device)\n",
    "    else:\n",
    "        ipca = IncrementalPCA(n_components=n_components, batch_size=5000)\n",
    "    \n",
    "    # Fit and transform\n",
    "    ipca.fit(pca_fit_data)\n",
    "    X_emb_train_pca = ipca.transform(X_emb_train_fam)\n",
    "    X_emb_val_pca = ipca.transform(X_emb_val_fams[fam_prefix]) if fam_prefix in X_emb_val_fams else None\n",
    "    \n",
    "    X_emb_train_pca_list.append(X_emb_train_pca)\n",
    "    if X_emb_val_pca is not None:\n",
    "        X_emb_val_pca_list.append(X_emb_val_pca)\n",
    "    \n",
    "    pca_transformers[fam_prefix] = ipca\n",
    "    \n",
    "    print(f'    \u2705 PCA fitted with {n_components} components')\n",
    "    print(f'    Reduced dim: {X_emb_train_pca.shape[1]}')\n",
    "    \n",
    "    del X_emb_train_pca, X_emb_val_pca, pca_fit_data\n",
    "    cleanup_memory()\n",
    "\n",
    "# Combine regular features + all PCA-compressed embeddings\n",
    "X_train = np.concatenate([X_reg_train] + X_emb_train_pca_list, axis=1)\n",
    "X_val = np.concatenate([X_reg_val] + X_emb_val_pca_list, axis=1)\n",
    "\n",
    "del X_reg_train, X_reg_val, X_emb_train_fams, X_emb_val_fams, X_emb_train_pca_list, X_emb_val_pca_list\n",
    "cleanup_memory()\n",
    "\n",
    "print(f'\\n\u2705 Combined features:')\n",
    "print(f'  Train shape: {X_train.shape}')\n",
    "print(f'  Val shape: {X_val.shape}')\n",
    "memory_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c132e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    \"\"\"Simple tabular dataset for PyTorch.\"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# DataLoader configuration\n",
    "BATCH_SIZE = 512\n",
    "VAL_BATCH_SIZE = 512\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "train_dataset = TabularDataset(X_train, y_train)\n",
    "val_dataset = TabularDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "print(f'\\n\ud83d\udcca DataLoader Configuration:')\n",
    "print(f'   Train batch size: {BATCH_SIZE}')\n",
    "print(f'   Val batch size: {VAL_BATCH_SIZE}')\n",
    "print(f'   Num workers: {NUM_WORKERS} (0 = single process, saves memory)')\n",
    "print(f'Class counts (train): {np.bincount(y_train.astype(int))}')\n",
    "memory_usage()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47142c2",
   "metadata": {},
   "source": [
    "## 3. Class Imbalance Handling: SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc80700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "print('\\n\ud83d\udcca Checking class imbalance and applying SMOTETomek resampling...')\n",
    "print(f'  Before: {len(X_train)} samples, Positive: {y_train.sum()}, Negative: {(y_train==0).sum()}')\n",
    "print(f'  Imbalance ratio: {(y_train==0).sum() / max(y_train.sum(), 1):.2f}:1')\n",
    "\n",
    "try:\n",
    "    smt = SMOTETomek(random_state=42, sampling_strategy='auto', n_jobs=-1)\n",
    "    X_train_resampled, y_train_resampled = smt.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(f'  After: {len(X_train_resampled)} samples, Positive: {y_train_resampled.sum()}, Negative: {(y_train_resampled==0).sum()}')\n",
    "    print(f'  Balance ratio: {(y_train_resampled==0).sum() / max(y_train_resampled.sum(), 1):.2f}:1')\n",
    "    \n",
    "    X_train = X_train_resampled\n",
    "    y_train = y_train_resampled\n",
    "    \n",
    "    del X_train_resampled, y_train_resampled\n",
    "    cleanup_memory()\n",
    "    \n",
    "    train_dataset = TabularDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=False\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f'  \u26a0\ufe0f SMOTETomek failed: {e}')\n",
    "    print('  Continuing with original training data...')\n",
    "    cleanup_memory()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43d5aa1",
   "metadata": {},
   "source": [
    "## 4. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354f1420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print('\\n\ud83d\udcca Applying Feature Scaling...')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "X_train = X_train_scaled\n",
    "X_val = X_val_scaled\n",
    "\n",
    "del X_train_scaled, X_val_scaled\n",
    "cleanup_memory()\n",
    "\n",
    "print(f'  \u2705 Features scaled: {X_train.shape}')\n",
    "\n",
    "# Update datasets\n",
    "train_dataset = TabularDataset(X_train, y_train)\n",
    "val_dataset = TabularDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "memory_usage()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dccb0f4",
   "metadata": {},
   "source": [
    "## 5. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3086ad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "print(f'\ud83d\udcca Input dimension: {input_dim}')\n",
    "print(f'   Regular features: {len(reg_cols)}')\n",
    "total_pca_dims = sum(PCA_COMPONENTS_PER_FAMILY.get(fam, 0) for fam in emb_family_to_cols.keys() if emb_family_to_cols.get(fam))\n",
    "print(f'   PCA-compressed embeddings: {total_pca_dims} total')\n",
    "\n",
    "model = LinearClassifier(input_dim)\n",
    "model = model.to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9118aed",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d7ae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "LR = 1e-3\n",
    "\n",
    "# Compute pos_weight for BCEWithLogitsLoss\n",
    "pos_count = (y_train == 1).sum()\n",
    "neg_count = (y_train == 0).sum()\n",
    "pos_weight_value = torch.tensor([neg_count / max(pos_count, 1)], dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_value)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "best_state_dict = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device).unsqueeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "        del xb, yb, logits, loss\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb_np = yb.numpy()\n",
    "            logits = model(xb)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy().ravel()\n",
    "            all_preds.append(probs)\n",
    "            all_targets.append(yb_np)\n",
    "            del xb, logits, probs, yb_np\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    \n",
    "    # Threshold tuning\n",
    "    roc_auc = roc_auc_score(all_targets, all_preds)\n",
    "    pr_auc = average_precision_score(all_targets, all_preds)\n",
    "    \n",
    "    best_epoch_f1 = 0.0\n",
    "    best_thr = 0.5\n",
    "    \n",
    "    thresholds = np.concatenate([\n",
    "        np.linspace(0.01, 0.05, 20),\n",
    "        np.linspace(0.05, 0.15, 50),\n",
    "        np.linspace(0.15, 0.3, 30),\n",
    "        np.linspace(0.3, 0.9, 20)\n",
    "    ])\n",
    "    \n",
    "    for thr in thresholds:\n",
    "        preds_bin = (all_preds >= thr).astype(int)\n",
    "        f1 = f1_score(all_targets, preds_bin, pos_label=1)\n",
    "        if f1 > best_epoch_f1:\n",
    "            best_epoch_f1 = f1\n",
    "            best_thr = thr\n",
    "        del preds_bin\n",
    "    \n",
    "    del all_preds, all_targets\n",
    "    \n",
    "    print(f'Epoch {epoch:02d} | train_loss={avg_train_loss:.4f} | val_f1={best_epoch_f1:.4f} @ thr={best_thr:.2f} | roc_auc={roc_auc:.4f} | pr_auc={pr_auc:.4f}')\n",
    "    memory_usage()\n",
    "    \n",
    "    if best_epoch_f1 > best_val_f1:\n",
    "        best_val_f1 = best_epoch_f1\n",
    "        best_state_dict = model.state_dict().copy()\n",
    "    \n",
    "    cleanup_memory()\n",
    "\n",
    "print('Best val F1:', best_val_f1)\n",
    "\n",
    "if best_state_dict is not None:\n",
    "    model.load_state_dict(best_state_dict)\n",
    "    MODEL_SAVE_DIR = PROJECT_ROOT / 'models' / 'saved_models'\n",
    "    MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    model_save_path = MODEL_SAVE_DIR / 'refined_model4_best.pt'\n",
    "    torch.save({\n",
    "        'model_state_dict': best_state_dict,\n",
    "        'input_dim': input_dim,\n",
    "        'best_val_f1': best_val_f1,\n",
    "        'epochs': EPOCHS,\n",
    "        'learning_rate': LR,\n",
    "        'pos_weight': pos_weight_value.cpu().item(),\n",
    "        'pca_components_per_family': PCA_COMPONENTS_PER_FAMILY\n",
    "    }, model_save_path)\n",
    "    print(f'\\n\ud83d\udcbe Saved best model to: {model_save_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb328e9",
   "metadata": {},
   "source": [
    "## 7. 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c54b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CV utilities\n",
    "try:\n",
    "    from model_training_utils import stratified_kfold_splits, find_optimal_threshold\n",
    "    USE_UTILS = True\n",
    "except ImportError:\n",
    "    USE_UTILS = False\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Combine train and val for CV\n",
    "X_full = np.vstack([X_train, X_val])\n",
    "y_full = np.concatenate([y_train, y_val])\n",
    "\n",
    "# Hyperparameter search space\n",
    "hyperparams_list = [\n",
    "    {'lr': 0.001, 'batch_size': 512},\n",
    "    {'lr': 0.0005, 'batch_size': 512},\n",
    "    {'lr': 0.001, 'batch_size': 256},\n",
    "]\n",
    "\n",
    "best_hyperparams = None\n",
    "best_cv_score = 0.0\n",
    "\n",
    "for hyperparams in hyperparams_list:\n",
    "    print(f'\\n{\"=\"*80}')\n",
    "    print(f'Hyperparameter Set: {hyperparams}')\n",
    "    print(f'{\"=\"*80}')\n",
    "    \n",
    "    cv_scores = []\n",
    "    \n",
    "    if USE_UTILS:\n",
    "        splits = stratified_kfold_splits(y_full, n_splits=5, shuffle=True)\n",
    "    else:\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "        splits = skf.split(X_full, y_full)\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(splits, 1):\n",
    "        print(f'\\nFold {fold_idx}/5')\n",
    "        \n",
    "        X_fold_train, X_fold_val = X_full[train_idx], X_full[val_idx]\n",
    "        y_fold_train, y_fold_val = y_full[train_idx], y_full[val_idx]\n",
    "        \n",
    "        # Scale\n",
    "        scaler_fold = StandardScaler()\n",
    "        X_fold_train = scaler_fold.fit_transform(X_fold_train)\n",
    "        X_fold_val = scaler_fold.transform(X_fold_val)\n",
    "        \n",
    "        # Create model\n",
    "        fold_model = LinearClassifier(input_dim)\n",
    "        fold_model = fold_model.to(device)\n",
    "        \n",
    "        # Training\n",
    "        fold_dataset = TabularDataset(X_fold_train, y_fold_train)\n",
    "        fold_loader = DataLoader(fold_dataset, batch_size=hyperparams['batch_size'], shuffle=True)\n",
    "        \n",
    "        pos_count_fold = (y_fold_train == 1).sum()\n",
    "        neg_count_fold = (y_fold_train == 0).sum()\n",
    "        pos_weight_fold = torch.tensor([neg_count_fold / max(pos_count_fold, 1)], dtype=torch.float32).to(device)\n",
    "        \n",
    "        criterion_fold = nn.BCEWithLogitsLoss(pos_weight=pos_weight_fold)\n",
    "        optimizer_fold = torch.optim.Adam(fold_model.parameters(), lr=hyperparams['lr'])\n",
    "        \n",
    "        best_fold_f1 = 0.0\n",
    "        \n",
    "        for epoch in range(1, 6):  # Max 5 epochs per fold\n",
    "            fold_model.train()\n",
    "            for xb, yb in fold_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device).unsqueeze(1)\n",
    "                optimizer_fold.zero_grad()\n",
    "                logits = fold_model(xb)\n",
    "                loss = criterion_fold(logits, yb)\n",
    "                loss.backward()\n",
    "                optimizer_fold.step()\n",
    "            \n",
    "            # Validation\n",
    "            fold_model.eval()\n",
    "            val_preds = []\n",
    "            val_targets = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                val_dataset_fold = TabularDataset(X_fold_val, y_fold_val)\n",
    "                val_loader_fold = DataLoader(val_dataset_fold, batch_size=512, shuffle=False)\n",
    "                \n",
    "                for xb, yb in val_loader_fold:\n",
    "                    xb = xb.to(device)\n",
    "                    logits = fold_model(xb)\n",
    "                    probs = torch.sigmoid(logits).cpu().numpy().ravel()\n",
    "                    val_preds.append(probs)\n",
    "                    val_targets.append(yb.numpy())\n",
    "            \n",
    "            val_preds = np.concatenate(val_preds)\n",
    "            val_targets = np.concatenate(val_targets)\n",
    "            \n",
    "            # Find best threshold\n",
    "            if USE_UTILS:\n",
    "                best_thr_fold, best_f1_fold = find_optimal_threshold(val_targets, val_preds)\n",
    "            else:\n",
    "                thresholds = np.linspace(0.01, 0.5, 50)\n",
    "                best_f1_fold = 0.0\n",
    "                best_thr_fold = 0.5\n",
    "                for thr in thresholds:\n",
    "                    preds_bin = (val_preds >= thr).astype(int)\n",
    "                    f1 = f1_score(val_targets, preds_bin, pos_label=1)\n",
    "                    if f1 > best_f1_fold:\n",
    "                        best_f1_fold = f1\n",
    "                        best_thr_fold = thr\n",
    "            \n",
    "            if best_f1_fold > best_fold_f1:\n",
    "                best_fold_f1 = best_f1_fold\n",
    "            \n",
    "            del val_preds, val_targets\n",
    "            \n",
    "            if best_fold_f1 > 0.3:  # Early stopping\n",
    "                break\n",
    "        \n",
    "        cv_scores.append(best_fold_f1)\n",
    "        print(f'  Fold {fold_idx} - Val F1: {best_fold_f1:.4f}')\n",
    "        \n",
    "        cleanup_memory()\n",
    "    \n",
    "    mean_cv_score = np.mean(cv_scores)\n",
    "    print(f'\\n\ud83d\udcca CV Results: Mean F1: {mean_cv_score:.4f} \u00b1 {np.std(cv_scores):.4f}')\n",
    "    \n",
    "    if mean_cv_score > best_cv_score:\n",
    "        best_cv_score = mean_cv_score\n",
    "        best_hyperparams = hyperparams\n",
    "        print(f'  \u2705 New best!')\n",
    "    \n",
    "    cleanup_memory()\n",
    "\n",
    "print(f'\\n\ud83c\udfc6 Best hyperparameters: {best_hyperparams}')\n",
    "print(f'\ud83c\udfc6 Best CV F1: {best_cv_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565ffd13",
   "metadata": {},
   "source": [
    "## 8. Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f14abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best hyperparameters\n",
    "if best_hyperparams is None:\n",
    "    best_hyperparams = {'lr': 0.001, 'batch_size': 512}\n",
    "\n",
    "final_model = LinearClassifier(input_dim)\n",
    "final_model = final_model.to(device)\n",
    "\n",
    "final_dataset = TabularDataset(X_train, y_train)\n",
    "final_loader = DataLoader(final_dataset, batch_size=best_hyperparams['batch_size'], shuffle=True)\n",
    "\n",
    "pos_weight_final = torch.tensor([neg_count / max(pos_count, 1)], dtype=torch.float32).to(device)\n",
    "criterion_final = nn.BCEWithLogitsLoss(pos_weight=pos_weight_final)\n",
    "optimizer_final = torch.optim.Adam(final_model.parameters(), lr=best_hyperparams['lr'])\n",
    "\n",
    "best_final_f1 = 0.0\n",
    "best_final_state = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    final_model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for xb, yb in final_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device).unsqueeze(1)\n",
    "        optimizer_final.zero_grad()\n",
    "        logits = final_model(xb)\n",
    "        loss = criterion_final(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer_final.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "    \n",
    "    avg_loss = running_loss / len(final_loader.dataset)\n",
    "    \n",
    "    # Validation\n",
    "    final_model.eval()\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = final_model(xb)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy().ravel()\n",
    "            val_preds.append(probs)\n",
    "            val_targets.append(yb.numpy())\n",
    "    \n",
    "    val_preds = np.concatenate(val_preds)\n",
    "    val_targets = np.concatenate(val_targets)\n",
    "    \n",
    "    # Find best threshold\n",
    "    thresholds = np.concatenate([\n",
    "        np.linspace(0.01, 0.05, 20),\n",
    "        np.linspace(0.05, 0.15, 50),\n",
    "        np.linspace(0.15, 0.3, 30),\n",
    "    ])\n",
    "    \n",
    "    best_epoch_f1 = 0.0\n",
    "    best_thr = 0.5\n",
    "    \n",
    "    for thr in thresholds:\n",
    "        preds_bin = (val_preds >= thr).astype(int)\n",
    "        f1 = f1_score(val_targets, preds_bin, pos_label=1)\n",
    "        if f1 > best_epoch_f1:\n",
    "            best_epoch_f1 = f1\n",
    "            best_thr = thr\n",
    "    \n",
    "    print(f'Epoch {epoch:02d} | loss={avg_loss:.4f} | val_f1={best_epoch_f1:.4f} @ thr={best_thr:.2f}')\n",
    "    \n",
    "    if best_epoch_f1 > best_final_f1:\n",
    "        best_final_f1 = best_epoch_f1\n",
    "        best_final_state = final_model.state_dict().copy()\n",
    "        final_threshold = best_thr\n",
    "    \n",
    "    del val_preds, val_targets\n",
    "    cleanup_memory()\n",
    "\n",
    "if best_final_state is not None:\n",
    "    final_model.load_state_dict(best_final_state)\n",
    "    print(f'\\n\u2705 Final model trained. Best F1: {best_final_f1:.4f} @ threshold: {final_threshold:.4f}')\n",
    "else:\n",
    "    final_threshold = 0.5\n",
    "    print(f'\\n\u26a0\ufe0f Using default threshold: {final_threshold}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66b33fe",
   "metadata": {},
   "source": [
    "## 9. Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b13380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re",
    "",
    "def extract_work_id(id_value: str) -> str:\n",
    "    \"\"\"Extract work_id from URL or return as is if already just ID.\"\"\"\n",
    "    if isinstance(id_value, str) and id_value.startswith('W') and len(id_value) > 1 and '/' not in id_value:\n",
    "        return id_value\n",
    "    id_str = str(id_value)\n",
    "    match = re.search(r'W\\d+', id_str)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return id_str\n",
    "\n",
    "    \"\"\"Extract work_id from URL or return as is if already just ID.\"\"\"",
    "    if id_value.startswith('W') and len(id_value) > 1 and '/' not in id_value:",
    "        return id_value",
    "    match = re.search(r'W\\d+', id_value)",
    "    if match:",
    "        return match.group(0)",
    "    return id_value",
    "",
    "# Load test data",
    "test_df = load_parquet_split('test')",
    "test_ids = test_df['id'].to_numpy()",
    "X_reg_test, X_emb_test_fams, _, _, _ = split_features_reg_and_all_emb(test_df)",
    "",
    "# Apply feature selection to regular features if used",
    "if handpicked_features is not None:",
    "    available_features = set(reg_cols)",
    "    selected_features = [f for f in handpicked_features if f in available_features]",
    "    if len(selected_features) < len(reg_cols):",
    "        feature_idx_map = {f: i for i, f in enumerate(reg_cols)}",
    "        selected_indices = [feature_idx_map[f] for f in selected_features]",
    "        X_reg_test = X_reg_test[:, selected_indices]",
    "",
    "# Apply PCA transform to test embeddings per family",
    "X_emb_test_pca_list = []",
    "for fam_prefix in X_emb_test_fams.keys():",
    "    if fam_prefix in pca_transformers:",
    "        X_emb_test_pca = pca_transformers[fam_prefix].transform(X_emb_test_fams[fam_prefix])",
    "        X_emb_test_pca_list.append(X_emb_test_pca)",
    "        del X_emb_test_pca",
    "",
    "# Combine regular + all PCA-compressed embeddings",
    "X_test = np.concatenate([X_reg_test] + X_emb_test_pca_list, axis=1)",
    "",
    "del X_reg_test, X_emb_test_fams, X_emb_test_pca_list, test_df",
    "",
    "# Scale",
    "X_test_scaled = scaler.transform(X_test)",
    "",
    "# Predict",
    "test_dataset = TabularDataset(X_test_scaled, np.zeros(len(X_test_scaled)))",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)",
    "",
    "final_model.eval()",
    "test_preds = []",
    "",
    "with torch.no_grad():",
    "    for xb, _ in test_loader:",
    "        xb = xb.to(device)",
    "        logits = final_model(xb)",
    "        probs = torch.sigmoid(logits).cpu().numpy().ravel()",
    "        test_preds.append(probs)",
    "",
    "test_preds = np.concatenate(test_preds)",
    "test_preds_binary = (test_preds >= final_threshold).astype(int)",
    "",
    "# Save submission",
    "SUBMISSION_DIR = PROJECT_ROOT / 'data' / 'submission_files'",
    "SUBMISSION_DIR.mkdir(parents=True, exist_ok=True)",
    "submission_path = SUBMISSION_DIR / 'submission_refined_model4.csv'",
    "",
    "# Extract work_id from test_ids",
    "    work_ids = np.array([extract_work_id(str(id_val)) for id_val in test_ids])",
    "    submission_df = pl.DataFrame({",
    "    'work_id': work_ids,",
    "    'label': test_preds_binary",
    "})",
    "submission_df.write_csv(submission_path)",
    "",
    "print(f'\\n\u2705 Submission saved to: {submission_path}')",
    "print(f'   Predictions: {test_preds_binary.sum()} positive, {(test_preds_binary==0).sum()} negative')",
    "",
    "cleanup_memory()",
    "memory_usage()",
    "print('\\n\u2705 All done!')",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}