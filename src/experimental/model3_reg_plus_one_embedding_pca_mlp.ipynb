{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeffe21e",
   "metadata": {},
   "source": [
    "# Model 3: Regular features + one embedding family (PCA) \u2013 Deep MLP\n",
    "\n",
    "This notebook trains a **deep MLP** in PyTorch on regular features **plus** a PCA-compressed\n",
    "embedding family (e.g. `sent_transformer_`), using GPU if available.\n",
    "\n",
    "**Key Features:**\n",
    "- \u2705 **Deeper architecture** (4 hidden layers) to capture complex patterns\n",
    "- \u2705 **Batch normalization** for training stability\n",
    "- \u2705 **Progressive dimension reduction** (512 \u2192 256 \u2192 128 \u2192 64 \u2192 1)\n",
    "- \u2705 **Dropout** for regularization\n",
    "- \u2705 5-fold Cross-Validation\n",
    "- \u2705 Hyperparameter Tuning\n",
    "- \u2705 Threshold Fine-tuning\n",
    "- \u2705 Model Weight Saving\n",
    "- \u2705 Submission.csv Generation\n",
    "- \u2705 OOM Safe with aggressive memory management\n",
    "- \u2705 SMOTETomek for class imbalance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d94604a",
   "metadata": {},
   "source": [
    "# \ud83d\udcd1 Model 3 - Code Navigation Index\n",
    "\n",
    "## Quick Navigation\n",
    "- **[Setup](#1-setup)** - Imports, paths, device configuration, robustness utilities\n",
    "- **[Data Loading](#2-data-loading--feature-extraction)** - Load and split features\n",
    "- **[PCA Preprocessing](#3-feature-preprocessing-pca)** - Embedding compression (if applicable)\n",
    "- **[SMOTETomek](#4-class-imbalance-handling-smotetomek)** - Class imbalance resampling\n",
    "- **[Feature Scaling](#5-feature-scaling)** - StandardScaler normalization\n",
    "- **[Cross-Validation](#6-cross-validation--hyperparameter-tuning)** - Hyperparameter optimization\n",
    "- **[Threshold Tuning](#7-threshold-tuning--final-evaluation)** - Optimal threshold finding\n",
    "- **[Model Saving](#8-save-model)** - Save model weights and metadata\n",
    "- **[Submission](#9-generate-submission)** - Generate test predictions\n",
    "\n",
    "## Model Type: Deep MLP (regular + one embedding, PCA)\n",
    "\n",
    "## Key Features\n",
    "\u2705 GPU-friendly with CPU fallback  \n",
    "\u2705 Aggressive garbage collection  \n",
    "\u2705 OOM resistant with chunked processing  \n",
    "\u2705 Kernel panic resistant (signal handlers, checkpoints)  \n",
    "\u2705 Polars-only (no pandas)  \n",
    "\u2705 GPU-friendly PCA (IncrementalTorchPCA option)  \n",
    "\u2705 SMOTETomek for class imbalance  \n",
    "\u2705 Feature scaling & embedding normalization  \n",
    "\u2705 Hyperparameter tuning (RandomizedSearchCV/GridSearchCV)  \n",
    "\u2705 Fine-grained threshold optimization (120+ thresholds)  \n",
    "\u2705 Model weights saved  \n",
    "\u2705 Chunked/batched data processing  \n",
    "\n",
    "## Memory Management\n",
    "- `cleanup_memory()`: Aggressive GC + GPU cache clearing\n",
    "- `check_memory_safe()`: Pre-operation memory checks\n",
    "- `chunked_operation()`: Process large data in chunks\n",
    "- `safe_operation()`: Retry decorator with OOM handling\n",
    "- Signal handlers: SIGINT/SIGTERM for graceful shutdown\n",
    "- Checkpoints: Resume from failures\n",
    "\n",
    "## Device Handling\n",
    "- Automatic GPU detection with CPU fallback\n",
    "- `device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')`\n",
    "- All tensors moved to device explicitly\n",
    "- GPU cache cleared aggressively after operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d17676",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a1a023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import gc\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import signal\n",
    "import atexit\n",
    "from functools import wraps\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Device (GPU if available, else CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# Paths\n",
    "current = Path(os.getcwd())\n",
    "PROJECT_ROOT = current\n",
    "for _ in range(5):\n",
    "    if (PROJECT_ROOT / 'data').exists():\n",
    "        break\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "else:\n",
    "    PROJECT_ROOT = current.parent.parent\n",
    "\n",
    "MODEL_READY_DIR = PROJECT_ROOT / 'data' / 'model_ready'\n",
    "utils_path = PROJECT_ROOT / 'src' / 'utils'\n",
    "print('PROJECT_ROOT:', PROJECT_ROOT)\n",
    "print('MODEL_READY_DIR:', MODEL_READY_DIR)\n",
    "\n",
    "# Import PCA utilities\n",
    "USE_TORCH_PCA = False  # Set to True to use PyTorch PCA (requires more memory)\n",
    "if utils_path.exists():\n",
    "    sys.path.insert(0, str(utils_path))\n",
    "\n",
    "if USE_TORCH_PCA:\n",
    "    try:\n",
    "        from pca_utils import IncrementalTorchPCA\n",
    "        IncrementalPCA = IncrementalTorchPCA  # Alias for compatibility\n",
    "        IS_TORCH_PCA = True\n",
    "        print(\"\u2705 Using PyTorch PCA (GPU-friendly)\")\n",
    "    except ImportError:\n",
    "        from sklearn.decomposition import IncrementalPCA\n",
    "        IS_TORCH_PCA = False\n",
    "        print(\"\u26a0\ufe0f Using sklearn IncrementalPCA (CPU only)\")\n",
    "else:\n",
    "    from sklearn.decomposition import IncrementalPCA\n",
    "    IS_TORCH_PCA = False\n",
    "    print(\"\u2705 Using sklearn IncrementalPCA (memory-efficient)\")\n",
    "\n",
    "# Import memory utilities from shared module\n",
    "if utils_path.exists():\n",
    "    sys.path.insert(0, str(utils_path))\n",
    "\n",
    "try:\n",
    "    from model_training_utils import cleanup_memory, memory_usage\n",
    "    print(\"\u2705 Memory utilities imported from shared module\")\n",
    "except ImportError:\n",
    "    def cleanup_memory():\n",
    "        \"\"\"Aggressive memory cleanup for both CPU and GPU.\"\"\"\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "            torch.cuda.ipc_collect()\n",
    "        gc.collect()\n",
    "    \n",
    "    def memory_usage():\n",
    "        \"\"\"Display current memory usage statistics.\"\"\"\n",
    "        try:\n",
    "            import psutil\n",
    "            process = psutil.Process(os.getpid())\n",
    "            mem_info = process.memory_info()\n",
    "            print(f\"\ud83d\udcbe Memory: {mem_info.rss / 1024**3:.2f} GB (RAM)\", end=\"\")\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_mem = torch.cuda.memory_allocated() / 1024**3\n",
    "                gpu_reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "                print(f\" | {gpu_mem:.2f}/{gpu_reserved:.2f} GB (GPU used/reserved)\")\n",
    "            else:\n",
    "                print()\n",
    "        except ImportError:\n",
    "            print(\"\ud83d\udcbe Memory tracking requires psutil: pip install psutil\")\n",
    "    \n",
    "    print(\"\u26a0\ufe0f Using fallback memory utilities\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad239972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ENHANCED ROBUSTNESS UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "# Global checkpoint state\n",
    "_checkpoint_state = {\n",
    "    'pca_complete': False,\n",
    "    'scaling_complete': False,\n",
    "    'cv_complete': False,\n",
    "    'final_model_trained': False,\n",
    "    'last_saved_checkpoint': None\n",
    "}\n",
    "\n",
    "def save_checkpoint(state_name: str, data: dict, checkpoint_dir: Path = None):\n",
    "    \"\"\"Save checkpoint to resume from failures.\"\"\"\n",
    "    if checkpoint_dir is None:\n",
    "        checkpoint_dir = PROJECT_ROOT / 'data' / 'checkpoints'\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    checkpoint_path = checkpoint_dir / f'model3_checkpoint_{state_name}.pkl'\n",
    "    try:\n",
    "        with open(checkpoint_path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "        _checkpoint_state['last_saved_checkpoint'] = checkpoint_path\n",
    "        print(f\"\u2705 Checkpoint saved: {checkpoint_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f Failed to save checkpoint: {e}\")\n",
    "\n",
    "def load_checkpoint(state_name: str, checkpoint_dir: Path = None):\n",
    "    \"\"\"Load checkpoint to resume from failures.\"\"\"\n",
    "    if checkpoint_dir is None:\n",
    "        checkpoint_dir = PROJECT_ROOT / 'data' / 'checkpoints'\n",
    "    checkpoint_path = checkpoint_dir / f'model3_checkpoint_{state_name}.pkl'\n",
    "    if checkpoint_path.exists():\n",
    "        try:\n",
    "            with open(checkpoint_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            print(f\"\u2705 Checkpoint loaded: {checkpoint_path}\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"\u26a0\ufe0f Failed to load checkpoint: {e}\")\n",
    "    return None\n",
    "\n",
    "def check_memory_safe(ram_threshold_gb=0.85, gpu_threshold=0.80):\n",
    "    \"\"\"Check if memory usage is safe before operations.\"\"\"\n",
    "    try:\n",
    "        import psutil\n",
    "        process = psutil.Process(os.getpid())\n",
    "        ram_gb = process.memory_info().rss / 1024**3\n",
    "        total_ram = psutil.virtual_memory().total / 1024**3\n",
    "        ram_ratio = ram_gb / total_ram if total_ram > 0 else 0\n",
    "        gpu_ratio = 0\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_used = torch.cuda.memory_allocated() / 1024**3\n",
    "            gpu_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "            gpu_ratio = gpu_used / gpu_total if gpu_total > 0 else 0\n",
    "        is_safe = ram_ratio < ram_threshold_gb and gpu_ratio < gpu_threshold\n",
    "        return is_safe, {'ram_gb': ram_gb, 'ram_ratio': ram_ratio, 'gpu_ratio': gpu_ratio}\n",
    "    except:\n",
    "        return True, {}\n",
    "\n",
    "def chunked_operation(data, operation_func, chunk_size: int = 10000, progress_every: int = 10, operation_name: str = \"operation\"):\n",
    "    \"\"\"Execute operation on data in chunks with progress tracking.\"\"\"\n",
    "    total_chunks = (len(data) + chunk_size - 1) // chunk_size\n",
    "    results = []\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        chunk_num = i // chunk_size + 1\n",
    "        chunk = data[i:i+chunk_size]\n",
    "        try:\n",
    "            is_safe, mem_info = check_memory_safe(ram_threshold_gb=0.85, gpu_threshold=0.80)\n",
    "            if not is_safe:\n",
    "                cleanup_memory()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                time.sleep(0.5)\n",
    "            chunk_result = operation_func(chunk)\n",
    "            results.append(chunk_result)\n",
    "            if chunk_num % progress_every == 0 or chunk_num == total_chunks:\n",
    "                print(f\"  Progress: {chunk_num}/{total_chunks} chunks ({chunk_num*100//total_chunks}%)\")\n",
    "            del chunk\n",
    "            if chunk_num % 5 == 0:\n",
    "                cleanup_memory()\n",
    "        except (MemoryError, RuntimeError) as e:\n",
    "            error_msg = str(e).lower()\n",
    "            if 'out of memory' in error_msg or 'oom' in error_msg:\n",
    "                cleanup_memory()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                smaller_chunk_size = max(1000, chunk_size // 2)\n",
    "                if smaller_chunk_size < chunk_size:\n",
    "                    return chunked_operation(data[i:], operation_func, chunk_size=smaller_chunk_size, progress_every=progress_every, operation_name=operation_name)\n",
    "                else:\n",
    "                    raise\n",
    "            else:\n",
    "                raise\n",
    "    return results\n",
    "\n",
    "def emergency_cleanup():\n",
    "    \"\"\"Emergency cleanup on exit.\"\"\"\n",
    "    try:\n",
    "        cleanup_memory()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        print(\"\u2705 Emergency cleanup completed\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "atexit.register(emergency_cleanup)\n",
    "\n",
    "# Signal handler for graceful shutdown\n",
    "def signal_handler(signum, frame):\n",
    "    \"\"\"Handle signals for graceful shutdown.\"\"\"\n",
    "    print(f\"\u26a0\ufe0f Received signal {signum}, saving checkpoint...\")\n",
    "    save_checkpoint('emergency', {'status': 'signal_received', 'signal': signum})\n",
    "    emergency_cleanup()\n",
    "    raise KeyboardInterrupt\n",
    "\n",
    "try:\n",
    "    signal.signal(signal.SIGINT, signal_handler)\n",
    "    signal.signal(signal.SIGTERM, signal_handler)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"\u2705 Enhanced robustness utilities loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a159233f",
   "metadata": {},
   "source": [
    "## 2. Dataset & utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc19c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquet_split(split: str) -> pl.DataFrame:\n",
    "    \"\"\"Load a model_ready parquet split with error handling.\"\"\"\n",
    "    try:\n",
    "        path = MODEL_READY_DIR / f'{split}_model_ready.parquet'\n",
    "        if not path.exists():\n",
    "            alt = MODEL_READY_DIR / f'{split}_model_ready_reduced.parquet'\n",
    "            if alt.exists():\n",
    "                path = alt\n",
    "            else:\n",
    "                raise FileNotFoundError(f'Could not find {split} data')\n",
    "        print(f'Loading {split} from {path}')\n",
    "        return pl.read_parquet(path)\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error loading {split}: {e}\")\n",
    "        raise\n",
    "\n",
    "def split_features_reg_and_one_emb(df: pl.DataFrame, embedding_family_prefix: str = 'sent_transformer_'):\n",
    "    \"\"\"Split features into regular and one embedding family.\"\"\"\n",
    "    cols = df.columns\n",
    "    dtypes = df.dtypes\n",
    "    label = df['label'].to_numpy() if 'label' in cols else None\n",
    "    \n",
    "    EMBEDDING_FAMILY_PREFIXES = ['sent_transformer_', 'scibert_', 'specter_', 'specter2_', 'ner_']\n",
    "    \n",
    "    NUMERIC_DTYPES = {\n",
    "        pl.Int8, pl.Int16, pl.Int32, pl.Int64,\n",
    "        pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,\n",
    "        pl.Float32, pl.Float64\n",
    "    }\n",
    "    \n",
    "    reg_cols = []\n",
    "    emb_cols = []\n",
    "    \n",
    "    for c, dt in zip(cols, dtypes):\n",
    "        if c in ('id', 'label'):\n",
    "            continue\n",
    "        if c.startswith(embedding_family_prefix):\n",
    "            emb_cols.append(c)\n",
    "        elif dt in NUMERIC_DTYPES:\n",
    "            reg_cols.append(c)\n",
    "    \n",
    "    X_reg = df.select(reg_cols).to_numpy() if reg_cols else None\n",
    "    X_emb = df.select(emb_cols).to_numpy() if emb_cols else None\n",
    "    \n",
    "    return X_reg, X_emb, label, reg_cols, emb_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4f4d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ROBUST FEATURE SELECTION: Use reasoned handpicked features\n",
    "# ============================================================================\n",
    "\n",
    "USE_FEATURE_SELECTION = False  # Disabled: JSON contains all features anyway\n",
    "\n",
    "if USE_FEATURE_SELECTION:\n",
    "    try:\n",
    "        curated_path = MODEL_READY_DIR / 'handpicked_features_reasoned.json'\n",
    "        if curated_path.exists():\n",
    "            with open(curated_path) as f:\n",
    "                curated_data = json.load(f)\n",
    "            handpicked_features = curated_data['handpicked_features']\n",
    "            print(f'\\n\ud83d\udcca Feature Selection: Using {len(handpicked_features)} reasoned handpicked features')\n",
    "        else:\n",
    "            print(f'  \u26a0\ufe0f Reasoned features file not found, using all features')\n",
    "            handpicked_features = None\n",
    "    except Exception as e:\n",
    "        print(f'  \u26a0\ufe0f Error in feature selection: {e}')\n",
    "        handpicked_features = None\n",
    "else:\n",
    "    handpicked_features = None\n",
    "    print('\\n\ud83d\udcca Feature Selection: DISABLED (using all features)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ba25ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select embedding family to use\n",
    "EMBEDDING_FAMILY_PREFIX = 'sent_transformer_'  # Options: 'sent_transformer_', 'scibert_', 'specter2_'\n",
    "print(f'\\n\ud83d\udcca Using embedding family: {EMBEDDING_FAMILY_PREFIX}')\n",
    "\n",
    "# Load data\n",
    "train_df = load_parquet_split('train')\n",
    "val_df = load_parquet_split('val')\n",
    "\n",
    "X_reg_train, X_emb_train, y_train, reg_cols, emb_cols = split_features_reg_and_one_emb(train_df, EMBEDDING_FAMILY_PREFIX)\n",
    "X_reg_val, X_emb_val, y_val, _, _ = split_features_reg_and_one_emb(val_df, EMBEDDING_FAMILY_PREFIX)\n",
    "\n",
    "# Apply feature selection to regular features if enabled\n",
    "if handpicked_features is not None:\n",
    "    available_features = set(reg_cols)\n",
    "    selected_features = [f for f in handpicked_features if f in available_features]\n",
    "    if len(selected_features) < len(reg_cols):\n",
    "        feature_idx_map = {f: i for i, f in enumerate(reg_cols)}\n",
    "        selected_indices = [feature_idx_map[f] for f in selected_features]\n",
    "        X_reg_train = X_reg_train[:, selected_indices]\n",
    "        X_reg_val = X_reg_val[:, selected_indices]\n",
    "        reg_cols = selected_features\n",
    "        print(f'  \u2705 Feature selection applied! Regular features: {len(reg_cols)}')\n",
    "\n",
    "print(f'Regular feature count: {len(reg_cols)}')\n",
    "print(f'Embedding feature count: {len(emb_cols) if emb_cols else 0}')\n",
    "\n",
    "del train_df, val_df\n",
    "cleanup_memory()\n",
    "memory_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a2f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PCA COMPRESSION OF EMBEDDING FAMILY\n",
    "# ============================================================================\n",
    "\n",
    "N_COMPONENTS = 64  # Number of PCA components\n",
    "\n",
    "if X_emb_train is not None and X_emb_train.shape[1] > 0:\n",
    "    print(f'\\n\ud83d\udcca Applying PCA compression to {EMBEDDING_FAMILY_PREFIX}...')\n",
    "    print(f'  Original embedding dim: {X_emb_train.shape[1]}')\n",
    "    print(f'  Target PCA components: {N_COMPONENTS}')\n",
    "    \n",
    "    max_pca_rows = min(150_000, X_emb_train.shape[0])\n",
    "    if X_emb_train.shape[0] > max_pca_rows:\n",
    "        idx = np.random.choice(X_emb_train.shape[0], size=max_pca_rows, replace=False)\n",
    "        pca_fit_data = X_emb_train[idx]\n",
    "    else:\n",
    "        pca_fit_data = X_emb_train\n",
    "    \n",
    "    if IS_TORCH_PCA:\n",
    "        ipca = IncrementalPCA(n_components=N_COMPONENTS, batch_size=5000, device=device)\n",
    "    else:\n",
    "        ipca = IncrementalPCA(n_components=N_COMPONENTS, batch_size=5000)\n",
    "    \n",
    "    ipca.fit(pca_fit_data)\n",
    "    X_emb_train_pca = ipca.transform(X_emb_train)\n",
    "    X_emb_val_pca = ipca.transform(X_emb_val) if X_emb_val is not None else None\n",
    "    \n",
    "    print(f'  \u2705 PCA fitted with {N_COMPONENTS} components on {len(pca_fit_data)} rows')\n",
    "    print(f'  Reduced embedding dim: {X_emb_train_pca.shape[1]}')\n",
    "    \n",
    "    X_train = np.concatenate([X_reg_train, X_emb_train_pca], axis=1)\n",
    "    X_val = np.concatenate([X_reg_val, X_emb_val_pca], axis=1)\n",
    "    \n",
    "    del X_reg_train, X_reg_val, X_emb_train, X_emb_val, X_emb_train_pca, X_emb_val_pca, pca_fit_data\n",
    "    cleanup_memory()\n",
    "else:\n",
    "    print('\\n\u26a0\ufe0f No embedding features found, using regular features only')\n",
    "    X_train = X_reg_train\n",
    "    X_val = X_reg_val\n",
    "    ipca = None\n",
    "    del X_reg_train, X_reg_val\n",
    "    cleanup_memory()\n",
    "\n",
    "print(f'Train shape: {X_train.shape} Val shape: {X_val.shape}')\n",
    "memory_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950ca6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    \"\"\"Simple tabular dataset for PyTorch.\"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# DataLoader configuration\n",
    "BATCH_SIZE = 512\n",
    "VAL_BATCH_SIZE = 512\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "train_dataset = TabularDataset(X_train, y_train)\n",
    "val_dataset = TabularDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "print(f'\\n\ud83d\udcca DataLoader Configuration:')\n",
    "print(f'   Train batch size: {BATCH_SIZE}')\n",
    "print(f'   Val batch size: {VAL_BATCH_SIZE}')\n",
    "print(f'   Num workers: {NUM_WORKERS} (0 = single process, saves memory)')\n",
    "print(f'Class counts (train): {np.bincount(y_train.astype(int))}')\n",
    "memory_usage()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d413dc1",
   "metadata": {},
   "source": [
    "## 3. Class Imbalance Handling: SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d527360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "print('\\n\ud83d\udcca Checking class imbalance and applying SMOTETomek resampling...')\n",
    "print(f'  Before: {len(X_train)} samples, Positive: {y_train.sum()}, Negative: {(y_train==0).sum()}')\n",
    "print(f'  Imbalance ratio: {(y_train==0).sum() / max(y_train.sum(), 1):.2f}:1')\n",
    "\n",
    "try:\n",
    "    smt = SMOTETomek(random_state=42, sampling_strategy='auto', n_jobs=-1)\n",
    "    X_train_resampled, y_train_resampled = smt.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(f'  After: {len(X_train_resampled)} samples, Positive: {y_train_resampled.sum()}, Negative: {(y_train_resampled==0).sum()}')\n",
    "    print(f'  Balance ratio: {(y_train_resampled==0).sum() / max(y_train_resampled.sum(), 1):.2f}:1')\n",
    "    \n",
    "    X_train = X_train_resampled\n",
    "    y_train = y_train_resampled\n",
    "    \n",
    "    del X_train_resampled, y_train_resampled\n",
    "    cleanup_memory()\n",
    "    \n",
    "    train_dataset = TabularDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=False\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f'  \u26a0\ufe0f SMOTETomek failed: {e}')\n",
    "    print('  Continuing with original training data...')\n",
    "    cleanup_memory()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226b4097",
   "metadata": {},
   "source": [
    "## 4. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4af62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print('\\n\ud83d\udcca Applying Feature Scaling...')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "X_train = X_train_scaled\n",
    "X_val = X_val_scaled\n",
    "\n",
    "del X_train_scaled, X_val_scaled\n",
    "cleanup_memory()\n",
    "\n",
    "print(f'  \u2705 Features scaled: {X_train.shape}')\n",
    "\n",
    "# Update datasets\n",
    "train_dataset = TabularDataset(X_train, y_train)\n",
    "val_dataset = TabularDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "memory_usage()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437621d2",
   "metadata": {},
   "source": [
    "## 5. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa198b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Multi-Layer Perceptron with batch normalization and progressive dimension reduction.\n",
    "    \n",
    "    Architecture Design:\n",
    "    - 4 Hidden Layers: 512 \u2192 256 \u2192 128 \u2192 64 \u2192 1\n",
    "    - Batch Normalization: Stabilizes training\n",
    "    - Progressive Compression: Prevents overfitting while maintaining capacity\n",
    "    - Dropout: Regularization for small datasets\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int,\n",
    "                 hidden_dims: tuple = (512, 256, 128, 64),\n",
    "                 dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Build hidden layers with batch norm\n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer (no batch norm, no activation)\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "print(f'\ud83d\udcca Input dimension: {input_dim}')\n",
    "print(f'   Regular features: {len(reg_cols)}')\n",
    "print(f'   PCA-compressed embeddings: {N_COMPONENTS}')\n",
    "\n",
    "model = DeepMLP(input_dim)\n",
    "model = model.to(device)\n",
    "print(f'\\n\ud83c\udfd7\ufe0f  Model Architecture:')\n",
    "print(model)\n",
    "print(f'\\n\ud83d\udcc8 Total parameters: {sum(p.numel() for p in model.parameters()):,}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c48f2a",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb8f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 15  # More epochs for MLP\n",
    "LR = 1e-3\n",
    "\n",
    "# Compute pos_weight for BCEWithLogitsLoss\n",
    "pos_count = (y_train == 1).sum()\n",
    "neg_count = (y_train == 0).sum()\n",
    "pos_weight_value = torch.tensor([neg_count / max(pos_count, 1)], dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_value)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "best_state_dict = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device).unsqueeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "        del xb, yb, logits, loss\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb_np = yb.numpy()\n",
    "            logits = model(xb)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy().ravel()\n",
    "            all_preds.append(probs)\n",
    "            all_targets.append(yb_np)\n",
    "            del xb, logits, probs, yb_np\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    \n",
    "    # Threshold tuning\n",
    "    roc_auc = roc_auc_score(all_targets, all_preds)\n",
    "    pr_auc = average_precision_score(all_targets, all_preds)\n",
    "    \n",
    "    best_epoch_f1 = 0.0\n",
    "    best_thr = 0.5\n",
    "    \n",
    "    thresholds = np.concatenate([\n",
    "        np.linspace(0.01, 0.05, 20),\n",
    "        np.linspace(0.05, 0.15, 50),\n",
    "        np.linspace(0.15, 0.3, 30),\n",
    "        np.linspace(0.3, 0.9, 20)\n",
    "    ])\n",
    "    \n",
    "    for thr in thresholds:\n",
    "        preds_bin = (all_preds >= thr).astype(int)\n",
    "        f1 = f1_score(all_targets, preds_bin, pos_label=1)\n",
    "        if f1 > best_epoch_f1:\n",
    "            best_epoch_f1 = f1\n",
    "            best_thr = thr\n",
    "        del preds_bin\n",
    "    \n",
    "    del all_preds, all_targets\n",
    "    \n",
    "    print(f'Epoch {epoch:02d} | train_loss={avg_train_loss:.4f} | val_f1={best_epoch_f1:.4f} @ thr={best_thr:.2f} | roc_auc={roc_auc:.4f} | pr_auc={pr_auc:.4f}')\n",
    "    memory_usage()\n",
    "    \n",
    "    if best_epoch_f1 > best_val_f1:\n",
    "        best_val_f1 = best_epoch_f1\n",
    "        best_state_dict = model.state_dict().copy()\n",
    "    \n",
    "    cleanup_memory()\n",
    "\n",
    "print('Best val F1:', best_val_f1)\n",
    "\n",
    "if best_state_dict is not None:\n",
    "    model.load_state_dict(best_state_dict)\n",
    "    MODEL_SAVE_DIR = PROJECT_ROOT / 'models' / 'saved_models'\n",
    "    MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    model_save_path = MODEL_SAVE_DIR / 'refined_model3_best.pt'\n",
    "    torch.save({\n",
    "        'model_state_dict': best_state_dict,\n",
    "        'input_dim': input_dim,\n",
    "        'best_val_f1': best_val_f1,\n",
    "        'epochs': EPOCHS,\n",
    "        'learning_rate': LR,\n",
    "        'pos_weight': pos_weight_value.cpu().item(),\n",
    "        'embedding_family': EMBEDDING_FAMILY_PREFIX,\n",
    "        'pca_components': N_COMPONENTS\n",
    "    }, model_save_path)\n",
    "    print(f'\\n\ud83d\udcbe Saved best model to: {model_save_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd64d18",
   "metadata": {},
   "source": [
    "## 7. 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3efe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CV utilities\n",
    "try:\n",
    "    from model_training_utils import stratified_kfold_splits, find_optimal_threshold\n",
    "    USE_UTILS = True\n",
    "except ImportError:\n",
    "    USE_UTILS = False\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Combine train and val for CV\n",
    "X_full = np.vstack([X_train, X_val])\n",
    "y_full = np.concatenate([y_train, y_val])\n",
    "\n",
    "# Hyperparameter search space\n",
    "hyperparams_list = [\n",
    "    {'lr': 0.001, 'batch_size': 512, 'n_components': 64},\n",
    "    {'lr': 0.0005, 'batch_size': 512, 'n_components': 64},\n",
    "    {'lr': 0.001, 'batch_size': 256, 'n_components': 64},\n",
    "]\n",
    "\n",
    "best_hyperparams = None\n",
    "best_cv_score = 0.0\n",
    "\n",
    "for hyperparams in hyperparams_list:\n",
    "    print(f'\\n{\"=\"*80}')\n",
    "    print(f'Hyperparameter Set: {hyperparams}')\n",
    "    print(f'{\"=\"*80}')\n",
    "    \n",
    "    cv_scores = []\n",
    "    \n",
    "    if USE_UTILS:\n",
    "        splits = stratified_kfold_splits(y_full, n_splits=5, shuffle=True)\n",
    "    else:\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "        splits = skf.split(X_full, y_full)\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(splits, 1):\n",
    "        print(f'\\nFold {fold_idx}/5')\n",
    "        \n",
    "        X_fold_train, X_fold_val = X_full[train_idx], X_full[val_idx]\n",
    "        y_fold_train, y_fold_val = y_full[train_idx], y_full[val_idx]\n",
    "        \n",
    "        # Scale\n",
    "        scaler_fold = StandardScaler()\n",
    "        X_fold_train = scaler_fold.fit_transform(X_fold_train)\n",
    "        X_fold_val = scaler_fold.transform(X_fold_val)\n",
    "        \n",
    "        # Create model (DeepMLP instead of LinearClassifier)\n",
    "        fold_model = DeepMLP(input_dim)\n",
    "        fold_model = fold_model.to(device)\n",
    "        \n",
    "        # Training\n",
    "        fold_dataset = TabularDataset(X_fold_train, y_fold_train)\n",
    "        fold_loader = DataLoader(fold_dataset, batch_size=hyperparams['batch_size'], shuffle=True)\n",
    "        \n",
    "        pos_count_fold = (y_fold_train == 1).sum()\n",
    "        neg_count_fold = (y_fold_train == 0).sum()\n",
    "        pos_weight_fold = torch.tensor([neg_count_fold / max(pos_count_fold, 1)], dtype=torch.float32).to(device)\n",
    "        \n",
    "        criterion_fold = nn.BCEWithLogitsLoss(pos_weight=pos_weight_fold)\n",
    "        optimizer_fold = torch.optim.Adam(fold_model.parameters(), lr=hyperparams['lr'])\n",
    "        \n",
    "        best_fold_f1 = 0.0\n",
    "        \n",
    "        for epoch in range(1, 11):  # Max 10 epochs per fold for MLP\n",
    "            fold_model.train()\n",
    "            for xb, yb in fold_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device).unsqueeze(1)\n",
    "                optimizer_fold.zero_grad()\n",
    "                logits = fold_model(xb)\n",
    "                loss = criterion_fold(logits, yb)\n",
    "                loss.backward()\n",
    "                optimizer_fold.step()\n",
    "            \n",
    "            # Validation\n",
    "            fold_model.eval()\n",
    "            val_preds = []\n",
    "            val_targets = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                val_dataset_fold = TabularDataset(X_fold_val, y_fold_val)\n",
    "                val_loader_fold = DataLoader(val_dataset_fold, batch_size=512, shuffle=False)\n",
    "                \n",
    "                for xb, yb in val_loader_fold:\n",
    "                    xb = xb.to(device)\n",
    "                    logits = fold_model(xb)\n",
    "                    probs = torch.sigmoid(logits).cpu().numpy().ravel()\n",
    "                    val_preds.append(probs)\n",
    "                    val_targets.append(yb.numpy())\n",
    "            \n",
    "            val_preds = np.concatenate(val_preds)\n",
    "            val_targets = np.concatenate(val_targets)\n",
    "            \n",
    "            # Find best threshold\n",
    "            if USE_UTILS:\n",
    "                best_thr_fold, best_f1_fold = find_optimal_threshold(val_targets, val_preds)\n",
    "            else:\n",
    "                thresholds = np.linspace(0.01, 0.5, 50)\n",
    "                best_f1_fold = 0.0\n",
    "                best_thr_fold = 0.5\n",
    "                for thr in thresholds:\n",
    "                    preds_bin = (val_preds >= thr).astype(int)\n",
    "                    f1 = f1_score(val_targets, preds_bin, pos_label=1)\n",
    "                    if f1 > best_f1_fold:\n",
    "                        best_f1_fold = f1\n",
    "                        best_thr_fold = thr\n",
    "            \n",
    "            if best_f1_fold > best_fold_f1:\n",
    "                best_fold_f1 = best_f1_fold\n",
    "            \n",
    "            del val_preds, val_targets\n",
    "            \n",
    "            if best_fold_f1 > 0.3:  # Early stopping\n",
    "                break\n",
    "        \n",
    "        cv_scores.append(best_fold_f1)\n",
    "        print(f'  Fold {fold_idx} - Val F1: {best_fold_f1:.4f}')\n",
    "        \n",
    "        cleanup_memory()\n",
    "    \n",
    "    mean_cv_score = np.mean(cv_scores)\n",
    "    print(f'\\n\ud83d\udcca CV Results: Mean F1: {mean_cv_score:.4f} \u00b1 {np.std(cv_scores):.4f}')\n",
    "    \n",
    "    if mean_cv_score > best_cv_score:\n",
    "        best_cv_score = mean_cv_score\n",
    "        best_hyperparams = hyperparams\n",
    "        print(f'  \u2705 New best!')\n",
    "    \n",
    "    cleanup_memory()\n",
    "\n",
    "print(f'\\n\ud83c\udfc6 Best hyperparameters: {best_hyperparams}')\n",
    "print(f'\ud83c\udfc6 Best CV F1: {best_cv_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccf8f03",
   "metadata": {},
   "source": [
    "## 8. Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126c13f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best hyperparameters\n",
    "if best_hyperparams is None:\n",
    "    best_hyperparams = {'lr': 0.001, 'batch_size': 512, 'n_components': 64}\n",
    "\n",
    "final_model = DeepMLP(input_dim)\n",
    "final_model = final_model.to(device)\n",
    "\n",
    "final_dataset = TabularDataset(X_train, y_train)\n",
    "final_loader = DataLoader(final_dataset, batch_size=best_hyperparams['batch_size'], shuffle=True)\n",
    "\n",
    "pos_weight_final = torch.tensor([neg_count / max(pos_count, 1)], dtype=torch.float32).to(device)\n",
    "criterion_final = nn.BCEWithLogitsLoss(pos_weight=pos_weight_final)\n",
    "optimizer_final = torch.optim.Adam(final_model.parameters(), lr=best_hyperparams['lr'])\n",
    "\n",
    "best_final_f1 = 0.0\n",
    "best_final_state = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    final_model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for xb, yb in final_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device).unsqueeze(1)\n",
    "        optimizer_final.zero_grad()\n",
    "        logits = final_model(xb)\n",
    "        loss = criterion_final(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer_final.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "    \n",
    "    avg_loss = running_loss / len(final_loader.dataset)\n",
    "    \n",
    "    # Validation\n",
    "    final_model.eval()\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = final_model(xb)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy().ravel()\n",
    "            val_preds.append(probs)\n",
    "            val_targets.append(yb.numpy())\n",
    "    \n",
    "    val_preds = np.concatenate(val_preds)\n",
    "    val_targets = np.concatenate(val_targets)\n",
    "    \n",
    "    # Find best threshold\n",
    "    thresholds = np.concatenate([\n",
    "        np.linspace(0.01, 0.05, 20),\n",
    "        np.linspace(0.05, 0.15, 50),\n",
    "        np.linspace(0.15, 0.3, 30),\n",
    "    ])\n",
    "    \n",
    "    best_epoch_f1 = 0.0\n",
    "    best_thr = 0.5\n",
    "    \n",
    "    for thr in thresholds:\n",
    "        preds_bin = (val_preds >= thr).astype(int)\n",
    "        f1 = f1_score(val_targets, preds_bin, pos_label=1)\n",
    "        if f1 > best_epoch_f1:\n",
    "            best_epoch_f1 = f1\n",
    "            best_thr = thr\n",
    "    \n",
    "    print(f'Epoch {epoch:02d} | loss={avg_loss:.4f} | val_f1={best_epoch_f1:.4f} @ thr={best_thr:.2f}')\n",
    "    \n",
    "    if best_epoch_f1 > best_final_f1:\n",
    "        best_final_f1 = best_epoch_f1\n",
    "        best_final_state = final_model.state_dict().copy()\n",
    "        final_threshold = best_thr\n",
    "    \n",
    "    del val_preds, val_targets\n",
    "    cleanup_memory()\n",
    "\n",
    "if best_final_state is not None:\n",
    "    final_model.load_state_dict(best_final_state)\n",
    "    print(f'\\n\u2705 Final model trained. Best F1: {best_final_f1:.4f} @ threshold: {final_threshold:.4f}')\n",
    "else:\n",
    "    final_threshold = 0.5\n",
    "    print(f'\\n\u26a0\ufe0f Using default threshold: {final_threshold}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45553ec0",
   "metadata": {},
   "source": [
    "## 9. Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080f9f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re",
    "",
    "def extract_work_id(id_value: str) -> str:\n",
    "    \"\"\"Extract work_id from URL or return as is if already just ID.\"\"\"\n",
    "    if isinstance(id_value, str) and id_value.startswith('W') and len(id_value) > 1 and '/' not in id_value:\n",
    "        return id_value\n",
    "    id_str = str(id_value)\n",
    "    match = re.search(r'W\\d+', id_str)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return id_str\n",
    "\n",
    "    \"\"\"Extract work_id from URL or return as is if already just ID.\"\"\"",
    "    if id_value.startswith('W') and len(id_value) > 1 and '/' not in id_value:",
    "        return id_value",
    "    match = re.search(r'W\\d+', id_value)",
    "    if match:",
    "        return match.group(0)",
    "    return id_value",
    "",
    "# Load test data",
    "test_df = load_parquet_split('test')",
    "test_ids = test_df['id'].to_numpy()",
    "X_reg_test, X_emb_test, _, _, _ = split_features_reg_and_one_emb(test_df, EMBEDDING_FAMILY_PREFIX)",
    "",
    "# Apply feature selection to regular features if used",
    "if handpicked_features is not None:",
    "    available_features = set(reg_cols)",
    "    selected_features = [f for f in handpicked_features if f in available_features]",
    "    if len(selected_features) < len(reg_cols):",
    "        feature_idx_map = {f: i for i, f in enumerate(reg_cols)}",
    "        selected_indices = [feature_idx_map[f] for f in selected_features]",
    "        X_reg_test = X_reg_test[:, selected_indices]",
    "",
    "# Apply PCA transform to test embeddings",
    "if X_emb_test is not None and ipca is not None:",
    "    X_emb_test_pca = ipca.transform(X_emb_test)",
    "    X_test = np.concatenate([X_reg_test, X_emb_test_pca], axis=1)",
    "    del X_emb_test_pca",
    "else:",
    "    X_test = X_reg_test",
    "",
    "del X_reg_test, X_emb_test, test_df",
    "",
    "# Scale",
    "X_test_scaled = scaler.transform(X_test)",
    "",
    "# Predict",
    "test_dataset = TabularDataset(X_test_scaled, np.zeros(len(X_test_scaled)))",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)",
    "",
    "final_model.eval()",
    "test_preds = []",
    "",
    "with torch.no_grad():",
    "    for xb, _ in test_loader:",
    "        xb = xb.to(device)",
    "        logits = final_model(xb)",
    "        probs = torch.sigmoid(logits).cpu().numpy().ravel()",
    "        test_preds.append(probs)",
    "",
    "test_preds = np.concatenate(test_preds)",
    "test_preds_binary = (test_preds >= final_threshold).astype(int)",
    "",
    "# Save submission",
    "SUBMISSION_DIR = PROJECT_ROOT / 'data' / 'submission_files'",
    "SUBMISSION_DIR.mkdir(parents=True, exist_ok=True)",
    "submission_path = SUBMISSION_DIR / 'submission_refined_model3.csv'",
    "",
    "# Extract work_id from test_ids",
    "    work_ids = np.array([extract_work_id(str(id_val)) for id_val in test_ids])",
    "    submission_df = pl.DataFrame({",
    "    'work_id': work_ids,",
    "    'label': test_preds_binary",
    "})",
    "submission_df.write_csv(submission_path)",
    "",
    "print(f'\\n\u2705 Submission saved to: {submission_path}')",
    "print(f'   Predictions: {test_preds_binary.sum()} positive, {(test_preds_binary==0).sum()} negative')",
    "",
    "cleanup_memory()",
    "memory_usage()",
    "print('\\n\u2705 All done!')",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}