{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31790620",
   "metadata": {},
   "source": [
    "# Model K: RandomForestClassifier with All Features",
    "",
    "This notebook trains an **RandomForestClassifier** classifier on all available features (regular + all embeddings) with comprehensive preprocessing:",
    "- \u2705 All regular features",
    "- \u2705 All embedding families (PCA-compressed)",
    "- \u2705 Feature scaling (StandardScaler)",
    "- \u2705 5-fold Cross-Validation",
    "- \u2705 Comprehensive Hyperparameter Tuning (Optuna)",
    "- \u2705 Threshold Fine-tuning",
    "- \u2705 Model Saving",
    "- \u2705 Submission.csv Generation",
    "- \u2705 OOM Safe with aggressive memory management",
    "- \u2705 SMOTETomek for class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f75e41",
   "metadata": {},
   "source": [
    "# \ud83d\udcd1 Model K - Code Navigation Index",
    "",
    "## Quick Navigation",
    "- **[Setup](#1-setup)** - Imports, paths, device configuration, robustness utilities",
    "- **[Data Loading](#2-data-loading--feature-extraction)** - Load and split features",
    "- **[PCA Preprocessing](#3-feature-preprocessing-pca)** - Embedding compression (if applicable)",
    "- **[SMOTETomek](#4-class-imbalance-handling-smotetomek)** - Class imbalance resampling",
    "- **[Feature Scaling](#5-feature-scaling)** - StandardScaler normalization",
    "- **[Cross-Validation](#6-cross-validation--hyperparameter-tuning)** - Hyperparameter optimization",
    "- **[Threshold Tuning](#7-threshold-tuning--final-evaluation)** - Optimal threshold finding",
    "- **[Model Saving](#8-save-model)** - Save model weights and metadata",
    "- **[Submission](#9-generate-submission)** - Generate test predictions",
    "",
    "## Model Type: RandomForestClassifier (all features)",
    "",
    "## Key Features",
    "\u2705 GPU-friendly with CPU fallback  ",
    "\u2705 Aggressive garbage collection  ",
    "\u2705 OOM resistant with chunked processing  ",
    "\u2705 Kernel panic resistant (signal handlers, checkpoints)  ",
    "\u2705 Polars-only (no pandas)  ",
    "\u2705 GPU-friendly PCA (IncrementalTorchPCA option)  ",
    "\u2705 SMOTETomek for class imbalance  ",
    "\u2705 Feature scaling & embedding normalization  ",
    "\u2705 Hyperparameter tuning (Optuna/GridSearchCV)  ",
    "\u2705 Fine-grained threshold optimization (120+ thresholds)  ",
    "\u2705 Model weights saved  ",
    "\u2705 Chunked/batched data processing  ",
    "",
    "## Memory Management",
    "- `cleanup_memory()`: Aggressive GC + GPU cache clearing",
    "- `check_memory_safe()`: Pre-operation memory checks",
    "- `chunked_operation()`: Process large data in chunks",
    "- `safe_operation()`: Retry decorator with OOM handling",
    "- Signal handlers: SIGINT/SIGTERM for graceful shutdown",
    "- Checkpoints: Resume from failures",
    "",
    "## Device Handling",
    "- Automatic GPU detection with CPU fallback",
    "- `device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')`",
    "- All tensors moved to device explicitly",
    "- GPU cache cleared aggressively after operations",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcbd00b",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f91258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import gc\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "from typing import Dict, Optional\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import signal\n",
    "import atexit\n",
    "from functools import wraps\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "# Device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "# Paths\n",
    "current = Path(os.getcwd())\n",
    "PROJECT_ROOT = current\n",
    "for _ in range(5):\n",
    "    if (PROJECT_ROOT / \"data\").exists():\n",
    "        break\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "else:\n",
    "    PROJECT_ROOT = current.parent.parent\n",
    "MODEL_READY_DIR = PROJECT_ROOT / \"data\" / \"model_ready\"\n",
    "MODEL_SAVE_DIR = PROJECT_ROOT / \"models\" / \"saved_models\"\n",
    "SUBMISSION_DIR = PROJECT_ROOT / \"data\" / \"submission_files\"\n",
    "CHECKPOINT_DIR = PROJECT_ROOT / \"data\" / \"checkpoints\" / \"modelK\"\n",
    "MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SUBMISSION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "utils_path = PROJECT_ROOT / \"src\" / \"utils\"\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"MODEL_READY_DIR:\", MODEL_READY_DIR)\n",
    "# Import PCA utilities\n",
    "USE_TORCH_PCA = False\n",
    "if utils_path.exists():\n",
    "    sys.path.insert(0, str(utils_path))\n",
    "if USE_TORCH_PCA:\n",
    "    try:\n",
    "        from pca_utils import IncrementalTorchPCA\n",
    "        IncrementalPCA = IncrementalTorchPCA\n",
    "        IS_TORCH_PCA = True\n",
    "        print(\"\u2705 Using PyTorch PCA (GPU-friendly)\")\n",
    "    except ImportError:\n",
    "        from sklearn.decomposition import IncrementalPCA\n",
    "        IS_TORCH_PCA = False\n",
    "        print(\"\u26a0\ufe0f Using sklearn IncrementalPCA (CPU only)\")\n",
    "else:\n",
    "    from sklearn.decomposition import IncrementalPCA\n",
    "    IS_TORCH_PCA = False\n",
    "    print(\"\u2705 Using sklearn IncrementalPCA (memory-efficient)\")\n",
    "# ML libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report, precision_recall_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Import memory utilities from shared module\n",
    "try:\n",
    "    from model_training_utils import cleanup_memory, memory_usage, check_memory_safe\n",
    "    print(\"\u2705 Memory utilities imported from shared module\")\n",
    "except ImportError:\n",
    "    # Fallback definitions if utils not available\n",
    "    def cleanup_memory():\n",
    "        \"\"\"Aggressive memory cleanup for both CPU and GPU.\"\"\"\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "            torch.cuda.ipc_collect()\n",
    "        gc.collect()\n",
    "    def memory_usage():\n",
    "        \"\"\"Display current memory usage statistics.\"\"\"\n",
    "        try:\n",
    "            import psutil\n",
    "            process = psutil.Process(os.getpid())\n",
    "            mem_gb = process.memory_info().rss / 1024**3\n",
    "            print(f\"\ud83d\udcbe Memory: {mem_gb:.2f} GB (RAM)\", end=\"\")\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_mem = torch.cuda.memory_allocated() / 1024**3\n",
    "                gpu_reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "                print(f\" | {gpu_mem:.2f}/{gpu_reserved:.2f} GB (GPU used/reserved)\")\n",
    "            else:\n",
    "                print()\n",
    "        except:\n",
    "            pass\n",
    "    def check_memory_safe(ram_threshold_gb=0.85, gpu_threshold=0.80):\n",
    "        \"\"\"Check if memory usage is safe for operations.\"\"\"\n",
    "        try:\n",
    "            import psutil\n",
    "            process = psutil.Process(os.getpid())\n",
    "            ram_gb = process.memory_info().rss / 1024**3\n",
    "            total_ram = psutil.virtual_memory().total / 1024**3\n",
    "            ram_ratio = ram_gb / total_ram if total_ram > 0 else 0\n",
    "            gpu_ratio = 0\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_used = torch.cuda.memory_allocated() / 1024**3\n",
    "                gpu_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "                gpu_ratio = gpu_used / gpu_total if gpu_total > 0 else 0\n",
    "            is_safe = ram_ratio < ram_threshold_gb and gpu_ratio < gpu_threshold\n",
    "            return is_safe, {\"ram_gb\": ram_gb, \"ram_ratio\": ram_ratio, \"gpu_ratio\": gpu_ratio}\n",
    "        except:\n",
    "            return True, {}\n",
    "    print(\"\u26a0\ufe0f Using fallback memory utilities\")\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "memory_usage()\n",
    "# ============================================================================\n",
    "# ENHANCED ROBUSTNESS UTILITIES\n",
    "# ============================================================================\n",
    "# Global checkpoint state\n",
    "_checkpoint_state = {\n",
    "    \"pca_complete\": False,\n",
    "    \"scaling_complete\": False,\n",
    "    \"cv_complete\": False,\n",
    "    \"final_model_trained\": False,\n",
    "    \"last_saved_checkpoint\": None,\n",
    "}\n",
    "def save_checkpoint(state_name: str, data: dict, checkpoint_dir: Path = None):\n",
    "    \"\"\"Save checkpoint to resume from failures.\"\"\"\n",
    "    if checkpoint_dir is None:\n",
    "        checkpoint_dir = PROJECT_ROOT / \"data\" / \"checkpoints\"\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    checkpoint_path = checkpoint_dir / f\"modelK_checkpoint_{state_name}.pkl\"\n",
    "    try:\n",
    "        with open(checkpoint_path, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        _checkpoint_state[\"last_saved_checkpoint\"] = checkpoint_path\n",
    "        print(f\"\u2705 Checkpoint saved: {checkpoint_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f Failed to save checkpoint: {e}\")\n",
    "def load_checkpoint(state_name: str, checkpoint_dir: Path = None):\n",
    "    \"\"\"Load checkpoint to resume from failures.\"\"\"\n",
    "    if checkpoint_dir is None:\n",
    "        checkpoint_dir = PROJECT_ROOT / \"data\" / \"checkpoints\"\n",
    "    checkpoint_path = checkpoint_dir / f\"modelK_checkpoint_{state_name}.pkl\"\n",
    "    if checkpoint_path.exists():\n",
    "        try:\n",
    "            with open(checkpoint_path, \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "            print(f\"\u2705 Checkpoint loaded: {checkpoint_path}\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"\u26a0\ufe0f Failed to load checkpoint: {e}\")\n",
    "    return None\n",
    "def safe_operation(operation_name: str, max_retries: int = 3, checkpoint_on_success: bool = False):\n",
    "    \"\"\"Decorator for safe operations with retry and checkpoint support.\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    is_safe, mem_info = check_memory_safe(ram_threshold_gb=0.80, gpu_threshold=0.75)\n",
    "                    if not is_safe:\n",
    "                        cleanup_memory()\n",
    "                        if torch.cuda.is_available():\n",
    "                            torch.cuda.empty_cache()\n",
    "                        time.sleep(1)\n",
    "                    result = func(*args, **kwargs)\n",
    "                    cleanup_memory()\n",
    "                    if checkpoint_on_success:\n",
    "                        save_checkpoint(operation_name, {\"status\": \"complete\", \"result\": result})\n",
    "                    return result\n",
    "                except (MemoryError, RuntimeError) as e:\n",
    "                    error_msg = str(e).lower()\n",
    "                    if \"out of memory\" in error_msg or \"oom\" in error_msg:\n",
    "                        if attempt < max_retries - 1:\n",
    "                            cleanup_memory()\n",
    "                            if torch.cuda.is_available():\n",
    "                                torch.cuda.empty_cache()\n",
    "                            time.sleep(2)\n",
    "                            continue\n",
    "                        else:\n",
    "                            raise\n",
    "                    else:\n",
    "                        raise\n",
    "                except Exception as e:\n",
    "                    if attempt < max_retries - 1:\n",
    "                        cleanup_memory()\n",
    "                        time.sleep(1)\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise\n",
    "            return None\n",
    "        return wrapper\n",
    "    return decorator\n",
    "def chunked_operation(\n",
    "    data,\n",
    "    operation_func,\n",
    "    chunk_size: int = 10000,\n",
    "    progress_every: int = 10,\n",
    "    operation_name: str = \"operation\",\n",
    "):\n",
    "    \"\"\"Execute operation on data in chunks with progress tracking.\"\"\"\n",
    "    total_chunks = (len(data) + chunk_size - 1) // chunk_size\n",
    "    results = []\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        chunk_num = i // chunk_size + 1\n",
    "        chunk = data[i : i + chunk_size]\n",
    "        try:\n",
    "            is_safe, mem_info = check_memory_safe(ram_threshold_gb=0.85, gpu_threshold=0.80)\n",
    "            if not is_safe:\n",
    "                cleanup_memory()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                time.sleep(0.5)\n",
    "            chunk_result = operation_func(chunk)\n",
    "            results.append(chunk_result)\n",
    "            if chunk_num % progress_every == 0 or chunk_num == total_chunks:\n",
    "                print(f\"  Progress: {chunk_num}/{total_chunks} chunks ({chunk_num*100//total_chunks}%)\")\n",
    "            del chunk\n",
    "            if chunk_num % 5 == 0:\n",
    "                cleanup_memory()\n",
    "        except (MemoryError, RuntimeError) as e:\n",
    "            error_msg = str(e).lower()\n",
    "            if \"out of memory\" in error_msg or \"oom\" in error_msg:\n",
    "                cleanup_memory()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                smaller_chunk_size = max(1000, chunk_size // 2)\n",
    "                if smaller_chunk_size < chunk_size:\n",
    "                    return chunked_operation(\n",
    "                        data[i:],\n",
    "                        operation_func,\n",
    "                        chunk_size=smaller_chunk_size,\n",
    "                        progress_every=progress_every,\n",
    "                        operation_name=operation_name,\n",
    "                    )\n",
    "                else:\n",
    "                    raise\n",
    "            else:\n",
    "                raise\n",
    "    return results\n",
    "def emergency_cleanup():\n",
    "    \"\"\"Emergency cleanup on exit.\"\"\"\n",
    "    try:\n",
    "        cleanup_memory()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        print(\"\u2705 Emergency cleanup completed\")\n",
    "    except:\n",
    "        pass\n",
    "atexit.register(emergency_cleanup)\n",
    "def signal_handler(signum, frame):\n",
    "    \"\"\"Handle signals for graceful shutdown.\"\"\"\n",
    "    print(f\"\u26a0\ufe0f Received signal {signum}, saving checkpoint...\")\n",
    "    save_checkpoint(\"emergency\", {\"status\": \"signal_received\", \"signal\": signum})\n",
    "    emergency_cleanup()\n",
    "    raise KeyboardInterrupt\n",
    "try:\n",
    "    signal.signal(signal.SIGINT, signal_handler)\n",
    "    signal.signal(signal.SIGTERM, signal_handler)\n",
    "except:\n",
    "    pass\n",
    "print(\"\u2705 Enhanced robustness utilities loaded\")\n",
    "def safe_prediction(predict_func, *args, **kwargs):\n",
    "    \"\"\"Execute prediction with chunked processing.\"\"\"\n",
    "    try:\n",
    "        is_safe, mem_info = check_memory_safe(ram_threshold_gb=0.85, gpu_threshold=0.80)\n",
    "        if not is_safe:\n",
    "            cleanup_memory()\n",
    "        if \"X\" in kwargs and len(kwargs[\"X\"]) > 50000:\n",
    "            X = kwargs[\"X\"]\n",
    "            chunk_size = 10000\n",
    "            predictions = []\n",
    "            for i in range(0, len(X), chunk_size):\n",
    "                chunk = X[i : i + chunk_size]\n",
    "                kwargs[\"X\"] = chunk\n",
    "                chunk_preds = predict_func(*args, **kwargs)\n",
    "                predictions.append(chunk_preds)\n",
    "                del chunk, chunk_preds\n",
    "                if i % (chunk_size * 5) == 0:\n",
    "                    cleanup_memory()\n",
    "            return np.concatenate(predictions)\n",
    "        else:\n",
    "            return predict_func(*args, **kwargs)\n",
    "    except (MemoryError, RuntimeError) as e:\n",
    "        error_msg = str(e).lower()\n",
    "        if \"out of memory\" in error_msg or \"oom\" in error_msg:\n",
    "            cleanup_memory()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            if \"X\" in kwargs:\n",
    "                X = kwargs[\"X\"]\n",
    "                chunk_size = 5000\n",
    "                predictions = []\n",
    "                for i in range(0, len(X), chunk_size):\n",
    "                    chunk = X[i : i + chunk_size]\n",
    "                    kwargs[\"X\"] = chunk\n",
    "                    chunk_preds = predict_func(*args, **kwargs)\n",
    "                    predictions.append(chunk_preds)\n",
    "                    del chunk, chunk_preds\n",
    "                    cleanup_memory()\n",
    "                return np.concatenate(predictions)\n",
    "            else:\n",
    "                raise\n",
    "        else:\n",
    "            raise\n",
    "print(\"\u2705 Training robustness wrappers loaded\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a155ed",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21551565",
   "metadata": {},
   "outputs": [],
   "source": "def load_parquet_split(split: str) -> pl.DataFrame:\n    \"\"\"Load a model_ready parquet split with error handling.\"\"\"\n    try:\n        path = MODEL_READY_DIR / f\"{split}_model_ready.parquet\"\n        if not path.exists():\n            alt = MODEL_READY_DIR / f\"{split}_model_ready_reduced.parquet\"\n            if alt.exists():\n                path = alt\n            else:\n                raise FileNotFoundError(f\"Could not find {split} data\")\n        print(f\"Loading {split} from {path}\")\n        return pl.read_parquet(path)\n    except Exception as e:\n        print(f\"\u274c Error loading {split}: {e}\")\n        raise\ndef split_features_reg_and_all_emb(df: pl.DataFrame):\n    \"\"\"Split features into regular and embedding families.\"\"\"\n    cols = df.columns\n    dtypes = df.dtypes\n    label = df[\"label\"].to_numpy() if \"label\" in cols else None\n    reg_cols = []\n    EMBEDDING_FAMILY_PREFIXES = [\"sent_transformer_\", \"scibert_\", \"specter_\", \"specter2_\", \"ner_\"]\n    emb_family_to_cols = {p: [] for p in EMBEDDING_FAMILY_PREFIXES}\n    NUMERIC_DTYPES = {\n        pl.Int8,\n        pl.Int16,\n        pl.Int32,\n        pl.Int64,\n        pl.UInt8,\n        pl.UInt16,\n        pl.UInt32,\n        pl.UInt64,\n        pl.Float32,\n        pl.Float64,\n    }\n    for c, dt in zip(cols, dtypes):\n        if c in (\"id\", \"label\"):\n            continue\n        matched = False\n        for p in EMBEDDING_FAMILY_PREFIXES:\n            if c.startswith(p):\n                emb_family_to_cols[p].append(c)\n                matched = True\n                break\n        if not matched and dt in NUMERIC_DTYPES:\n            reg_cols.append(c)\n    X_reg = df.select(reg_cols).to_numpy() if reg_cols else None\n    X_emb_families = {}\n    for p, clist in emb_family_to_cols.items():\n        if clist:\n            X_emb_families[p] = df.select(clist).to_numpy()\n    return X_reg, X_emb_families, label, reg_cols, emb_family_to_cols\n# Load data\ntry:\n    train_df = load_parquet_split(\"train\")\n    val_df = load_parquet_split(\"val\")\n    X_reg_train, X_emb_train_fams, y_train, reg_cols, emb_family_to_cols = (\n        split_features_reg_and_all_emb(train_df)\n    )\n    X_reg_val, X_emb_val_fams, y_val, _, _ = split_features_reg_and_all_emb(val_df)\n    print(f\"\\n\ud83d\udcca Data Summary:\")\n    print(f\"  Regular features: {len(reg_cols)}\")\n    for fam, arr in X_emb_train_fams.items():\n        print(f\"  Embedding {fam}: {arr.shape[1]} dims\")\n    print(\n        f\"  Train samples: {len(y_train)}, Positive: {y_train.sum()}, Negative: {(y_train==0).sum()}\"\n    )\n    print(f\"  Val samples: {len(y_val)}, Positive: {y_val.sum()}, Negative: {(y_val==0).sum()}\")\n    del train_df, val_df\n    cleanup_memory()\n    memory_usage()\nexcept Exception as e:\n    print(f\"\u274c Error loading data: {e}\")\n    raise\n"
  },
  {
   "cell_type": "markdown",
   "id": "347958ec",
   "metadata": {},
   "source": [
    "## 3. Feature Preprocessing: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7689b88f",
   "metadata": {},
   "outputs": [],
   "source": "# PCA compression per embedding family\nPCA_COMPONENTS_PER_FAMILY = {\n    \"sent_transformer_\": 32,\n    \"scibert_\": 32,\n    \"specter_\": 32,\n    \"specter2_\": 32,\n    \"ner_\": 16,\n}\n@safe_operation(\"pca\", max_retries=3, checkpoint_on_success=True)\ndef apply_pca_to_embeddings(\n    X_emb_fams: Dict[str, np.ndarray], fit_on_train: bool = True, pca_models: Optional[Dict] = None\n):\n    \"\"\"Apply IncrementalPCA to each embedding family (GPU-friendly, OOM-resistant).\"\"\"\n    X_emb_pca_list = []\n    new_pca_models = {}\n    cleanup_memory()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n    is_safe, mem_info = check_memory_safe(ram_threshold_gb=0.75, gpu_threshold=0.70)\n    if not is_safe:\n        print(f\"\u26a0\ufe0f Memory usage high before PCA: {mem_info}\")\n        cleanup_memory()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n    for fam, X_emb in X_emb_fams.items():\n        n_components = PCA_COMPONENTS_PER_FAMILY.get(fam, 32)\n        try:\n            cleanup_memory()\n            if fit_on_train or pca_models is None:\n                if IS_TORCH_PCA:\n                    ipca = IncrementalPCA(\n                        n_components=min(n_components, X_emb.shape[1]),\n                        batch_size=2000,\n                        device=device,\n                    )\n                else:\n                    ipca = IncrementalPCA(\n                        n_components=min(n_components, X_emb.shape[1]), batch_size=2000\n                    )\n                max_pca_rows = int(X_emb.shape[0] * 0.3)\n                if X_emb.shape[0] > max_pca_rows:\n                    print(\n                        f\"  Fitting PCA on subset ({max_pca_rows}/{X_emb.shape[0]} samples) for {fam}\"\n                    )\n                    idx = np.random.choice(X_emb.shape[0], size=max_pca_rows, replace=False)\n                    X_emb_subset = X_emb[idx].copy()\n                    del idx\n                    cleanup_memory()\n                    ipca.fit(X_emb_subset)\n                    del X_emb_subset\n                    cleanup_memory()\n                else:\n                    X_emb_copy = X_emb.copy() if X_emb.flags[\"OWNDATA\"] == False else X_emb\n                    ipca.fit(X_emb_copy)\n                    if X_emb_copy is not X_emb:\n                        del X_emb_copy\n                    cleanup_memory()\n                new_pca_models[fam] = ipca\n            else:\n                ipca = pca_models[fam]\n            chunk_size = 5000\n            if X_emb.shape[0] > chunk_size:\n                X_emb_pca_chunks = []\n                for i in range(0, X_emb.shape[0], chunk_size):\n                    chunk = X_emb[i : i + chunk_size].copy()\n                    chunk_pca = ipca.transform(chunk)\n                    X_emb_pca_chunks.append(chunk_pca)\n                    del chunk, chunk_pca\n                    cleanup_memory()\n                    if i % (chunk_size * 5) == 0 and torch.cuda.is_available():\n                        torch.cuda.empty_cache()\n                X_emb_pca = np.vstack(X_emb_pca_chunks)\n                del X_emb_pca_chunks\n                cleanup_memory()\n            else:\n                X_emb_copy = X_emb.copy() if X_emb.flags[\"OWNDATA\"] == False else X_emb\n                X_emb_pca = ipca.transform(X_emb_copy)\n                if X_emb_copy is not X_emb:\n                    del X_emb_copy\n                cleanup_memory()\n            X_emb_pca_list.append(X_emb_pca)\n            del X_emb_pca\n            cleanup_memory()\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                torch.cuda.synchronize()\n        except RuntimeError as e:\n            if \"out of memory\" in str(e).lower() or \"OOM\" in str(e).upper():\n                print(f\"\u274c OOM error processing {fam}\")\n                cleanup_memory()\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n                    torch.cuda.synchronize()\n                raise\n            else:\n                raise\n    cleanup_memory()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    if X_emb_pca_list:\n        X_emb_combined = np.hstack(X_emb_pca_list)\n    else:\n        X_emb_combined = None\n    return X_emb_combined, new_pca_models if fit_on_train else pca_models\n# Apply IncrementalPCA to embeddings\ntry:\n    cleanup_memory()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n    print(\"\\n\ud83d\udcca Applying IncrementalPCA to embedding families...\")\n    for fam in X_emb_train_fams.keys():\n        n_comp = PCA_COMPONENTS_PER_FAMILY.get(fam, 32)\n        print(f\"  {fam}: {X_emb_train_fams[fam].shape[1]} dims \u2192 {n_comp} components\")\n    X_emb_train_pca, pca_models_train = apply_pca_to_embeddings(X_emb_train_fams, fit_on_train=True)\n    cleanup_memory()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    X_emb_val_pca, _ = apply_pca_to_embeddings(\n        X_emb_val_fams, fit_on_train=False, pca_models=pca_models_train\n    )\n    print(f\"\\n\ud83d\udcca After IncrementalPCA:\")\n    print(f\"  Train embeddings: {X_emb_train_pca.shape}\")\n    print(f\"  Val embeddings: {X_emb_val_pca.shape}\")\n    # Combine regular + embeddings\n    if X_reg_train is not None:\n        X_train = np.hstack([X_reg_train, X_emb_train_pca])\n        X_val = np.hstack([X_reg_val, X_emb_val_pca])\n    else:\n        X_train = X_emb_train_pca\n        X_val = X_emb_val_pca\n    print(f\"  Combined train: {X_train.shape}\")\n    print(f\"  Combined val: {X_val.shape}\")\n    del X_reg_train, X_reg_val, X_emb_train_fams, X_emb_val_fams, X_emb_train_pca, X_emb_val_pca\n    cleanup_memory()\n    memory_usage()\nexcept Exception as e:\n    print(f\"\u274c Error in PCA: {e}\")\n    raise\n"
  },
  {
   "cell_type": "markdown",
   "id": "727018dc",
   "metadata": {},
   "source": [
    "## 4. Class Imbalance Handling: SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dec740",
   "metadata": {},
   "outputs": [],
   "source": "from imblearn.combine import SMOTETomek\nprint(\"\\n\ud83d\udcca Checking class imbalance and applying SMOTETomek resampling...\")\nprint(f\"  Before: {len(X_train)} samples, Positive: {y_train.sum()}, Negative: {(y_train==0).sum()}\")\nprint(f\"  Imbalance ratio: {(y_train==0).sum() / max(y_train.sum(), 1):.2f}:1\")\ntry:\n    smt = SMOTETomek(random_state=42, sampling_strategy=\"auto\", n_jobs=-1)\n    X_train_resampled, y_train_resampled = smt.fit_resample(X_train, y_train)\n    print(f\"  After: {len(X_train_resampled)} samples, Positive: {y_train_resampled.sum()}, Negative: {(y_train_resampled==0).sum()}\")\n    print(f\"  Balance ratio: {(y_train_resampled==0).sum() / max(y_train_resampled.sum(), 1):.2f}:1\")\n    X_train = X_train_resampled\n    y_train = y_train_resampled\n    del X_train_resampled, y_train_resampled\n    cleanup_memory()\nexcept Exception as e:\n    print(f\"  \u26a0\ufe0f SMOTETomek failed: {e}\")\n    print(\"  Continuing with original training data...\")\n    cleanup_memory()\n"
  },
  {
   "cell_type": "markdown",
   "id": "f125dd8b",
   "metadata": {},
   "source": [
    "## 5. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea3bf82",
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\ud83d\udcca Applying Feature Scaling to combined features...\")\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nX_train = X_train_scaled\nX_val = X_val_scaled\ndel X_train_scaled, X_val_scaled\ncleanup_memory()\nprint(\"  \u2705 Scaling complete!\")\nmemory_usage()\n"
  },
  {
   "cell_type": "markdown",
   "id": "da7df909",
   "metadata": {},
   "source": [
    "## 6. Cross-Validation & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b0ac3d",
   "metadata": {},
   "outputs": [],
   "source": "# Combine train and val for CV\nX_full = np.vstack([X_train, X_val])\ny_full = np.hstack([y_train, y_val])\nprint(f\"\\n\ud83d\udcca Full dataset for CV: {X_full.shape}, labels: {y_full.shape}\")\nprint(f\"  Positive samples: {y_full.sum()}, Negative: {(y_full==0).sum()}\")\n# Setup 5-fold CV\nN_FOLDS = 5\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n# XGBoost hyperparameter grid\nXGB_PARAM_GRID = {\n    \"n_estimators\": [100, 200, 300],\n    \"max_depth\": [3, 5, 7],\n    \"learning_rate\": [0.01, 0.1, 0.2],\n    \"subsample\": [0.8, 0.9, 1.0],\n    \"colsample_bytree\": [0.8, 0.9, 1.0],\n    \"min_child_weight\": [1, 3, 5],\n    \"gamma\": [0, 0.1, 0.2],\n    \"reg_alpha\": [0, 0.1, 0.5],\n    \"reg_lambda\": [1, 1.5, 2.0],\n    \"scale_pos_weight\": [1, (y_full == 0).sum() / max((y_full == 1).sum(), 1)],\n}\n# Use RandomizedSearchCV for faster tuning\nUSE_RANDOMIZED_SEARCH = True\nN_ITER_RANDOM = 50\nprint(f\"\\n\ud83d\udd0d Hyperparameter tuning:\")\nprint(f\"  Method: {'RandomizedSearchCV' if USE_RANDOMIZED_SEARCH else 'GridSearchCV'}\")\nprint(f\"  CV folds: {N_FOLDS}\")\nif USE_RANDOMIZED_SEARCH:\n    print(f\"  Random iterations: {N_ITER_RANDOM}\")\ncleanup_memory()\nmemory_usage()\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36e770c",
   "metadata": {},
   "outputs": [],
   "source": "# Optuna hyperparameter tuning\nbest_params = None\nbest_cv_score = 0.0\ndef objective(trial):\n    \"\"\"Optuna objective function for RandomForestClassifier hyperparameter tuning.\"\"\"\n    # TODO: Implement model-specific hyperparameter space\n    # For now, use default parameters\n    model = RandomForestClassifier(random_state=SEED)\n    cv_scores = []\n    for train_idx, val_idx in skf.split(X_full, y_full):\n        X_cv_train, X_cv_val = X_full[train_idx], X_full[val_idx]\n        y_cv_train, y_cv_val = y_full[train_idx], y_full[val_idx]\n        model.fit(X_cv_train, y_cv_train)\n        y_pred = model.predict(X_cv_val)\n        f1 = f1_score(y_cv_val, y_pred, pos_label=1, zero_division=0)\n        cv_scores.append(f1)\n        cleanup_memory()\n    return np.mean(cv_scores)\ntry:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Starting Hyperparameter Tuning with Optuna\")\n    print(\"=\" * 80)\n    study = optuna.create_study(\n        direction=\"maximize\",\n        sampler=TPESampler(seed=SEED),\n        study_name=\"modelk_randomforestclassifier\"\n    )\n    # Load checkpoint if exists\n    checkpoint_data = load_checkpoint(\"optuna_study\")\n    if checkpoint_data and \"study\" in checkpoint_data:\n        print(\"  Resuming from checkpoint...\")\n        study = checkpoint_data[\"study\"]\n    start_time = time.time()\n    study.optimize(objective, n_trials=50, timeout=3600, show_progress_bar=True)\n    elapsed_time = time.time() - start_time\n    best_params = study.best_params\n    best_cv_score = study.best_value\n    # Save checkpoint\n    save_checkpoint(\"optuna_study\", {\"study\": study, \"best_params\": best_params, \"best_cv_score\": best_cv_score})\n    print(f\"\\n\u2705 Hyperparameter tuning complete ({elapsed_time/60:.1f} min)\")\n    print(f\"  Best CV F1: {best_cv_score:.4f}\")\n    print(f\"  Best parameters:\")\n    for key, value in best_params.items():\n        print(f\"    {key}: {value}\")\n    cleanup_memory()\n    memory_usage()\nexcept Exception as e:\n    print(f\"\u274c Error in hyperparameter tuning: {e}\")\n    print(\"\u26a0\ufe0f Using default parameters...\")\n    best_params = {}\n    best_cv_score = 0.0\n"
  },
  {
   "cell_type": "markdown",
   "id": "383be480",
   "metadata": {},
   "source": [
    "## 7. Threshold Tuning & Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c6b51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on full data\n",
    "try:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Training Final Model on Full Dataset\")\n",
    "    print(\"=\" * 80)\n",
    "    # Use best parameters or defaults\n",
    "    final_model = RandomForestClassifier(\n",
    "        **best_params,\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    final_model.fit(X_full, y_full)\n",
    "    # Get predictions on validation set (original split)\n",
    "    y_val_proba = safe_prediction(final_model.predict_proba, X=X_val)[:, 1]\n",
    "    # Find optimal threshold using precision-recall curve\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(y_val, y_val_proba)\n",
    "    f1_scores_pr = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    best_pr_idx = np.argmax(f1_scores_pr)\n",
    "    best_pr_threshold = pr_thresholds[best_pr_idx] if best_pr_idx < len(pr_thresholds) else 0.5\n",
    "    best_pr_f1 = f1_scores_pr[best_pr_idx]\n",
    "    # Manual fine-grained search in optimal region\n",
    "    thresholds = np.concatenate([\n",
    "        np.linspace(0.01, 0.05, 20),\n",
    "        np.linspace(0.05, 0.15, 50),\n",
    "        np.linspace(0.15, 0.3, 30),\n",
    "        np.linspace(0.3, 0.9, 20)\n",
    "    ])\n",
    "    best_threshold = best_pr_threshold\n",
    "    best_f1 = best_pr_f1\n",
    "    for thr in thresholds:\n",
    "        y_pred = (y_val_proba >= thr).astype(int)\n",
    "        f1 = f1_score(y_val, y_pred, pos_label=1, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = thr\n",
    "    print(f\"\\n\u2705 Final Optimal Threshold: {best_threshold:.4f}\")\n",
    "    print(f\"\u2705 Final Validation F1: {best_f1:.4f}\")\n",
    "    # Classification report\n",
    "    y_val_pred = (y_val_proba >= best_threshold).astype(int)\n",
    "    print(\"\\n\ud83d\udcca Classification Report:\")\n",
    "    print(classification_report(y_val, y_val_pred, digits=4, zero_division=0))\n",
    "    cleanup_memory()\n",
    "    memory_usage()\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error in final training: {e}\")\n",
    "    raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f25c7ca",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bbbf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "try:\n",
    "    model_save_path = MODEL_SAVE_DIR / \"modelK_randomforestclassifier_all_features_best.pkl\"\n",
    "    save_dict = {\n",
    "        \"model\": final_model,\n",
    "        \"scaler\": scaler if \"scaler\" in locals() else None,\n",
    "        \"pca_models\": pca_models_train if \"pca_models_train\" in locals() else None,\n",
    "        \"best_params\": best_params,\n",
    "        \"best_cv_score\": best_cv_score,\n",
    "        \"best_threshold\": best_threshold,\n",
    "        \"best_f1\": best_f1,\n",
    "        \"reg_cols\": reg_cols,\n",
    "        \"emb_family_to_cols\": emb_family_to_cols,\n",
    "    }\n",
    "    with open(model_save_path, \"wb\") as f:\n",
    "        pickle.dump(save_dict, f)\n",
    "    print(f\"\\n\ud83d\udcbe Model saved to: {model_save_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error saving model: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fdd758",
   "metadata": {},
   "source": [
    "## 9. Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f7e363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re",
    "",
    "def extract_work_id(id_value: str) -> str:\n",
    "    \"\"\"Extract work_id from URL or return as is if already just ID.\"\"\"\n",
    "    if isinstance(id_value, str) and id_value.startswith('W') and len(id_value) > 1 and '/' not in id_value:\n",
    "        return id_value\n",
    "    id_str = str(id_value)\n",
    "    match = re.search(r'W\\d+', id_str)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return id_str\n",
    "\n",
    "    \"\"\"Extract work_id from URL or return as is if already just ID.\"\"\"",
    "    if id_value.startswith('W') and len(id_value) > 1 and '/' not in id_value:",
    "        return id_value",
    "    match = re.search(r'W\\d+', id_value)",
    "    if match:",
    "        return match.group(0)",
    "    return id_value",
    "",
    "# Load test data and generate predictions",
    "try:",
    "    print(\"\\n\" + \"=\" * 80)",
    "    print(\"Generating Test Predictions\")",
    "    print(\"=\" * 80)",
    "    test_df = load_parquet_split(\"test\")",
    "    test_ids = test_df[\"id\"].to_numpy()",
    "    # Process test data same as train",
    "    X_reg_test, X_emb_test_fams, _, _, _ = split_features_reg_and_all_emb(test_df)",
    "    del test_df",
    "    # Apply PCA",
    "    X_emb_test_pca, _ = apply_pca_to_embeddings(",
    "        X_emb_test_fams, fit_on_train=False, pca_models=pca_models_train",
    "    )",
    "    # Combine",
    "    if X_reg_test is not None:",
    "        X_test = np.hstack([X_reg_test, X_emb_test_pca])",
    "    else:",
    "        X_test = X_emb_test_pca",
    "    del X_reg_test, X_emb_test_fams, X_emb_test_pca",
    "    cleanup_memory()",
    "    # Scale",
    "    if \"scaler\" in locals():",
    "        X_test = scaler.transform(X_test)",
    "    # Predict in chunks for OOM protection",
    "    chunk_size = 10000",
    "    if X_test.shape[0] > chunk_size:",
    "        print(f\"  Predicting in chunks (size={chunk_size}) for OOM protection...\")",
    "        y_test_proba_chunks = []",
    "        for i in range(0, X_test.shape[0], chunk_size):",
    "            chunk_proba = safe_prediction(final_model.predict_proba, X=X_test[i : i + chunk_size])[:, 1]",
    "            y_test_proba_chunks.append(chunk_proba)",
    "            del chunk_proba",
    "            cleanup_memory()",
    "        y_test_proba = np.concatenate(y_test_proba_chunks)",
    "        del y_test_proba_chunks",
    "    else:",
    "        y_test_proba = safe_prediction(final_model.predict_proba, X=X_test)[:, 1]",
    "    y_test_pred = (y_test_proba >= best_threshold).astype(int)",
    "    # Create submission using Polars",
    "    work_ids = np.array([extract_work_id(str(id_val)) for id_val in test_ids])",
    "    submission_df = pl.DataFrame({\"work_id\": work_ids, \"label\": y_test_pred})",
    "    submission_path = SUBMISSION_DIR / \"submission_modelk.csv\"",
    "    submission_df.write_csv(submission_path)",
    "    print(f\"\\n\u2705 Submission saved to: {submission_path}\")",
    "    print(f\"  Test predictions: {len(y_test_pred)}, Positive: {y_test_pred.sum()}, Negative: {(y_test_pred==0).sum()}\")",
    "    cleanup_memory()",
    "    memory_usage()",
    "except Exception as e:",
    "    print(f\"\u274c Error generating submission: {e}\")",
    "    raise",
    ""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}