Activating virtual environment: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/venv
==========================================
JOB STARTUP INFORMATION
==========================================
Host:        gl1006.arc-ts.umich.edu
Date:        2025-11-20T00:38:09-05:00
SLURM_JOBID: 36398959
Working directory: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2
Python:      /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/venv/bin/python
==========================================
Verifying Python environment...
‚úì Basic packages available
‚úì Python environment verified
Installing Jupyter kernel: model_xgboost-36398959...
Installed kernelspec model_xgboost-36398959 in /home/santoshd/.local/share/jupyter/kernels/model_xgboost-36398959
‚úì Kernel installed successfully
Running pre-flight validation...
‚úì Found model-ready files
‚úì Notebook found: src/notebooks/model_xgboost_all_features.ipynb
‚úÖ Pre-flight validation passed!
=== Cleaning Previous Runs ===
‚úì Cleanup completed
Running papermill -> /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/runs/model_xgboost_executed_20251120-003924.ipynb
Notebook: src/notebooks/model_xgboost_all_features.ipynb
Kernel: model_xgboost-36398959
Input Notebook:  src/notebooks/model_xgboost_all_features.ipynb
Output Notebook: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/runs/model_xgboost_executed_20251120-003924.ipynb
Executing notebook with kernel: model_xgboost-36398959
Executing Cell 1---------------------------------------
Ending Cell 1------------------------------------------
Executing Cell 2---------------------------------------
Ending Cell 2------------------------------------------
Executing Cell 3---------------------------------------
Ending Cell 3------------------------------------------
Executing Cell 4---------------------------------------
Ending Cell 4------------------------------------------
Executing Cell 5---------------------------------------

================================================================================
MODEL_XGBOOST EXECUTION STARTED
Start Time: 2025-11-20 00:40:53
================================================================================


Using device: cuda

Ending Cell 5------------------------------------------
Executing Cell 6---------------------------------------
PROJECT_ROOT: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2
MODEL_READY_DIR: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/model_ready

Ending Cell 6------------------------------------------
Executing Cell 7---------------------------------------
‚úÖ Using sklearn IncrementalPCA (memory-efficient)

Ending Cell 7------------------------------------------
Executing Cell 8---------------------------------------
Ending Cell 8------------------------------------------
Executing Cell 9---------------------------------------
‚úÖ Memory utilities imported from shared module
üíæ Memory: 0.62 GB (RAM) | 0.00/0.00 GB (GPU used/reserved)

Ending Cell 9------------------------------------------
Executing Cell 10--------------------------------------
‚úÖ Enhanced robustness utilities loaded
‚úÖ Training robustness wrappers loaded

Ending Cell 10-----------------------------------------
Executing Cell 11--------------------------------------
Ending Cell 11-----------------------------------------
Executing Cell 12--------------------------------------

================================================================================
PHASE 1: Data Loading
================================================================================
Loading train from /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/model_ready/train_model_ready.parquet

Loading val from /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/model_ready/val_model_ready.parquet


üìä Data Summary:
  Regular features: 54
  Embedding sent_transformer_: 384 dims
  Embedding scibert_: 768 dims
  Embedding specter2_: 768 dims
  Train samples: 960000, Positive: 65808, Negative: 894192
  Val samples: 120000, Positive: 8075, Negative: 111925

‚è±Ô∏è  Data Loading Time: 168.12 seconds (2.80 minutes)

üíæ Memory: 32.75 GB (RAM) | 0.00/0.00 GB (GPU used/reserved)

Ending Cell 12-----------------------------------------
Executing Cell 13--------------------------------------
Ending Cell 13-----------------------------------------
Executing Cell 14--------------------------------------
Ending Cell 14-----------------------------------------
Executing Cell 15--------------------------------------

================================================================================
PHASE 2: Feature Preprocessing (No PCA)
================================================================================


üìä Using all embeddings without PCA compression...
  sent_transformer_: 384 dims (no compression)
  scibert_: 768 dims (no compression)
  specter2_: 768 dims (no compression)


üìä After combining embeddings:
  Train embeddings: (960000, 1920)
  Val embeddings: (120000, 1920)

  Combined train: (960000, 1974)
  Combined val: (120000, 1974)

‚è±Ô∏è  Feature Preprocessing Time: 14.78 seconds (0.25 minutes)

üíæ Memory: 39.03 GB (RAM) | 0.00/0.00 GB (GPU used/reserved)

Ending Cell 15-----------------------------------------
Executing Cell 16--------------------------------------
Ending Cell 16-----------------------------------------
Executing Cell 17--------------------------------------

================================================================================
PHASE 3: SMOTEENN Resampling (OOM-Resistant)
================================================================================

üìä Checking class imbalance and applying incremental SMOTE resampling...
  Before: 960000 samples, Positive: 65808, Negative: 894192
  Imbalance ratio: 13.59:1

  Sampling 50% of data for SMOTEENN resampling...

  SMOTE sample: 480000 samples (pos: 32904, neg: 447096)
  Keep as-is: 480000 samples (pos: 32904, neg: 447096)


  Using incremental SMOTEENN with chunk size: 10,000 samples
  Sampling strategy: 0.15 (target minority/majority ratio)
  Shuffling data to ensure good class distribution across chunks...

  Processing 48 chunks of up to 10,000 samples each...
    Chunk 1/48: 10000 samples (pos: 679, neg: 9321)

      ‚úì Resampled to 7730 samples

    Chunk 2/48: 10000 samples (pos: 717, neg: 9283)

      ‚úì Resampled to 7538 samples

    Chunk 3/48: 10000 samples (pos: 677, neg: 9323)

      ‚úì Resampled to 7708 samples

    Chunk 4/48: 10000 samples (pos: 694, neg: 9306)

      ‚úì Resampled to 7693 samples

    Chunk 5/48: 10000 samples (pos: 726, neg: 9274)

      ‚úì Resampled to 7621 samples

    Chunk 6/48: 10000 samples (pos: 670, neg: 9330)

      ‚úì Resampled to 7703 samples

    Chunk 7/48: 10000 samples (pos: 734, neg: 9266)

      ‚úì Resampled to 7475 samples

    Chunk 8/48: 10000 samples (pos: 706, neg: 9294)

      ‚úì Resampled to 7616 samples

    Chunk 9/48: 10000 samples (pos: 734, neg: 9266)

      ‚úì Resampled to 7487 samples

    Chunk 10/48: 10000 samples (pos: 686, neg: 9314)

      ‚úì Resampled to 7683 samples

    Chunk 11/48: 10000 samples (pos: 655, neg: 9345)

      ‚úì Resampled to 7698 samples

    Chunk 12/48: 10000 samples (pos: 693, neg: 9307)

      ‚úì Resampled to 7647 samples

    Chunk 13/48: 10000 samples (pos: 668, neg: 9332)

      ‚úì Resampled to 7585 samples

    Chunk 14/48: 10000 samples (pos: 716, neg: 9284)

      ‚úì Resampled to 7706 samples

    Chunk 15/48: 10000 samples (pos: 663, neg: 9337)

      ‚úì Resampled to 7600 samples

    Chunk 16/48: 10000 samples (pos: 679, neg: 9321)

      ‚úì Resampled to 7747 samples

    Chunk 17/48: 10000 samples (pos: 631, neg: 9369)

      ‚úì Resampled to 7717 samples

    Chunk 18/48: 10000 samples (pos: 668, neg: 9332)

      ‚úì Resampled to 7690 samples

    Chunk 19/48: 10000 samples (pos: 736, neg: 9264)

      ‚úì Resampled to 7452 samples

    Chunk 20/48: 10000 samples (pos: 697, neg: 9303)

      ‚úì Resampled to 7483 samples

    Chunk 21/48: 10000 samples (pos: 677, neg: 9323)

      ‚úì Resampled to 7628 samples

    Chunk 22/48: 10000 samples (pos: 640, neg: 9360)

      ‚úì Resampled to 7677 samples

    Chunk 23/48: 10000 samples (pos: 671, neg: 9329)

      ‚úì Resampled to 7541 samples

    Chunk 24/48: 10000 samples (pos: 740, neg: 9260)

      ‚úì Resampled to 7473 samples

    Chunk 25/48: 10000 samples (pos: 681, neg: 9319)

      ‚úì Resampled to 7610 samples

    Chunk 26/48: 10000 samples (pos: 627, neg: 9373)

      ‚úì Resampled to 7700 samples

    Chunk 27/48: 10000 samples (pos: 716, neg: 9284)

      ‚úì Resampled to 7480 samples

    Chunk 28/48: 10000 samples (pos: 660, neg: 9340)

      ‚úì Resampled to 7689 samples

    Chunk 29/48: 10000 samples (pos: 712, neg: 9288)

      ‚úì Resampled to 7573 samples

    Chunk 30/48: 10000 samples (pos: 656, neg: 9344)

      ‚úì Resampled to 7828 samples

    Chunk 31/48: 10000 samples (pos: 725, neg: 9275)

      ‚úì Resampled to 7641 samples

    Chunk 32/48: 10000 samples (pos: 682, neg: 9318)

      ‚úì Resampled to 7647 samples

    Chunk 33/48: 10000 samples (pos: 702, neg: 9298)

      ‚úì Resampled to 7595 samples

    Chunk 34/48: 10000 samples (pos: 682, neg: 9318)

      ‚úì Resampled to 7639 samples

    Chunk 35/48: 10000 samples (pos: 646, neg: 9354)

      ‚úì Resampled to 7575 samples

    Chunk 36/48: 10000 samples (pos: 689, neg: 9311)

      ‚úì Resampled to 7655 samples

    Chunk 37/48: 10000 samples (pos: 664, neg: 9336)

      ‚úì Resampled to 7753 samples

    Chunk 38/48: 10000 samples (pos: 667, neg: 9333)

      ‚úì Resampled to 7712 samples

    Chunk 39/48: 10000 samples (pos: 633, neg: 9367)

      ‚úì Resampled to 7688 samples

    Chunk 40/48: 10000 samples (pos: 699, neg: 9301)

      ‚úì Resampled to 7340 samples

    Chunk 41/48: 10000 samples (pos: 732, neg: 9268)

      ‚úì Resampled to 7453 samples

    Chunk 42/48: 10000 samples (pos: 707, neg: 9293)

      ‚úì Resampled to 7519 samples

    Chunk 43/48: 10000 samples (pos: 683, neg: 9317)

      ‚úì Resampled to 7779 samples

    Chunk 44/48: 10000 samples (pos: 653, neg: 9347)

      ‚úì Resampled to 7688 samples

    Chunk 45/48: 10000 samples (pos: 700, neg: 9300)

      ‚úì Resampled to 7657 samples

    Chunk 46/48: 10000 samples (pos: 661, neg: 9339)

      ‚úì Resampled to 7609 samples

    Chunk 47/48: 10000 samples (pos: 681, neg: 9319)

      ‚úì Resampled to 7662 samples

    Chunk 48/48: 10000 samples (pos: 689, neg: 9311)

      ‚úì Resampled to 7603 samples


  Combining 48 resampled chunks...

  Combining resampled data with kept portion...


  ‚úÖ After resampling: 845993 samples
     Positive: 64003, Negative: 781990
     Balance ratio: 12.22:1

‚è±Ô∏è  Incremental SMOTEENN Time: 255.93 seconds (4.27 minutes)

üíæ Memory: 37.39 GB (RAM) | 0.00/0.00 GB (GPU used/reserved)

Ending Cell 17-----------------------------------------
Executing Cell 18--------------------------------------
Ending Cell 18-----------------------------------------
Executing Cell 19--------------------------------------

================================================================================
PHASE 4: Feature Scaling
================================================================================

üìä Applying Feature Scaling to combined features...

  Fitting scaler on sample (20000 samples)...

  Transforming train data in chunks (size=20000)...

  Transforming val data in chunks (size=20000)...

  ‚úÖ Scaling complete!

‚è±Ô∏è  Feature Scaling Time: 62.81 seconds (1.05 minutes)
üíæ Memory: 48.40 GB (RAM) | 0.00/0.00 GB (GPU used/reserved)

Ending Cell 19-----------------------------------------
Executing Cell 20--------------------------------------
Ending Cell 20-----------------------------------------
Executing Cell 21--------------------------------------

================================================================================
PHASE 5: Hyperparameter Tuning Setup
================================================================================

‚ö†Ô∏è Dataset too large (965993 samples), using subset (20000 samples) for CV

  Using 20000 samples for hyperparameter tuning

üìä Full dataset for CV: (20000, 1974), labels: (20000,)
  Positive samples: 1492, Negative: 18508

üîç Hyperparameter tuning:
  Method: RandomizedSearchCV
  CV folds: 5
  Random iterations: 20

üíæ Memory: 42.57 GB (RAM) | 0.00/0.00 GB (GPU used/reserved)

Ending Cell 21-----------------------------------------
Executing Cell 22--------------------------------------

================================================================================
PHASE 5B: Hyperparameter Tuning
================================================================================
Fitting 5 folds for each of 20 candidates, totalling 100 fits


‚úÖ Hyperparameter tuning complete (91.2 min)
  Best CV F1: 0.5833
  Best parameters:
    subsample: 1.0
    scale_pos_weight: 12.404825737265416
    reg_lambda: 2.0
    reg_alpha: 0.5
    num_leaves: 100
    n_estimators: 300
    min_child_samples: 5
    max_depth: 5
    learning_rate: 0.1
    colsample_bytree: 1.0
üíæ Memory: 42.84 GB (RAM) | 0.00/0.00 GB (GPU used/reserved)

Ending Cell 22-----------------------------------------
Executing Cell 23--------------------------------------

================================================================================
PLOTTING: Cross-Validation Performance Curves
================================================================================

<Figure size 1400x500 with 2 Axes>
<IPython.core.display.Image object>
‚úÖ CV curves saved to: logs/model_xgboost_cv_curves.png

Ending Cell 23-----------------------------------------
Executing Cell 24--------------------------------------
Ending Cell 24-----------------------------------------
Executing Cell 25--------------------------------------

================================================================================
PHASE 6: Final Model Training & Threshold Tuning
================================================================================
Training Final Model on Full Dataset...

  Memory cleaned before final training

  Training with early stopping...
  ‚ö†Ô∏è Early stopping not supported, training without it


‚úÖ Final Optimal Threshold: 0.7623
‚úÖ Final Validation F1: 0.5125

‚è±Ô∏è  Final Model Training & Threshold Tuning Time: 1913.33 seconds (31.89 minutes)

üìä Classification Report:
              precision    recall  f1-score   support

           0     0.9716    0.9427    0.9569    111925
           1     0.4376    0.6183    0.5125      8075

    accuracy                         0.9208    120000
   macro avg     0.7046    0.7805    0.7347    120000
weighted avg     0.9357    0.9208    0.9270    120000


üíæ Memory: 43.82 GB (RAM) | 0.00/0.00 GB (GPU used/reserved)

Ending Cell 25-----------------------------------------
Executing Cell 26--------------------------------------

================================================================================
PLOTTING: Final Model Performance Visualizations
================================================================================

<Figure size 1600x1200 with 7 Axes>
<IPython.core.display.Image object>
‚úÖ Final performance plots saved to: logs/model_xgboost_final_performance.png

Ending Cell 26-----------------------------------------
Executing Cell 27--------------------------------------
Ending Cell 27-----------------------------------------
Executing Cell 28--------------------------------------

üìä Calibrating model probabilities...

  ‚úÖ Model calibrated

  Calibrated F1: 0.5138 (vs original 0.5125)
  Calibrated threshold: 0.2535

üíæ Model saved to: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/models/saved_models/model_xgboost_all_features_best.pkl

Ending Cell 28-----------------------------------------
Executing Cell 29--------------------------------------
Ending Cell 29-----------------------------------------
Executing Cell 30--------------------------------------

================================================================================
PHASE 7: Test Predictions
================================================================================
Generating Test Predictions...
Loading test from /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/model_ready/test_model_ready.parquet

  Predicting in chunks (size=10000) for OOM protection...


‚úÖ Submission saved to: /gpfs/accounts/si670f25_class_root/si670f25_class/santoshd/Kaggle_2/data/submission_files/submission_model_xgboost.csv
  Test predictions: 120000, Positive: 11271, Negative: 108729

‚è±Ô∏è  Test Predictions Time: 17.45 seconds (0.29 minutes)

üíæ Memory: 48.89 GB (RAM) | 0.00/0.00 GB (GPU used/reserved)

================================================================================

================================================================================
MODEL_XGBOOST EXECUTION COMPLETED
Start Time: 2025-11-20 00:40:53
End Time: 2025-11-20 03:56:53
Total Execution Time: 11760.09 seconds (196.00 minutes / 3.27 hours)
Final Validation F1 Score: 0.5125
================================================================================

End Time: 2025-11-20 03:56:53
Final Validation F1 Score: 0.5125
================================================================================


Ending Cell 30-----------------------------------------
‚úì Notebook execution completed in 11854s
Checking for output files...
‚úì Found submission file: submission_model_xgboost.csv
‚úì Found saved model: model_xgboost_all_features_best.pkl
‚úì Found execution log: model_xgboost_all_features_run.log

============================================================
EXECUTION SUMMARY
============================================================
Notebook execution time: 11854s
============================================================
Removed /home/santoshd/.local/share/jupyter/kernels/model_xgboost-36398959
